#!/bin/bash
# ============================================================
# Ollama æ¨¡å‹éƒ¨ç½²è„šæœ¬
# ç”¨äºéƒ¨ç½² Qwen2.5-32B-Instruct å’Œ nomic-embed-text-v1.5
# ============================================================

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ‰“å°å‡½æ•°
print_header() {
    echo -e "${BLUE}============================================================${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${BLUE}============================================================${NC}"
}

print_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

print_error() {
    echo -e "${RED}âŒ $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}âš ï¸  $1${NC}"
}

print_info() {
    echo -e "${BLUE}â„¹ï¸  $1${NC}"
}

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
MODELS_DIR="$SCRIPT_DIR"
LLM_MODELFILE="$MODELS_DIR/Modelfile.Qwen2.5-32B-Instruct"
EMBED_MODELFILE="$MODELS_DIR/Modelfile.nomic-embed-text-v1.5"
LLM_MODEL_NAME="qwen2.5-32b"
EMBED_MODEL_NAME="nomic-embed"

echo ""
print_header "  Ollama æ¨¡å‹éƒ¨ç½²å·¥å…·"
echo "  LLM: Qwen2.5-32B-Instruct (Q4_K_M, 5ä¸ªåˆ†ç‰‡)"
echo "  Embedding: nomic-embed-text-v1.5 (Q4_K_M)"
echo ""

# ============================================================
# [1/5] æ£€æŸ¥ Ollama å®‰è£…
# ============================================================
echo "[1/5] æ£€æŸ¥ Ollama å®‰è£…..."
if ! command -v ollama &> /dev/null; then
    print_error "æœªæ‰¾åˆ° Ollama å‘½ä»¤"
    echo ""
    echo "è¯·å…ˆå®‰è£… Ollama:"
    echo "  macOS/Linux: curl -fsSL https://ollama.ai/install.sh | sh"
    echo "  Windows: https://ollama.ai/download"
    echo ""
    exit 1
fi
print_success "Ollama å·²å®‰è£…"
echo ""

# ============================================================
# [2/5] æ£€æŸ¥ Modelfile æ–‡ä»¶
# ============================================================
echo "[2/5] æ£€æŸ¥ Modelfile æ–‡ä»¶..."
if [ ! -f "$LLM_MODELFILE" ]; then
    print_error "æœªæ‰¾åˆ° LLM Modelfile"
    echo "   è·¯å¾„: $LLM_MODELFILE"
    echo ""
    echo "è¯·å…ˆè¿è¡Œ download_models.py ä¸‹è½½æ¨¡å‹å¹¶ç”Ÿæˆ Modelfile"
    exit 1
fi
print_success "æ‰¾åˆ° LLM Modelfile"

if [ ! -f "$EMBED_MODELFILE" ]; then
    print_error "æœªæ‰¾åˆ°åµŒå…¥æ¨¡å‹ Modelfile"
    echo "   è·¯å¾„: $EMBED_MODELFILE"
    echo ""
    echo "è¯·å…ˆè¿è¡Œ download_models.py ä¸‹è½½æ¨¡å‹å¹¶ç”Ÿæˆ Modelfile"
    exit 1
fi
print_success "æ‰¾åˆ°åµŒå…¥æ¨¡å‹ Modelfile"
echo ""

# ============================================================
# [3/5] éªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
# ============================================================
echo "[3/5] éªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§..."
LLM_DIR="$MODELS_DIR/Qwen2.5-32B-Instruct"
EMBED_DIR="$MODELS_DIR/nomic-embed-text-v1.5"

# æ£€æŸ¥ LLM åˆ†ç‰‡æ–‡ä»¶
MISSING_SHARDS=0
for i in 00001 00002 00003 00004 00005; do
    SHARD_FILE="$LLM_DIR/qwen2.5-32b-instruct-q4_k_m-${i}-of-00005.gguf"
    if [ ! -f "$SHARD_FILE" ]; then
        print_error "ç¼ºå°‘åˆ†ç‰‡: qwen2.5-32b-instruct-q4_k_m-${i}-of-00005.gguf"
        MISSING_SHARDS=1
    else
        print_success "qwen2.5-32b-instruct-q4_k_m-${i}-of-00005.gguf"
    fi
done

if [ $MISSING_SHARDS -eq 1 ]; then
    echo ""
    print_error "LLM åˆ†ç‰‡æ–‡ä»¶ä¸å®Œæ•´"
    echo "è¯·é‡æ–°è¿è¡Œ download_models.py ä¸‹è½½æ‰€æœ‰åˆ†ç‰‡"
    exit 1
fi

# æ£€æŸ¥åµŒå…¥æ¨¡å‹æ–‡ä»¶
EMBED_FILE="$EMBED_DIR/nomic-embed-text-v1.5.Q4_K_M.gguf"
if [ ! -f "$EMBED_FILE" ]; then
    print_error "åµŒå…¥æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨"
    echo "   è·¯å¾„: $EMBED_FILE"
    exit 1
fi
print_success "nomic-embed-text-v1.5.Q4_K_M.gguf"
echo ""

# ============================================================
# [4/5] åˆ›å»º LLM æ¨¡å‹
# ============================================================
echo "[4/5] åˆ›å»º Ollama LLM æ¨¡å‹..."
echo "æ‰§è¡Œå‘½ä»¤: ollama create $LLM_MODEL_NAME -f \"$LLM_MODELFILE\""
echo ""

if ollama create "$LLM_MODEL_NAME" -f "$LLM_MODELFILE"; then
    echo ""
    print_success "LLM æ¨¡å‹åˆ›å»ºæˆåŠŸ: $LLM_MODEL_NAME"
else
    echo ""
    print_error "LLM æ¨¡å‹åˆ›å»ºå¤±è´¥"
    exit 1
fi
echo ""

# ============================================================
# [5/5] åˆ›å»ºåµŒå…¥æ¨¡å‹
# ============================================================
echo "[5/5] åˆ›å»º Ollama åµŒå…¥æ¨¡å‹..."
echo "æ‰§è¡Œå‘½ä»¤: ollama create $EMBED_MODEL_NAME -f \"$EMBED_MODELFILE\""
echo ""

if ollama create "$EMBED_MODEL_NAME" -f "$EMBED_MODELFILE"; then
    echo ""
    print_success "åµŒå…¥æ¨¡å‹åˆ›å»ºæˆåŠŸ: $EMBED_MODEL_NAME"
else
    echo ""
    print_error "åµŒå…¥æ¨¡å‹åˆ›å»ºå¤±è´¥"
    exit 1
fi
echo ""

# ============================================================
# æ˜¾ç¤ºå·²åˆ›å»ºçš„æ¨¡å‹
# ============================================================
print_header "  éƒ¨ç½²å®Œæˆ!"
echo ""
echo "ğŸ“‹ å·²åˆ›å»ºçš„æ¨¡å‹:"
ollama list
echo ""

# ============================================================
# æä¾›æµ‹è¯•å‘½ä»¤
# ============================================================
print_header "  æµ‹è¯•æ¨¡å‹"
echo ""
echo "1. æµ‹è¯• LLM æ¨¡å‹:"
echo "   ollama run $LLM_MODEL_NAME \"ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±\""
echo ""
echo "2. æµ‹è¯•åµŒå…¥æ¨¡å‹:"
echo "   ollama run $EMBED_MODEL_NAME"
echo ""
echo "3. åœ¨ LightRAG ä¸­ä½¿ç”¨:"
echo "   è®¾ç½®ç¯å¢ƒå˜é‡:"
echo "     export LLM_MODEL=$LLM_MODEL_NAME"
echo "     export EMBEDDING_MODEL=$EMBED_MODEL_NAME"
echo ""
echo "   ç„¶åè¿è¡Œ:"
echo "     cd /path/to/LightRAG"
echo "     python examples/lightrag_ollama_demo.py"
echo ""
print_header "============================================================"
