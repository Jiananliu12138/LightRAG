#!/usr/bin/env python3
"""
ä» Hugging Face ä¸‹è½½ GGUF æ ¼å¼æ¨¡å‹
LLM: Qwen2.5-32B-Instruct (Q4_K_M, ~19.8GB, 5 ä¸ªåˆ†ç‰‡æ–‡ä»¶)
Embedding: nomic-embed-text-v1.5 (Q4_K_M, ~100MB)
ä¸“ä¸º Ollama éƒ¨ç½²ä¼˜åŒ–

æ³¨æ„ï¼šLLM æ¨¡å‹ç”± 5 ä¸ªåˆ†ç‰‡æ–‡ä»¶ç»„æˆï¼Œå¿…é¡»ä¸‹è½½å…¨éƒ¨åˆ†ç‰‡æ‰èƒ½ä½¿ç”¨
"""

import os
from pathlib import Path
from huggingface_hub import snapshot_download, hf_hub_download

# è®¾ç½®ä¸‹è½½ç›®å½•
MODELS_DIR = Path(__file__).parent
os.makedirs(MODELS_DIR, exist_ok=True)

print(f"æ¨¡å‹å°†ä¸‹è½½åˆ°: {MODELS_DIR.absolute()}")
print("=" * 60)

# ============== æ¨¡å‹é…ç½®ï¼ˆGGUF Q4_K_M é‡åŒ–ï¼‰==============

# GGUF é‡åŒ–æ ¼å¼: ä½¿ç”¨ Q4_K_M (å¹³è¡¡è´¨é‡å’Œå¤§å°)
# LLM: Qwen2.5-32B-Instruct (~19.8GB, 5 ä¸ªåˆ†ç‰‡ï¼Œå¿…é¡»å…¨éƒ¨ä¸‹è½½)
# Embedding: nomic-embed-text-v1.5 (~100MB, å•æ–‡ä»¶)

LLM_CONFIG = {
    "name": "Qwen2.5-32B-Instruct",
    "repo_id": "Qwen/Qwen2.5-32B-Instruct-GGUF",
    "files": [  # åˆ†ç‰‡æ–‡ä»¶åˆ—è¡¨ (å¿…é¡»ä¸‹è½½å…¨éƒ¨ 5 ä¸ªåˆ†ç‰‡)
        "qwen2.5-32b-instruct-q4_k_m-00001-of-00005.gguf",  # ~3.96GB
        "qwen2.5-32b-instruct-q4_k_m-00002-of-00005.gguf",  # ~3.95GB
        "qwen2.5-32b-instruct-q4_k_m-00003-of-00005.gguf",  # ~3.99GB
        "qwen2.5-32b-instruct-q4_k_m-00004-of-00005.gguf",  # ~3.95GB
        "qwen2.5-32b-instruct-q4_k_m-00005-of-00005.gguf",  # æœ€åä¸€ä¸ªåˆ†ç‰‡
    ],
    "size": "~19.8GB (5 ä¸ªåˆ†ç‰‡ï¼Œç¼ºä¸€ä¸å¯)",
}

EMBEDDING_CONFIG = {
    "name": "nomic-embed-text-v1.5",
    "repo_id": "nomic-ai/nomic-embed-text-v1.5-GGUF",
    "file": "nomic-embed-text-v1.5.Q4_K_M.gguf",  # ~100MB, Q4_K_M é‡åŒ–
    "format": "gguf",  # GGUF æ ¼å¼
    "size": "~100MB",
    "dim": 768,
    "description": "è‹±æ–‡åµŒå…¥æ¨¡å‹ï¼ŒGGUF æ ¼å¼",
}


def download_llm_gguf():
    """ä¸‹è½½ LLM çš„ GGUF Q4_K_M é‡åŒ–ç‰ˆæœ¬ï¼ˆæ‰€æœ‰åˆ†ç‰‡æ–‡ä»¶ï¼‰"""
    print("\n" + "="*60)
    print(f"ä¸‹è½½è¯­è¨€æ¨¡å‹: {LLM_CONFIG['name']}")
    print("é‡åŒ–æ ¼å¼: Q4_K_M")
    print(f"æ–‡ä»¶å¤§å°: {LLM_CONFIG['size']}")
    print(f"âš ï¸  æ³¨æ„ï¼šéœ€è¦ä¸‹è½½å…¨éƒ¨ {len(LLM_CONFIG['files'])} ä¸ªåˆ†ç‰‡æ–‡ä»¶")
    print("="*60)
    
    local_dir = MODELS_DIR / LLM_CONFIG['name']
    os.makedirs(local_dir, exist_ok=True)
    
    print("\nå¼€å§‹ä¸‹è½½...")
    print(f"ä»“åº“: {LLM_CONFIG['repo_id']}")
    print(f"ä¿å­˜åˆ°: {local_dir}\n")
    
    try:
        downloaded_files = []
        
        # ä¸‹è½½æ‰€æœ‰åˆ†ç‰‡æ–‡ä»¶
        print(f"ğŸ“¥ ä¸‹è½½ {len(LLM_CONFIG['files'])} ä¸ªåˆ†ç‰‡æ–‡ä»¶:")
        print("   (æ¯ä¸ªåˆ†ç‰‡çº¦ 4GBï¼Œæ€»è®¡çº¦ 20GBï¼Œè¯·è€å¿ƒç­‰å¾…)\n")
        
        for i, filename in enumerate(LLM_CONFIG['files'], 1):
            print(f"[{i}/{len(LLM_CONFIG['files'])}] æ­£åœ¨ä¸‹è½½: {filename}")
            hf_hub_download(
                repo_id=LLM_CONFIG['repo_id'],
                filename=filename,
                local_dir=str(local_dir),
                local_dir_use_symlinks=False,
                resume_download=True,
            )
            downloaded_files.append(filename)
            print(f"    âœ… å·²ä¸‹è½½: {filename}\n")
        
        # ä¸‹è½½é…ç½®æ–‡ä»¶ï¼ˆå¯é€‰ä½†æ¨èï¼‰
        config_files = [
            "config.json",
            "tokenizer.json", 
            "tokenizer_config.json",
            "special_tokens_map.json",
        ]
        
        print("\nğŸ“¥ ä¸‹è½½é…ç½®æ–‡ä»¶...")
        for config_file in config_files:
            try:
                hf_hub_download(
                    repo_id=LLM_CONFIG['repo_id'],
                    filename=config_file,
                    local_dir=str(local_dir),
                    local_dir_use_symlinks=False,
                    resume_download=True,
                )
                downloaded_files.append(config_file)
                print(f"âœ… å·²ä¸‹è½½: {config_file}")
            except Exception:
                print(f"âš ï¸  è·³è¿‡: {config_file}")
        
        print(f"\nâœ… {LLM_CONFIG['name']} ä¸‹è½½å®Œæˆï¼")
        print("   é‡åŒ–æ ¼å¼: Q4_K_M")
        print(f"   åˆ†ç‰‡æ–‡ä»¶: {len(LLM_CONFIG['files'])} ä¸ª")
        print(f"   æ€»æ–‡ä»¶æ•°: {len(downloaded_files)} (åŒ…å«é…ç½®æ–‡ä»¶)")
        return True, local_dir, "Q4_K_M"
    
    except Exception as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, None, None


def download_embedding_model():
    """ä¸‹è½½åµŒå…¥æ¨¡å‹ï¼ˆGGUF Q4_K_M é‡åŒ–ç‰ˆæœ¬ï¼‰"""
    print("\n" + "="*60)
    print(f"ä¸‹è½½åµŒå…¥æ¨¡å‹: {EMBEDDING_CONFIG['name']}")
    print("é‡åŒ–æ ¼å¼: Q4_K_M")
    print("è¯­è¨€: è‹±æ–‡ (English)")
    print(f"ç»´åº¦: {EMBEDDING_CONFIG['dim']}, å¤§å°: {EMBEDDING_CONFIG['size']}")
    print(f"è¯´æ˜: {EMBEDDING_CONFIG['description']}")
    print("="*60)
    
    filename = EMBEDDING_CONFIG['file']
    local_dir = MODELS_DIR / EMBEDDING_CONFIG['name']
    os.makedirs(local_dir, exist_ok=True)
    
    print("\nå¼€å§‹ä¸‹è½½...")
    print(f"ä»“åº“: {EMBEDDING_CONFIG['repo_id']}")
    print(f"æ–‡ä»¶: {filename}")
    print(f"ä¿å­˜åˆ°: {local_dir}\n")
    
    try:
        # ä¸‹è½½å•ä¸ª GGUF æ–‡ä»¶
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½: {filename}")
        hf_hub_download(
            repo_id=EMBEDDING_CONFIG['repo_id'],
            filename=filename,
            local_dir=str(local_dir),
            local_dir_use_symlinks=False,
            resume_download=True,
        )
        print(f"âœ… å·²ä¸‹è½½: {filename}")
        
        print(f"\nâœ… {EMBEDDING_CONFIG['name']} ä¸‹è½½å®Œæˆï¼")
        print("   é‡åŒ–æ ¼å¼: Q4_K_M")
        return True, local_dir
    
    except Exception as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, None


def create_ollama_modelfile(llm_dir, embed_dir):
    """åˆ›å»º Ollama Modelfile é…ç½®"""
    print("\n" + "="*60)
    print("ç”Ÿæˆ Ollama Modelfile é…ç½®")
    print("="*60)
    
    # éªŒè¯æ‰€æœ‰åˆ†ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    print("\nğŸ” éªŒè¯ LLM åˆ†ç‰‡æ–‡ä»¶...")
    missing_files = []
    for shard_file in LLM_CONFIG['files']:
        shard_path = llm_dir / shard_file
        if shard_path.exists():
            print(f"   âœ… {shard_file}")
        else:
            print(f"   âŒ {shard_file} (ç¼ºå¤±)")
            missing_files.append(shard_file)
    
    if missing_files:
        print(f"\nâš ï¸  è­¦å‘Šï¼šç¼ºå°‘ {len(missing_files)} ä¸ªåˆ†ç‰‡æ–‡ä»¶ï¼Œæ¨¡å‹å¯èƒ½æ— æ³•æ­£å¸¸åŠ è½½ï¼")
        print("   è¯·é‡æ–°è¿è¡Œä¸‹è½½è„šæœ¬ä»¥è·å–æ‰€æœ‰åˆ†ç‰‡ã€‚")
    else:
        print("\nâœ… æ‰€æœ‰ 5 ä¸ªåˆ†ç‰‡æ–‡ä»¶å®Œæ•´ï¼")
    
    # ç”Ÿæˆ LLM Modelfile
    # æ³¨æ„ï¼šOllama é€šè¿‡æ–‡ä»¶åæ¨¡å¼è‡ªåŠ¨è¯†åˆ«åˆ†ç‰‡ï¼Œåªéœ€æŒ‡å‘ç¬¬ä¸€ä¸ªåˆ†ç‰‡
    llm_gguf_file = llm_dir / LLM_CONFIG['files'][0]
    llm_modelfile = MODELS_DIR / f"Modelfile.{LLM_CONFIG['name']}"
    
    llm_content = f'''# Ollama Modelfile for {LLM_CONFIG['name']}
#
# åˆ†ç‰‡æ¨¡å‹è¯´æ˜ï¼š
# æ­¤æ¨¡å‹ç”± 5 ä¸ªåˆ†ç‰‡æ–‡ä»¶ç»„æˆ (00001-of-00005 è‡³ 00005-of-00005)
# Ollama ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½åŒç›®å½•ä¸‹çš„æ‰€æœ‰åˆ†ç‰‡
# 
# åˆ†ç‰‡æ–‡ä»¶åˆ—è¡¨ï¼š
# - {LLM_CONFIG['files'][0]}
# - {LLM_CONFIG['files'][1]}
# - {LLM_CONFIG['files'][2]}
# - {LLM_CONFIG['files'][3]}
# - {LLM_CONFIG['files'][4]}
#
# âš ï¸ æ‰€æœ‰åˆ†ç‰‡å¿…é¡»åœ¨åŒä¸€ç›®å½•ï¼Œç¼ºä¸€ä¸å¯ï¼

FROM {llm_gguf_file.absolute()}

TEMPLATE """{{{{ if .System }}}}<|im_start|>system
{{{{ .System }}}}<|im_end|>
{{{{ end }}}}{{{{ if .Prompt }}}}<|im_start|>user
{{{{ .Prompt }}}}<|im_end|>
{{{{ end }}}}<|im_start|>assistant
{{{{ .Response }}}}<|im_end|>
"""

PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"
PARAMETER temperature 0.7
PARAMETER top_p 0.8
PARAMETER top_k 20
PARAMETER num_ctx 8192
'''
    
    with open(llm_modelfile, 'w', encoding='utf-8') as f:
        f.write(llm_content)
    
    print(f"\nâœ… å·²ç”Ÿæˆ: {llm_modelfile}")
    if missing_files:
        print("   âš ï¸  ä½†è¯·æ³¨æ„ï¼šå­˜åœ¨ç¼ºå¤±çš„åˆ†ç‰‡æ–‡ä»¶ï¼")
    
    # éªŒè¯åµŒå…¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    print("\nğŸ” éªŒè¯åµŒå…¥æ¨¡å‹æ–‡ä»¶...")
    embed_gguf_file = embed_dir / EMBEDDING_CONFIG['file']
    if embed_gguf_file.exists():
        print(f"   âœ… {EMBEDDING_CONFIG['file']}")
    else:
        print(f"   âŒ {EMBEDDING_CONFIG['file']} (ç¼ºå¤±)")
    
    # ç”ŸæˆåµŒå…¥æ¨¡å‹ Modelfileï¼ˆGGUF æ ¼å¼ï¼Œå•æ–‡ä»¶ï¼‰
    embed_modelfile = MODELS_DIR / f"Modelfile.{EMBEDDING_CONFIG['name']}"
    embed_content = f'''# Ollama Modelfile for {EMBEDDING_CONFIG['name']}
# 
# åµŒå…¥æ¨¡å‹è¯´æ˜ï¼š
# æ­¤æ¨¡å‹ä¸ºå•æ–‡ä»¶ GGUF æ ¼å¼ (Q4_K_M é‡åŒ–)
# æ–‡ä»¶: {EMBEDDING_CONFIG['file']}

FROM {embed_gguf_file.absolute()}

PARAMETER num_ctx 8192
'''
    
    with open(embed_modelfile, 'w', encoding='utf-8') as f:
        f.write(embed_content)
    
    print(f"\nâœ… å·²ç”Ÿæˆ: {embed_modelfile}")
    
    return llm_modelfile, embed_modelfile


def main():
    print("\n" + "="*60)
    print("  LightRAG + Ollama æ¨¡å‹ä¸‹è½½å·¥å…·")
    print("  Qwen2.5-32B + Nomic Embed (GGUF Q4_K_M)")
    print("="*60)
    print("\né…ç½®:")
    print(f"  - LLM: {LLM_CONFIG['name']} (Q4_K_M, {LLM_CONFIG['size']})")
    print(f"  - Embedding: {EMBEDDING_CONFIG['name']} (Q4_K_M, {EMBEDDING_CONFIG['size']})")
    print("="*60)
    
    # æ£€æŸ¥ä¾èµ–
    try:
        from huggingface_hub import snapshot_download  # noqa: F401
    except ImportError:
        print("\nâŒ ç¼ºå°‘ä¾èµ–ï¼Œè¯·å…ˆå®‰è£…:")
        print("pip install huggingface_hub")
        return
    
    results = []
    
    # ä¸‹è½½ LLM
    print("\nğŸ“¥ ç¬¬ 1 æ­¥: ä¸‹è½½è¯­è¨€æ¨¡å‹ (GGUF æ ¼å¼)")
    llm_success, llm_dir, llm_quant = download_llm_gguf()
    if llm_success:
        results.append(("LLM", LLM_CONFIG['name'], llm_dir, llm_quant))
    
    # ä¸‹è½½åµŒå…¥æ¨¡å‹
    print("\nğŸ“¥ ç¬¬ 2 æ­¥: ä¸‹è½½åµŒå…¥æ¨¡å‹")
    embed_success, embed_dir = download_embedding_model()
    if embed_success:
        results.append(("Embedding", EMBEDDING_CONFIG['name'], embed_dir, "Q4_K_M"))
    
    # ç”Ÿæˆ Modelfile
    if llm_success and embed_success:
        print("\nğŸ“ ç¬¬ 3 æ­¥: ç”Ÿæˆ Ollama é…ç½®æ–‡ä»¶")
        llm_modelfile, embed_modelfile = create_ollama_modelfile(llm_dir, embed_dir)
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("  ä¸‹è½½å®Œæˆæ±‡æ€»")
    print("="*60)
    
    if results:
        print("\nâœ… å·²ä¸‹è½½çš„æ¨¡å‹:")
        for model_type, name, path, quant in results:
            print(f"\n  [{model_type}] {name}")
            print(f"    è·¯å¾„: {path}")
            print(f"    æ ¼å¼: {quant}")
        
        if llm_success and embed_success:
            print("\n" + "="*60)
            print("ğŸ“‹ åç»­æ­¥éª¤ - ä½¿ç”¨ Ollama éƒ¨ç½²:")
            print("="*60)
            print("\n1. åˆ›å»º LLM æ¨¡å‹:")
            print(f"   ollama create qwen2.5-32b -f {llm_modelfile}")
            
            print("\n2. åˆ›å»ºåµŒå…¥æ¨¡å‹:")
            print(f"   ollama create nomic-embed -f {embed_modelfile}")
            
            print("\n3. æµ‹è¯•æ¨¡å‹:")
            print("   ollama run qwen2.5-32b \"Hello, please introduce yourself\"")
            
            print("\n4. ä¿®æ”¹ LightRAG é…ç½®ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹:")
            print("   LLM_MODEL=qwen2.5-32b")
            print("   EMBEDDING_MODEL=nomic-embed")
            
            print("\n5. è¿è¡Œ LightRAG demo:")
            print("   cd F:\\thesis\\LightRAG")
            print("   python examples\\lightrag_ollama_demo.py")
    else:
        print("\nâš ï¸  æ²¡æœ‰æˆåŠŸä¸‹è½½ä»»ä½•æ¨¡å‹")
    
    print("\n" + "="*60)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ä¸‹è½½å·²å–æ¶ˆ")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
