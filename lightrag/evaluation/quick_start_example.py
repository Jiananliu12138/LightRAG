#!/usr/bin/env python3
"""
å¿«é€Ÿå…¥é—¨ç¤ºä¾‹ï¼šè¯„ä¼° RAG ç³»ç»Ÿçš„åˆ†å—å’Œæ£€ç´¢æ€§èƒ½

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨è¯„ä¼°ç³»ç»Ÿæ¥è¯„ä¼°å’Œå¯¹æ¯”ä¸åŒçš„é…ç½®ã€‚
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from component_evaluators import (
    ChunkingEvaluator,
    RetrievalEvaluator
)


async def quick_start():
    """å¿«é€Ÿå…¥é—¨ç¤ºä¾‹"""
    
    print(f"\n{'='*80}")
    print("ğŸš€ RAG è¯„ä¼°ç³»ç»Ÿ - å¿«é€Ÿå…¥é—¨ç¤ºä¾‹")
    print(f"{'='*80}\n")
    
    # ========================================================================
    # ç¤ºä¾‹ 1: è¯„ä¼°åˆ†å—è´¨é‡
    # ========================================================================
    
    print("ğŸ“š ç¤ºä¾‹ 1: è¯„ä¼°åˆ†å—è´¨é‡\n")
    
    # å‡†å¤‡æµ‹è¯•æ–‡æ¡£
    sample_doc = """
    LightRAG is a Simple and Fast Retrieval-Augmented Generation framework. 
    The framework was developed by HKUDS (Hong Kong University Data Science Lab). 
    LightRAG provides developers with tools to build RAG applications efficiently.
    
    Large language models face several limitations. LLMs have a knowledge cutoff 
    date that prevents them from accessing recent information. Large language 
    models generate hallucinations when providing responses without factual grounding. 
    LLMs lack domain-specific expertise in specialized fields.
    
    LightRAG solves these problems by combining large language models with external 
    knowledge retrieval. The framework ensures accurate responses by grounding LLM 
    outputs in actual documents. LightRAG provides contextual responses that reduce 
    hallucinations significantly. The system enables efficient retrieval from external 
    knowledge bases to supplement LLM capabilities.
    """.strip()
    
    # æ–¹æ³• 1: æŒ‰å›ºå®šå¤§å°åˆ†å— (256 å­—ç¬¦, 50 å­—ç¬¦é‡å )
    def chunk_fixed_size(text, size=256, overlap=50):
        chunks = []
        start = 0
        while start < len(text):
            end = min(start + size, len(text))
            chunks.append(text[start:end])
            start += (size - overlap)
        return chunks
    
    # æ–¹æ³• 2: æŒ‰æ®µè½åˆ†å—
    def chunk_by_paragraph(text):
        return [p.strip() for p in text.split('\n\n') if p.strip()]
    
    # æ–¹æ³• 3: æŒ‰å¥å­åˆ†å— (æ¯ä¸ª chunk æœ€å¤š 3 å¥)
    def chunk_by_sentence(text, max_sentences=3):
        import re
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        chunks = []
        for i in range(0, len(sentences), max_sentences):
            chunk = '. '.join(sentences[i:i+max_sentences]) + '.'
            chunks.append(chunk)
        return chunks
    
    # åˆ›å»ºè¯„ä¼°å™¨
    chunking_evaluator = ChunkingEvaluator()
    
    # è¯„ä¼°ä¸‰ç§åˆ†å—æ–¹æ³•
    print("æ–¹æ³• 1: å›ºå®šå¤§å°åˆ†å— (256 å­—ç¬¦)")
    chunks1 = chunk_fixed_size(sample_doc, size=256, overlap=50)
    metrics1 = await chunking_evaluator.evaluate(sample_doc, chunks1)
    
    print("\næ–¹æ³• 2: æŒ‰æ®µè½åˆ†å—")
    chunks2 = chunk_by_paragraph(sample_doc)
    metrics2 = await chunking_evaluator.evaluate(sample_doc, chunks2)
    
    print("\næ–¹æ³• 3: æŒ‰å¥å­åˆ†å— (æ¯ 3 å¥)")
    chunks3 = chunk_by_sentence(sample_doc, max_sentences=3)
    metrics3 = await chunking_evaluator.evaluate(sample_doc, chunks3)
    
    # å¯¹æ¯”ç»“æœ
    print(f"\n{'='*80}")
    print("ğŸ“Š åˆ†å—æ–¹æ³•å¯¹æ¯”")
    print(f"{'='*80}")
    print(f"å›ºå®šå¤§å°åˆ†å—: {metrics1._overall_score():.2%}")
    print(f"æŒ‰æ®µè½åˆ†å—:   {metrics2._overall_score():.2%}")
    print(f"æŒ‰å¥å­åˆ†å—:   {metrics3._overall_score():.2%}")
    
    # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
    best_score = max(metrics1._overall_score(), metrics2._overall_score(), metrics3._overall_score())
    if best_score == metrics1._overall_score():
        print("\nğŸ† æœ€ä½³æ–¹æ³•: å›ºå®šå¤§å°åˆ†å—")
    elif best_score == metrics2._overall_score():
        print("\nğŸ† æœ€ä½³æ–¹æ³•: æŒ‰æ®µè½åˆ†å—")
    else:
        print("\nğŸ† æœ€ä½³æ–¹æ³•: æŒ‰å¥å­åˆ†å—")
    
    # ========================================================================
    # ç¤ºä¾‹ 2: è¯„ä¼°æ£€ç´¢è´¨é‡
    # ========================================================================
    
    print(f"\n\n{'='*80}")
    print("ğŸ“š ç¤ºä¾‹ 2: è¯„ä¼°æ£€ç´¢è´¨é‡")
    print(f"{'='*80}\n")
    
    # æ¨¡æ‹Ÿæ–‡æ¡£åº“
    mock_docs = {
        "doc1": "LightRAG is a Simple and Fast Retrieval-Augmented Generation framework",
        "doc2": "LightRAG was developed by HKUDS",
        "doc3": "Python is a high-level programming language",
        "doc4": "RAG systems combine retrieval and generation",
        "doc5": "Vector databases store embeddings efficiently",
        "doc6": "Knowledge graphs represent structured information",
        "doc7": "RAGAS is a framework for evaluating RAG systems",
    }
    
    # æ¨¡æ‹Ÿæ£€ç´¢å‡½æ•°
    async def mock_retrieval(query: str, top_k: int):
        """ç®€å•çš„å…³é”®è¯åŒ¹é…æ£€ç´¢"""
        query_words = set(query.lower().split())
        scores = {}
        for doc_id, doc_text in mock_docs.items():
            doc_words = set(doc_text.lower().split())
            overlap = len(query_words & doc_words)
            scores[doc_id] = overlap
        
        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return [doc_id for doc_id, score in sorted_docs[:top_k]]
    
    # åˆ›å»ºæµ‹è¯•æŸ¥è¯¢
    test_queries = [
        {
            "query": "What is LightRAG?",
            "relevant_docs": ["doc1", "doc2"],
            "relevance_scores": {"doc1": 1.0, "doc2": 0.8}
        },
        {
            "query": "How do RAG systems work?",
            "relevant_docs": ["doc1", "doc4", "doc7"],
            "relevance_scores": {"doc1": 0.6, "doc4": 1.0, "doc7": 0.9}
        },
        {
            "query": "Tell me about vector databases",
            "relevant_docs": ["doc5"],
            "relevance_scores": {"doc5": 1.0}
        },
    ]
    
    # åˆ›å»ºè¯„ä¼°å™¨
    retrieval_evaluator = RetrievalEvaluator(retrieval_func=mock_retrieval)
    
    # è¯„ä¼°æ£€ç´¢è´¨é‡
    print("è¯„ä¼°æ£€ç´¢æ€§èƒ½...\n")
    retrieval_metrics = await retrieval_evaluator.evaluate(
        test_queries=test_queries,
        k_values=[1, 3, 5]
    )
    
    # ========================================================================
    # æ€»ç»“
    # ========================================================================
    
    print(f"\n{'='*80}")
    print("âœ… å¿«é€Ÿå…¥é—¨ç¤ºä¾‹å®Œæˆï¼")
    print(f"{'='*80}\n")
    
    print("ğŸ“Œ å…³é”®è¦ç‚¹:")
    print("  1. åˆ†å—è¯„ä¼°å™¨å¯ä»¥å¸®åŠ©æ‚¨é€‰æ‹©æœ€ä½³çš„åˆ†å—ç­–ç•¥")
    print("  2. æ£€ç´¢è¯„ä¼°å™¨å¯ä»¥è¡¡é‡ä¸åŒæ£€ç´¢æ–¹æ³•çš„æ€§èƒ½")
    print("  3. ä½¿ç”¨è¯„ä¼°ç³»ç»Ÿå¯ä»¥ç³»ç»ŸåŒ–åœ°ä¼˜åŒ–æ‚¨çš„ RAG ç³»ç»Ÿ")
    
    print(f"\nğŸ“š ä¸‹ä¸€æ­¥:")
    print("  â€¢ é˜…è¯»å®Œæ•´æŒ‡å—: RAG_EVALUATION_GUIDE.md")
    print("  â€¢ è¿è¡Œå®Œæ•´è¯„ä¼°: python evaluate_lightrag_complete.py")
    print("  â€¢ è‡ªå®šä¹‰è¯„ä¼°æ•°æ®: ä¿®æ”¹æµ‹è¯•æ•°æ®ä»¥é€‚é…æ‚¨çš„åœºæ™¯")
    
    print(f"\n{'='*80}\n")


if __name__ == "__main__":
    asyncio.run(quick_start())
