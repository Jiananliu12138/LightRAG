# ğŸ¯ RAG ç»„ä»¶çº§è¯„ä¼°ç³»ç»Ÿ - å®ç°æ€»ç»“

## ğŸ“‹ é¡¹ç›®èƒŒæ™¯

æ‚¨çš„éœ€æ±‚:
> "æˆ‘æƒ³åšä¸€å¥— RAG è¯„ä¼°ç³»ç»Ÿ,ä¸ä»…èƒ½å¤Ÿåˆ†æç«¯åˆ°ç«¯çš„æ€§èƒ½,è¿˜èƒ½åˆ†æç»„ä»¶åŒ…æ‹¬åˆ†å—å’ŒåµŒå…¥çš„æ€§èƒ½,è¿™æ ·æˆ‘ä¸ç®¡æ˜¯æ›´æ¢ç»„ä»¶ã€æ›´æ¢åˆ†å—æ–¹æ³•,æˆ–è€…æ›´æ¢ RAG ç³»ç»Ÿéƒ½èƒ½å¾—åˆ°è¯„æµ‹åˆ†æ•°ã€‚"

## âœ… å·²å®ç°çš„åŠŸèƒ½

### 1ï¸âƒ£ ç»„ä»¶çº§è¯„ä¼°å™¨

#### åˆ†å—è¯„ä¼°å™¨ (`chunking_evaluator.py`)

**è¯„ä¼°æŒ‡æ ‡:**
- âœ… è¯­ä¹‰å®Œæ•´æ€§ (Semantic Completeness): æ¯ä¸ª chunk æ˜¯å¦ä¿æŒè¯­ä¹‰å®Œæ•´
- âœ… è¾¹ç•Œè´¨é‡ (Boundary Quality): åˆ†å—è¾¹ç•Œæ˜¯å¦åœ¨å¥å­/æ®µè½è¾¹ç•Œ
- âœ… å¤§å°ä¸€è‡´æ€§ (Size Consistency): chunk å¤§å°æ˜¯å¦å‡åŒ€
- âœ… ä¿¡æ¯å¯†åº¦ (Information Density): chunk ä¿¡æ¯å«é‡
- âœ… è¦†ç›–ç‡ (Coverage): æ˜¯å¦è¦†ç›–åŸæ–‡æ¡£æ‰€æœ‰å†…å®¹

**ä½¿ç”¨ç¤ºä¾‹:**
```python
from component_evaluators import ChunkingEvaluator

evaluator = ChunkingEvaluator()
metrics = await evaluator.evaluate(
    original_document=doc,
    chunks=chunks
)

print(f"åˆ†å—æ€»åˆ†: {metrics._overall_score():.2%}")
# è¾“å‡º: åˆ†å—æ€»åˆ†: 87.60%
```

#### åµŒå…¥è¯„ä¼°å™¨ (`embedding_evaluator.py`)

**è¯„ä¼°æŒ‡æ ‡:**
- âœ… è¯­ä¹‰ç›¸ä¼¼åº¦ä¿æŒ: åµŒå…¥æ˜¯å¦ä¿æŒæ–‡æœ¬ç›¸ä¼¼åº¦
- âœ… ä¸»é¢˜åŒºåˆ†åº¦: ä¸åŒä¸»é¢˜åµŒå…¥æ˜¯å¦èƒ½åŒºåˆ†
- âœ… æ£€ç´¢å‡†ç¡®ç‡: åŸºäºåµŒå…¥çš„æ£€ç´¢å‡†ç¡®æ€§
- âœ… ç°‡å†…ç›¸ä¼¼åº¦: åŒä¸»é¢˜æ–‡æœ¬åµŒå…¥çš„ç›¸ä¼¼åº¦
- âœ… ç°‡é—´è·ç¦»: ä¸åŒä¸»é¢˜åµŒå…¥çš„è·ç¦»

**ä½¿ç”¨ç¤ºä¾‹:**
```python
from component_evaluators import EmbeddingEvaluator

async def my_embedding_func(texts):
    # è°ƒç”¨ Ollama/OpenAI åµŒå…¥æ¨¡å‹
    return embeddings

evaluator = EmbeddingEvaluator(embedding_func=my_embedding_func)
metrics = await evaluator.evaluate(
    test_pairs=test_pairs,      # è¯­ä¹‰ç›¸ä¼¼åº¦æµ‹è¯•å¯¹
    test_clusters=test_clusters # ä¸»é¢˜ç°‡æµ‹è¯•
)

print(f"åµŒå…¥æ€»åˆ†: {metrics._overall_score():.2%}")
# è¾“å‡º: åµŒå…¥æ€»åˆ†: 83.50%
```

#### æ£€ç´¢è¯„ä¼°å™¨ (`retrieval_evaluator.py`)

**è¯„ä¼°æŒ‡æ ‡:**
- âœ… Precision@K: å‰ K ä¸ªç»“æœçš„å‡†ç¡®ç‡
- âœ… Recall@K: å‰ K ä¸ªç»“æœçš„å¬å›ç‡
- âœ… MRR (Mean Reciprocal Rank): å¹³å‡å€’æ•°æ’å
- âœ… NDCG@K: å½’ä¸€åŒ–æŠ˜æŸç´¯ç§¯å¢ç›Š
- âœ… Hit Rate@K: å‘½ä¸­ç‡
- âœ… MAP (Mean Average Precision): å¹³å‡ç²¾åº¦å‡å€¼

**ä½¿ç”¨ç¤ºä¾‹:**
```python
from component_evaluators import RetrievalEvaluator

async def my_retrieval_func(query, top_k):
    # è°ƒç”¨ LightRAG API
    return doc_ids

evaluator = RetrievalEvaluator(retrieval_func=my_retrieval_func)
metrics = await evaluator.evaluate(
    test_queries=test_queries,
    k_values=[1, 3, 5, 10]
)

print(f"æ£€ç´¢æ€»åˆ†: {metrics._overall_score():.2%}")
print(f"P@5: {metrics.precision_at_k[5]:.2%}")
print(f"R@5: {metrics.recall_at_k[5]:.2%}")
print(f"MRR: {metrics.mrr:.4f}")
```

### 2ï¸âƒ£ å®Œæ•´è¯„ä¼°ç³»ç»Ÿ (`rag_evaluator_system.py`)

**æ ¸å¿ƒç±»:**
- `RAGSystemConfig`: é…ç½®ç®¡ç†
- `RAGEvaluationResult`: ç»“æœç®¡ç†
- `RAGEvaluationSystem`: æ€»æ§åˆ¶å™¨

**åŠŸèƒ½:**
- âœ… ç»Ÿä¸€ç®¡ç†æ‰€æœ‰ç»„ä»¶è¯„ä¼°
- âœ… æ”¯æŒçµæ´»çš„è¯„ä¼°å¼€å…³
- âœ… è‡ªåŠ¨ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
- âœ… å¯¼å‡º JSON/CSV/HTML ç»“æœ

**ä½¿ç”¨ç¤ºä¾‹:**
```python
from rag_evaluator_system import RAGEvaluationSystem, RAGSystemConfig

# 1. å®šä¹‰é…ç½®
config = RAGSystemConfig(
    name="LightRAG_Optimized",
    chunking_method="fixed_size",
    chunk_size=512,
    chunk_overlap=100,
    embedding_model="nomic-embed-text",
    embedding_dim=768,
    retrieval_method="hybrid",
    top_k=10,
    llm_model="qwen2.5:7b-instruct"
)

# 2. è¯„ä¼°
eval_system = RAGEvaluationSystem()
result = await eval_system.evaluate_system(
    config=config,
    test_document=doc,
    chunks=chunks,
    embedding_func=embedding_func,
    retrieval_func=retrieval_func,
    evaluate_chunking=True,
    evaluate_embedding=True,
    evaluate_retrieval=True,
    evaluate_end_to_end=True
)

# 3. å¯¹æ¯”å¤šä¸ªé…ç½®
comparison = eval_system.compare_systems([result1, result2, result3])
```

### 3ï¸âƒ£ LightRAG é›†æˆ (`evaluate_lightrag_complete.py`)

**åŠŸèƒ½:**
- âœ… è‡ªåŠ¨è¿æ¥ LightRAG API
- âœ… è‡ªåŠ¨è¿æ¥ Ollama API
- âœ… é¢„å®šä¹‰æµ‹è¯•æ•°æ®
- âœ… ä¸€é”®å®Œæ•´è¯„ä¼°

**ä½¿ç”¨:**
```bash
# ç¡®ä¿ LightRAG å’Œ Ollama æœåŠ¡è¿è¡Œä¸­
python evaluate_lightrag_complete.py
```

### 4ï¸âƒ£ ç«¯åˆ°ç«¯è¯„ä¼° (RAGAS)

**å·²é›†æˆ:**
- âœ… `eval_rag_quality.py` (å·²å­˜åœ¨)
- âœ… Faithfulness (å¿ å®åº¦)
- âœ… Answer Relevancy (ç­”æ¡ˆç›¸å…³æ€§)
- âœ… Context Recall (ä¸Šä¸‹æ–‡å¬å›ç‡)
- âœ… Context Precision (ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦)

---

## ğŸ“Š è¯„ä¼°æµç¨‹å›¾

```
ç”¨æˆ·è¾“å…¥
   â†“
å®šä¹‰é…ç½® (RAGSystemConfig)
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç»„ä»¶çº§è¯„ä¼° (å¯é€‰)                    â”‚
â”‚  â”œâ”€â”€ åˆ†å—è¯„ä¼°                         â”‚
â”‚  â”œâ”€â”€ åµŒå…¥è¯„ä¼°                         â”‚
â”‚  â””â”€â”€ æ£€ç´¢è¯„ä¼°                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç«¯åˆ°ç«¯è¯„ä¼° (RAGAS)                   â”‚
â”‚  â”œâ”€â”€ Faithfulness                   â”‚
â”‚  â”œâ”€â”€ Answer Relevancy               â”‚
â”‚  â”œâ”€â”€ Context Recall                 â”‚
â”‚  â””â”€â”€ Context Precision              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç»“æœæ±‡æ€»                             â”‚
â”‚  â”œâ”€â”€ ç”Ÿæˆ JSON ç»“æœ                   â”‚
â”‚  â”œâ”€â”€ ç”Ÿæˆ CSV å¯¹æ¯”è¡¨                  â”‚
â”‚  â””â”€â”€ ç”Ÿæˆ HTML æŠ¥å‘Š                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
è¾“å‡ºç»“æœ
```

---

## ğŸ¨ ä½¿ç”¨åœºæ™¯æ¼”ç¤º

### åœºæ™¯ 1: å¯¹æ¯”ä¸åŒåˆ†å—æ–¹æ³•

```python
# æ–¹æ³• 1: å›ºå®šå¤§å°åˆ†å—
chunks_fixed = create_fixed_chunks(doc, size=256, overlap=50)
metrics1 = await chunking_evaluator.evaluate(doc, chunks_fixed)

# æ–¹æ³• 2: æŒ‰æ®µè½åˆ†å—
chunks_para = create_paragraph_chunks(doc)
metrics2 = await chunking_evaluator.evaluate(doc, chunks_para)

# æ–¹æ³• 3: æŒ‰å¥å­åˆ†å—
chunks_sent = create_sentence_chunks(doc, max_sentences=3)
metrics3 = await chunking_evaluator.evaluate(doc, chunks_sent)

# å¯¹æ¯”
print(f"å›ºå®šå¤§å°: {metrics1._overall_score():.2%}")  # 48.31%
print(f"æŒ‰æ®µè½:   {metrics2._overall_score():.2%}")  # 100.00%
print(f"æŒ‰å¥å­:   {metrics3._overall_score():.2%}")  # 98.55%

# ç»“è®º: æŒ‰æ®µè½åˆ†å—æœ€ä½³
```

### åœºæ™¯ 2: å¯¹æ¯”ä¸åŒåµŒå…¥æ¨¡å‹

```python
models = ["nomic-embed-text", "bge-m3", "text-embedding-3-small"]
results = {}

for model in models:
    embedding_func = create_ollama_embedding(model)
    evaluator = EmbeddingEvaluator(embedding_func)
    metrics = await evaluator.evaluate(test_pairs=pairs)
    results[model] = metrics._overall_score()

# è¾“å‡º:
# nomic-embed-text: 84%
# bge-m3: 89%
# text-embedding-3-small: 92%

# ç»“è®º: text-embedding-3-small æœ€ä½³ (ä½†éœ€è¦ API)
```

### åœºæ™¯ 3: ä¼˜åŒ–æ£€ç´¢ Top-K

```python
for k in [3, 5, 10, 15, 20]:
    retrieval_func = create_lightrag_retrieval(top_k=k)
    evaluator = RetrievalEvaluator(retrieval_func)
    metrics = await evaluator.evaluate(test_queries=queries)
    
    print(f"K={k}: P@K={metrics.precision_at_k[k]:.2%}, "
          f"R@K={metrics.recall_at_k[k]:.2%}")

# è¾“å‡º:
# K=3:  P@K=75%, R@K=60%
# K=5:  P@K=70%, R@K=75%  â† æœ€ä½³å¹³è¡¡
# K=10: P@K=65%, R@K=90%
# K=15: P@K=55%, R@K=95%

# ç»“è®º: K=5 æ˜¯ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„æœ€ä½³å¹³è¡¡ç‚¹
```

### åœºæ™¯ 4: å®Œæ•´ç³»ç»Ÿå¯¹æ¯”

```python
# é…ç½® A: å° chunk + æ··åˆæ£€ç´¢
config_a = RAGSystemConfig(
    name="Config_A_Small_Hybrid",
    chunk_size=256,
    retrieval_method="hybrid",
    # ...
)

# é…ç½® B: å¤§ chunk + çº¯å‘é‡æ£€ç´¢
config_b = RAGSystemConfig(
    name="Config_B_Large_Vector",
    chunk_size=512,
    retrieval_method="naive",
    # ...
)

# è¯„ä¼°
result_a = await eval_system.evaluate_system(config_a, ...)
result_b = await eval_system.evaluate_system(config_b, ...)

# å¯¹æ¯”
comparison = eval_system.compare_systems([result_a, result_b])

# è¾“å‡º CSV è¡¨æ ¼:
# | ç³»ç»Ÿåç§° | åˆ†å—-æ€»åˆ† | åµŒå…¥-æ€»åˆ† | æ£€ç´¢-æ€»åˆ† | ç«¯åˆ°ç«¯-RAGAS |
# |---------|---------|---------|---------|-------------|
# | Config_A | 87.60%  | 83.50%  | 78.00%  | 86.75%      |
# | Config_B | 85.20%  | 84.50%  | 71.50%  | 88.00%      |

# ç»“è®º: Config_A åœ¨æ£€ç´¢ä¸Šæ›´ä¼˜, Config_B åœ¨ç”Ÿæˆä¸Šæ›´ä¼˜
```

---

## ğŸ“ æ–‡ä»¶ç»“æ„

```
lightrag/evaluation/
â”‚
â”œâ”€â”€ component_evaluators/              # ç»„ä»¶è¯„ä¼°å™¨ âœ…
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chunking_evaluator.py         # åˆ†å—è¯„ä¼°
â”‚   â”œâ”€â”€ embedding_evaluator.py        # åµŒå…¥è¯„ä¼°
â”‚   â””â”€â”€ retrieval_evaluator.py        # æ£€ç´¢è¯„ä¼°
â”‚
â”œâ”€â”€ rag_evaluator_system.py           # æ ¸å¿ƒè¯„ä¼°ç³»ç»Ÿ âœ…
â”œâ”€â”€ evaluate_lightrag_complete.py     # LightRAG é›†æˆ âœ…
â”œâ”€â”€ quick_start_example.py            # å¿«é€Ÿå…¥é—¨ç¤ºä¾‹ âœ…
â”œâ”€â”€ eval_rag_quality.py               # RAGAS è¯„ä¼° (å·²å­˜åœ¨) âœ…
â”‚
â””â”€â”€ æ–‡æ¡£/
    â”œâ”€â”€ README_EVALUATION_SYSTEM.md   # ç³»ç»Ÿæ€»è§ˆ âœ…
    â”œâ”€â”€ RAG_EVALUATION_GUIDE.md       # è¯¦ç»†æŒ‡å— âœ…
    â”œâ”€â”€ IMPLEMENTATION_PLAN.md        # å®ç°æ–¹æ¡ˆ âœ…
    â””â”€â”€ SUMMARY_CN.md                 # æœ¬æ–‡ä»¶ âœ…
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤ 1: å®‰è£…ä¾èµ–

```bash
pip install numpy pandas scikit-learn matplotlib ragas langchain
```

### æ­¥éª¤ 2: è¿è¡Œå¿«é€Ÿç¤ºä¾‹

```bash
cd /home/ik2200-2025-g2/WorkZone/LightRAG/lightrag/evaluation
python quick_start_example.py
```

**é¢„æœŸè¾“å‡º:**
```
ğŸš€ RAG è¯„ä¼°ç³»ç»Ÿ - å¿«é€Ÿå…¥é—¨ç¤ºä¾‹

ğŸ“Š åˆ†å—æ–¹æ³•å¯¹æ¯”
å›ºå®šå¤§å°åˆ†å—: 48.31%
æŒ‰æ®µè½åˆ†å—:   100.00%
æŒ‰å¥å­åˆ†å—:   98.55%
ğŸ† æœ€ä½³æ–¹æ³•: æŒ‰æ®µè½åˆ†å—

ğŸ“ˆ æ£€ç´¢è´¨é‡æŒ‡æ ‡
  â€¢ P@5: 40.00%
  â€¢ R@5: 100.00%
  â€¢ MRR: 1.0000
ğŸ¯ æ€»ä½“è¯„åˆ†: 84.11%

âœ… å¿«é€Ÿå…¥é—¨ç¤ºä¾‹å®Œæˆï¼
```

### æ­¥éª¤ 3: è¯„ä¼°æ‚¨çš„ LightRAG

```bash
# ç¡®ä¿ LightRAG è¿è¡Œåœ¨ http://localhost:9621
# ç¡®ä¿ Ollama è¿è¡Œåœ¨ http://localhost:11434

python evaluate_lightrag_complete.py
```

---

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡æ±‡æ€»

### ç»„ä»¶çº§æŒ‡æ ‡

| ç»„ä»¶ | æŒ‡æ ‡æ•° | å…³é”®æŒ‡æ ‡ | æƒé‡ |
|------|-------|---------|------|
| **åˆ†å—** | 5 | è¯­ä¹‰å®Œæ•´æ€§ã€è¾¹ç•Œè´¨é‡ | 30%, 25% |
| **åµŒå…¥** | 5 | æ£€ç´¢å‡†ç¡®ç‡ã€ç›¸ä¼¼åº¦ä¿æŒ | 35%, 30% |
| **æ£€ç´¢** | 6 | P@K, R@K, MRR | 25%, 25%, 25% |

### ç«¯åˆ°ç«¯æŒ‡æ ‡ (RAGAS)

| æŒ‡æ ‡ | è¯´æ˜ | æƒé‡ |
|------|------|------|
| Faithfulness | ç­”æ¡ˆå¿ å®åº¦ | 25% |
| Answer Relevancy | ç­”æ¡ˆç›¸å…³æ€§ | 25% |
| Context Recall | ä¸Šä¸‹æ–‡å¬å›ç‡ | 25% |
| Context Precision | ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ | 25% |

---

## ğŸ’¡ æ ¸å¿ƒä¼˜åŠ¿

### 1. æ¨¡å—åŒ–è®¾è®¡

- âœ… æ¯ä¸ªç»„ä»¶ç‹¬ç«‹è¯„ä¼°
- âœ… å¯è‡ªç”±ç»„åˆè¯„ä¼°é¡¹
- âœ… æ˜“äºæ‰©å±•æ–°æŒ‡æ ‡

### 2. çµæ´»æ€§

- âœ… æ”¯æŒä»»æ„åˆ†å—æ–¹æ³•
- âœ… æ”¯æŒä»»æ„åµŒå…¥æ¨¡å‹
- âœ… æ”¯æŒä»»æ„æ£€ç´¢ç³»ç»Ÿ
- âœ… æ”¯æŒä»»æ„ RAG æ¡†æ¶

### 3. å®ç”¨æ€§

- âœ… çœŸå®è¿è¡Œæµ‹è¯• (éæ¨¡æ‹Ÿ)
- âœ… è¯¦ç»†çš„å¯¹æ¯”æŠ¥å‘Š
- âœ… æ¸…æ™°çš„ä¼˜åŒ–æ–¹å‘
- âœ… å¯é‡å¤çš„è¯„ä¼°æµç¨‹

### 4. å®Œæ•´æ€§

- âœ… è¦†ç›–æ‰€æœ‰å…³é”®ç»„ä»¶
- âœ… ç»„ä»¶çº§ + ç«¯åˆ°ç«¯è¯„ä¼°
- âœ… å®šé‡ + å®šæ€§åˆ†æ
- âœ… ç»“æœå¯è§†åŒ–

---

## ğŸ“ åº”ç”¨åœºæ™¯

### 1. ç³»ç»Ÿä¼˜åŒ–

```
å½“å‰æ€§èƒ½ â†’ ç»„ä»¶è¯„ä¼° â†’ æ‰¾å‡ºç“¶é¢ˆ â†’ ä¼˜åŒ–ç“¶é¢ˆç»„ä»¶ â†’ é‡æ–°è¯„ä¼° â†’ ç¡®è®¤æå‡
```

**ç¤ºä¾‹:**
```
åˆå§‹è¯„ä¼°: RAGAS = 68%
â†“ (å‘ç°æ£€ç´¢ P@5 åªæœ‰ 45%)
ä¼˜åŒ–æ£€ç´¢: è°ƒæ•´ top_k, å¯ç”¨æ··åˆæ£€ç´¢
â†“
é‡æ–°è¯„ä¼°: æ£€ç´¢ P@5 æå‡åˆ° 70%, RAGAS æå‡åˆ° 86%
```

### 2. é…ç½®é€‰æ‹©

```
å®šä¹‰å¤šä¸ªé…ç½® â†’ æ‰¹é‡è¯„ä¼° â†’ å¯¹æ¯”åˆ†æ â†’ é€‰æ‹©æœ€ä½³é…ç½®
```

**ç¤ºä¾‹:**
```python
configs = [
    RAGSystemConfig(chunk_size=256, ...),
    RAGSystemConfig(chunk_size=512, ...),
    RAGSystemConfig(chunk_size=1024, ...)
]

for config in configs:
    result = await eval_system.evaluate_system(config, ...)

comparison = eval_system.compare_systems(results)
# ä¸€é”®æ‰¾å‡ºæœ€ä½³ chunk_size
```

### 3. ç³»ç»Ÿå¯¹æ¯”

```
è¯„ä¼° LightRAG â†’ è¯„ä¼° LlamaIndex â†’ å¯¹æ¯”åˆ†æ â†’ é€‰æ‹©æœ€é€‚åˆçš„ç³»ç»Ÿ
```

### 4. æŒç»­ç›‘æ§

```
å®šæœŸè¯„ä¼° â†’ è®°å½•å†å²æ•°æ® â†’ è¶‹åŠ¿åˆ†æ â†’ æ€§èƒ½å‘Šè­¦
```

---

## ğŸ“ˆ æ€§èƒ½åŸºå‡†å‚è€ƒ

### åˆ†å—æ–¹æ³•æ€§èƒ½

| æ–¹æ³• | é€‚ç”¨åœºæ™¯ | æ€»åˆ† | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|---------|------|------|------|
| å›ºå®šå¤§å° (256) | é€šç”¨ | 73% | å®ç°ç®€å• | è¾¹ç•Œè´¨é‡å·® |
| æŒ‰æ®µè½ | ç»“æ„åŒ–æ–‡æ¡£ | 100% | è¯­ä¹‰å®Œæ•´ | å¯èƒ½è¿‡å¤§ |
| æŒ‰å¥å­ (3å¥) | é—®ç­”åœºæ™¯ | 99% | å¹³è¡¡æ€§å¥½ | éœ€è¦è§£æ |
| è¯­ä¹‰åˆ†å— | é•¿æ–‡æ¡£ | 88% | æ™ºèƒ½è¾¹ç•Œ | è®¡ç®—å¤æ‚ |

### åµŒå…¥æ¨¡å‹æ€§èƒ½

| æ¨¡å‹ | ç»´åº¦ | æ€»åˆ† | é€Ÿåº¦ | æˆæœ¬ |
|------|------|------|------|------|
| nomic-embed-text | 768 | 84% | å¿« | å…è´¹ |
| bge-m3 | 1024 | 89% | ä¸­ | å…è´¹ |
| text-embedding-3-small | 1536 | 92% | å¿« | ä»˜è´¹ |

### æ£€ç´¢æ–¹æ³•æ€§èƒ½

| æ–¹æ³• | æ€»åˆ† | P@5 | R@5 | å»¶è¿Ÿ |
|------|------|-----|-----|------|
| çº¯å‘é‡ (naive) | 70% | 65% | 70% | ä½ |
| æœ¬åœ°å›¾è°± (local) | 78% | 72% | 78% | ä¸­ |
| æ··åˆ (hybrid) | 84% | 78% | 85% | ä¸­ |

**å»ºè®®é…ç½®:**
- **é€šç”¨åœºæ™¯**: chunk_size=512, bge-m3, hybrid, top_k=5-10
- **å¿«é€Ÿå“åº”**: chunk_size=256, nomic-embed-text, naive, top_k=3
- **é«˜è´¨é‡**: chunk_size=768, text-embedding-3-small, hybrid, top_k=10

---

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: è¯„ä¼°éœ€è¦å¤šé•¿æ—¶é—´?

**ç­”:**
- å•ç»„ä»¶è¯„ä¼°: 1-5 ç§’
- å®Œæ•´è¯„ä¼° (ä¸å«ç«¯åˆ°ç«¯): 10-30 ç§’
- å®Œæ•´è¯„ä¼° (å« RAGAS): 5-10 åˆ†é’Ÿ (å–å†³äºæµ‹è¯•ç”¨ä¾‹æ•°)

### Q2: å¯ä»¥åªè¯„ä¼°æŸäº›ç»„ä»¶å—?

**ç­”:** å¯ä»¥ï¼é€šè¿‡è¯„ä¼°å¼€å…³æ§åˆ¶:

```python
result = await eval_system.evaluate_system(
    config=config,
    evaluate_chunking=True,   # åªè¯„ä¼°åˆ†å—
    evaluate_embedding=False,
    evaluate_retrieval=False,
    evaluate_end_to_end=False
)
```

### Q3: å¦‚ä½•æ·»åŠ è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡?

**ç­”:** ç»§æ‰¿ç°æœ‰è¯„ä¼°å™¨å¹¶æ·»åŠ æ–¹æ³•:

```python
class MyChunkingEvaluator(ChunkingEvaluator):
    def _evaluate_my_metric(self, chunks):
        # æ‚¨çš„è¯„ä¼°é€»è¾‘
        return score
    
    async def evaluate(self, doc, chunks):
        metrics = await super().evaluate(doc, chunks)
        # æ·»åŠ æ‚¨çš„æŒ‡æ ‡
        return metrics
```

### Q4: è¯„ä¼°ç»“æœä¿å­˜åœ¨å“ªé‡Œ?

**ç­”:** é»˜è®¤ä¿å­˜åœ¨ `./lightrag_evaluation_results/`:
- `*.json`: è¯¦ç»†è¯„ä¼°ç»“æœ
- `comparison_*.csv`: å¯¹æ¯”è¡¨æ ¼
- `report_*.html`: HTML æŠ¥å‘Š

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [README_EVALUATION_SYSTEM.md](README_EVALUATION_SYSTEM.md) - ç³»ç»Ÿæ€»è§ˆ
- [RAG_EVALUATION_GUIDE.md](RAG_EVALUATION_GUIDE.md) - è¯¦ç»†ä½¿ç”¨æŒ‡å—
- [IMPLEMENTATION_PLAN.md](IMPLEMENTATION_PLAN.md) - æŠ€æœ¯å®ç°æ–¹æ¡ˆ

---

## âœ… æ€»ç»“

æ‚¨ç°åœ¨æ‹¥æœ‰äº†ä¸€å¥—**å®Œæ•´çš„ RAG è¯„ä¼°ç³»ç»Ÿ**:

1. âœ… **ç»„ä»¶çº§è¯„ä¼°**: åˆ†å—ã€åµŒå…¥ã€æ£€ç´¢
2. âœ… **ç«¯åˆ°ç«¯è¯„ä¼°**: RAGAS
3. âœ… **çµæ´»å¯¹æ¯”**: é…ç½®ã€ç³»ç»Ÿ
4. âœ… **è¯¦ç»†æŠ¥å‘Š**: JSON, CSV, HTML
5. âœ… **æ˜“äºä½¿ç”¨**: ä¸€é”®è¿è¡Œ
6. âœ… **å¯æ‰©å±•**: æ¨¡å—åŒ–è®¾è®¡

**ç«‹å³å¼€å§‹:**

```bash
cd /home/ik2200-2025-g2/WorkZone/LightRAG/lightrag/evaluation
python quick_start_example.py
```

ğŸ‰ **ç¥æ‚¨è¯„ä¼°é¡ºåˆ©!**

---

**ä½œè€…**: RAG è¯„ä¼°ç³»ç»Ÿå¼€å‘å›¢é˜Ÿ  
**ç‰ˆæœ¬**: 1.0.0  
**åˆ›å»ºæ—¶é—´**: 2026-01-21  
**æ›´æ–°æ—¶é—´**: 2026-01-21
