# ğŸ¯ RAG è¯„ä¼°ç³»ç»Ÿå®Œæ•´æŒ‡å—

## ğŸ“š ç›®å½•

1. [ç³»ç»Ÿæ¦‚è¿°](#ç³»ç»Ÿæ¦‚è¿°)
2. [è¯„ä¼°æŒ‡æ ‡è¯´æ˜](#è¯„ä¼°æŒ‡æ ‡è¯´æ˜)
3. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
4. [ç»„ä»¶çº§è¯„ä¼°](#ç»„ä»¶çº§è¯„ä¼°)
5. [ç«¯åˆ°ç«¯è¯„ä¼°](#ç«¯åˆ°ç«¯è¯„ä¼°)
6. [ç³»ç»Ÿå¯¹æ¯”](#ç³»ç»Ÿå¯¹æ¯”)
7. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## ç³»ç»Ÿæ¦‚è¿°

### æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  RAG è¯„ä¼°ç³»ç»Ÿ (RAG Evaluator)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 1. åˆ†å—è¯„ä¼°    â”‚  â”‚ 2. åµŒå…¥è¯„ä¼°    â”‚  â”‚ 3. æ£€ç´¢è¯„ä¼°    â”‚   â”‚
â”‚  â”‚ (Chunking)    â”‚  â”‚ (Embedding)   â”‚  â”‚ (Retrieval)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 4. ç”Ÿæˆè¯„ä¼°    â”‚  â”‚ 5. ç«¯åˆ°ç«¯è¯„ä¼° (RAGAS)            â”‚   â”‚
â”‚  â”‚ (Generation)  â”‚  â”‚ (Faithfulness, Relevancy...)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 6. å¯¹æ¯”åˆ†æ (Compare Configs/Systems)               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒåŠŸèƒ½

âœ… **ç»„ä»¶çº§è¯„ä¼°**
- åˆ†å—è´¨é‡è¯„ä¼°
- åµŒå…¥è´¨é‡è¯„ä¼°
- æ£€ç´¢æ€§èƒ½è¯„ä¼°
- ç”Ÿæˆè´¨é‡è¯„ä¼°

âœ… **ç«¯åˆ°ç«¯è¯„ä¼°**
- ä½¿ç”¨ RAGAS æ¡†æ¶
- è¯„ä¼°æ•´ä½“ç³»ç»Ÿæ€§èƒ½

âœ… **å¯¹æ¯”åˆ†æ**
- å¤šé…ç½®å¯¹æ¯”
- å¤šç³»ç»Ÿå¯¹æ¯”
- æœ€ä½³å®è·µæ¨è

---

## è¯„ä¼°æŒ‡æ ‡è¯´æ˜

### 1ï¸âƒ£ åˆ†å—è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | èŒƒå›´ | æœ€ä½³å€¼ |
|------|------|------|--------|
| **è¯­ä¹‰å®Œæ•´æ€§** | æ¯ä¸ª chunk æ˜¯å¦ä¿æŒè¯­ä¹‰å®Œæ•´ | 0-1 | è¶Šé«˜è¶Šå¥½ |
| **è¾¹ç•Œè´¨é‡** | åˆ†å—è¾¹ç•Œæ˜¯å¦åˆç†ï¼ˆå¥å­/æ®µè½è¾¹ç•Œï¼‰ | 0-1 | è¶Šé«˜è¶Šå¥½ |
| **å¤§å°ä¸€è‡´æ€§** | chunk å¤§å°æ˜¯å¦å‡åŒ€ | 0-1 | è¶Šé«˜è¶Šå¥½ |
| **ä¿¡æ¯å¯†åº¦** | chunk æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯ | 0-1 | è¶Šé«˜è¶Šå¥½ |
| **è¦†ç›–ç‡** | chunks æ˜¯å¦è¦†ç›–äº†åŸæ–‡æ¡£çš„æ‰€æœ‰å†…å®¹ | 0-1 | è¶Šé«˜è¶Šå¥½ |

**ç¤ºä¾‹è¾“å‡ºï¼š**
```
ğŸ“ˆ åˆ†å—è´¨é‡æŒ‡æ ‡:
  â€¢ è¯­ä¹‰å®Œæ•´æ€§ (Semantic Completeness): 85.00%
  â€¢ è¾¹ç•Œè´¨é‡ (Boundary Quality):       92.00%
  â€¢ å¤§å°ä¸€è‡´æ€§ (Size Consistency):     78.00%
  â€¢ ä¿¡æ¯å¯†åº¦ (Information Density):    88.00%
  â€¢ è¦†ç›–ç‡ (Coverage):                 95.00%

ğŸ¯ æ€»ä½“è¯„åˆ†: 87.60%
```

---

### 2ï¸âƒ£ åµŒå…¥è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | èŒƒå›´ | æœ€ä½³å€¼ |
|------|------|------|--------|
| **è¯­ä¹‰ç›¸ä¼¼åº¦ä¿æŒ** | åµŒå…¥ç›¸ä¼¼åº¦ä¸äººå·¥æ ‡æ³¨ç›¸ä¼¼åº¦çš„ç›¸å…³æ€§ | 0-1 | è¶Šé«˜è¶Šå¥½ |
| **ä¸»é¢˜åŒºåˆ†åº¦** | ä¸åŒä¸»é¢˜çš„åµŒå…¥æ˜¯å¦èƒ½å¤ŸåŒºåˆ† | 0-1 | è¶Šé«˜è¶Šå¥½ |
| **æ£€ç´¢å‡†ç¡®ç‡** | åŸºäºåµŒå…¥çš„æ£€ç´¢å‡†ç¡®ç‡ | 0-1 | è¶Šé«˜è¶Šå¥½ |
| **ç°‡å†…ç›¸ä¼¼åº¦** | åŒä¸»é¢˜æ–‡æœ¬åµŒå…¥çš„ç›¸ä¼¼åº¦ | 0-1 | è¶Šé«˜è¶Šå¥½ |
| **ç°‡é—´è·ç¦»** | ä¸åŒä¸»é¢˜æ–‡æœ¬åµŒå…¥çš„è·ç¦» | 0-1 | è¶Šé«˜è¶Šå¥½ |

**ç¤ºä¾‹è¾“å‡ºï¼š**
```
ğŸ“ˆ åµŒå…¥è´¨é‡æŒ‡æ ‡:
  â€¢ è¯­ä¹‰ç›¸ä¼¼åº¦ä¿æŒ: 82.00%
  â€¢ ä¸»é¢˜åŒºåˆ†åº¦:     78.00%
  â€¢ æ£€ç´¢å‡†ç¡®ç‡:     90.00%
  â€¢ ç°‡å†…ç›¸ä¼¼åº¦:     85.00%
  â€¢ ç°‡é—´è·ç¦»:       72.00%

ğŸ¯ æ€»ä½“è¯„åˆ†: 83.50%
```

---

### 3ï¸âƒ£ æ£€ç´¢è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | å…¬å¼ | æœ€ä½³å€¼ |
|------|------|------|--------|
| **Precision@K** | å‰Kä¸ªç»“æœä¸­ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹ | TP / (TP + FP) | è¶Šé«˜è¶Šå¥½ |
| **Recall@K** | å‰Kä¸ªç»“æœè¦†ç›–çš„ç›¸å…³æ–‡æ¡£æ¯”ä¾‹ | TP / (TP + FN) | è¶Šé«˜è¶Šå¥½ |
| **MRR** | ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„å¹³å‡å€’æ•°æ’å | 1/rank | è¶Šé«˜è¶Šå¥½ |
| **NDCG@K** | å½’ä¸€åŒ–æŠ˜æŸç´¯ç§¯å¢ç›Š | DCG / IDCG | è¶Šé«˜è¶Šå¥½ |
| **Hit Rate@K** | å‰Kä¸ªç»“æœä¸­æ˜¯å¦åŒ…å«ç›¸å…³æ–‡æ¡£ | 0 æˆ– 1 | è¶Šé«˜è¶Šå¥½ |
| **MAP** | å¹³å‡ç²¾åº¦å‡å€¼ | - | è¶Šé«˜è¶Šå¥½ |

**ç¤ºä¾‹è¾“å‡ºï¼š**
```
ğŸ“ˆ æ£€ç´¢è´¨é‡æŒ‡æ ‡

ğŸ“Š Precision@K:
  â€¢ P@ 1: 80.00%
  â€¢ P@ 3: 75.00%
  â€¢ P@ 5: 70.00%
  â€¢ P@10: 65.00%

ğŸ“Š Recall@K:
  â€¢ R@ 1: 40.00%
  â€¢ R@ 3: 60.00%
  â€¢ R@ 5: 75.00%
  â€¢ R@10: 90.00%

ğŸ“Š å…¶ä»–æŒ‡æ ‡:
  â€¢ MRR (Mean Reciprocal Rank): 0.8500
  â€¢ MAP (Mean Average Precision): 0.7800

ğŸ¯ æ€»ä½“è¯„åˆ†: 73.75%
```

---

### 4ï¸âƒ£ ç«¯åˆ°ç«¯è¯„ä¼°æŒ‡æ ‡ï¼ˆRAGASï¼‰

| æŒ‡æ ‡ | è¯´æ˜ | è¯„ä¼°å†…å®¹ | æœ€ä½³å€¼ |
|------|------|----------|--------|
| **Faithfulness** | ç­”æ¡ˆçš„å¿ å®åº¦ | ç­”æ¡ˆæ˜¯å¦åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ | è¶Šé«˜è¶Šå¥½ |
| **Answer Relevancy** | ç­”æ¡ˆçš„ç›¸å…³æ€§ | ç­”æ¡ˆæ˜¯å¦å›ç­”äº†é—®é¢˜ | è¶Šé«˜è¶Šå¥½ |
| **Context Recall** | ä¸Šä¸‹æ–‡å¬å›ç‡ | æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ˜¯å¦åŒ…å«ç­”æ¡ˆæ‰€éœ€ä¿¡æ¯ | è¶Šé«˜è¶Šå¥½ |
| **Context Precision** | ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ | æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ˜¯å¦éƒ½æ˜¯ç›¸å…³çš„ | è¶Šé«˜è¶Šå¥½ |

**ç¤ºä¾‹è¾“å‡ºï¼š**
```
ğŸ“Š EVALUATION RESULTS SUMMARY

# | Question | Faith | AnswRel | CtxRec | CtxPrec | RAGAS | Status
------------------------------------------------------------------------
1 | How does LightRAG solve...  | 1.0000 | 0.8694 | 1.0000 | 1.0000 | 0.9674 | âœ“
2 | What are the three main...  | 0.4000 | 0.7631 | 1.0000 | 1.0000 | 0.7908 | âœ“
3 | How does LightRAG's...      | 1.0000 | 0.7778 | 1.0000 | 1.0000 | 0.9444 | âœ“

Average RAGAS Score: 0.8675
```

---

## å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
# å®‰è£…è¯„ä¼°ç³»ç»Ÿä¾èµ–
pip install numpy pandas scikit-learn matplotlib ragas langchain

# ç¡®ä¿ LightRAG æœåŠ¡è¿è¡Œä¸­
# ç«¯å£: 9621

# ç¡®ä¿ Ollama æœåŠ¡è¿è¡Œä¸­
# ç«¯å£: 11434
```

### åŸºç¡€è¯„ä¼°æµç¨‹

#### 1. è¯„ä¼°å•ä¸ªç»„ä»¶

```python
import asyncio
from pathlib import Path
from component_evaluators import ChunkingEvaluator

async def evaluate_chunking():
    # è¯»å–æµ‹è¯•æ–‡æ¡£
    doc = Path("sample_documents/01_lightrag_overview.md").read_text()
    
    # åˆ›å»ºåˆ†å—
    chunks = [doc[i:i+256] for i in range(0, len(doc), 206)]  # overlap=50
    
    # è¯„ä¼°
    evaluator = ChunkingEvaluator()
    metrics = await evaluator.evaluate(
        original_document=doc,
        chunks=chunks
    )
    
    print(f"åˆ†å—æ€»åˆ†: {metrics._overall_score():.2%}")

asyncio.run(evaluate_chunking())
```

#### 2. å®Œæ•´ç³»ç»Ÿè¯„ä¼°

```bash
# è¿è¡Œå®Œæ•´è¯„ä¼°
python evaluate_lightrag_complete.py
```

**è¾“å‡ºç»“æœï¼š**
```
ğŸš€ LightRAG å®Œæ•´è¯„ä¼°ç³»ç»Ÿ
================================================================================

ğŸ“Š æ­¥éª¤ 1/4: åˆ†å—è¯„ä¼°
ğŸ“Š æ­¥éª¤ 2/4: åµŒå…¥è¯„ä¼°
ğŸ“Š æ­¥éª¤ 3/4: æ£€ç´¢è¯„ä¼°
ğŸ“Š æ­¥éª¤ 4/4: ç«¯åˆ°ç«¯è¯„ä¼° (RAGAS)

================================================================================
ğŸ“Š ç³»ç»Ÿå¯¹æ¯”åˆ†æ (2 ä¸ªç³»ç»Ÿ)
================================================================================

| ç³»ç»Ÿåç§°                    | åˆ†å—-æ€»åˆ† | åµŒå…¥-æ€»åˆ† | æ£€ç´¢-æ€»åˆ† | ç«¯åˆ°ç«¯-RAGAS |
|----------------------------|----------|----------|----------|--------------|
| LightRAG_Small_Chunks_256  | 0.8760   | 0.8350   | 0.7375   | 0.8675       |
| LightRAG_Large_Chunks_512  | 0.8520   | 0.8450   | 0.7150   | 0.8800       |

ğŸ† æœ€ä½³ç³»ç»Ÿ
================================================================================
  â€¢ åˆ†å—è´¨é‡æœ€ä½³: LightRAG_Small_Chunks_256 (87.60%)
  â€¢ åµŒå…¥è´¨é‡æœ€ä½³: LightRAG_Large_Chunks_512 (84.50%)
  â€¢ æ£€ç´¢è´¨é‡æœ€ä½³: LightRAG_Small_Chunks_256 (73.75%)
  â€¢ ç«¯åˆ°ç«¯æœ€ä½³: LightRAG_Large_Chunks_512 (88.00%)

âœ… è¯„ä¼°å®Œæˆï¼
ğŸ“ ç»“æœä¿å­˜åœ¨: ./lightrag_evaluation_results
```

---

## ç»„ä»¶çº§è¯„ä¼°

### åˆ†å—è¯„ä¼°è¯¦ç»†ç¤ºä¾‹

```python
from component_evaluators import ChunkingEvaluator

async def compare_chunking_methods():
    doc = "..." # æ‚¨çš„æ–‡æ¡£
    
    # æ–¹æ³•1: å›ºå®šå¤§å°åˆ†å—
    chunks_fixed = create_fixed_size_chunks(doc, size=256, overlap=50)
    
    # æ–¹æ³•2: å¥å­è¾¹ç•Œåˆ†å—
    chunks_sentence = create_sentence_chunks(doc, max_size=300)
    
    # æ–¹æ³•3: è¯­ä¹‰åˆ†å—
    chunks_semantic = create_semantic_chunks(doc)
    
    evaluator = ChunkingEvaluator()
    
    # è¯„ä¼°ä¸‰ç§æ–¹æ³•
    metrics1 = await evaluator.evaluate(doc, chunks_fixed)
    metrics2 = await evaluator.evaluate(doc, chunks_sentence)
    metrics3 = await evaluator.evaluate(doc, chunks_semantic)
    
    # å¯¹æ¯”
    print(f"å›ºå®šå¤§å°: {metrics1._overall_score():.2%}")
    print(f"å¥å­è¾¹ç•Œ: {metrics2._overall_score():.2%}")
    print(f"è¯­ä¹‰åˆ†å—: {metrics3._overall_score():.2%}")
```

### åµŒå…¥è¯„ä¼°è¯¦ç»†ç¤ºä¾‹

```python
from component_evaluators import EmbeddingEvaluator
import numpy as np

async def compare_embedding_models():
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_pairs = [
        ("LightRAG is a RAG framework", "LightRAG is a retrieval system", 0.9),
        ("Python is a language", "Java is a language", 0.8),
        # ... æ›´å¤šæµ‹è¯•å¯¹
    ]
    
    # åµŒå…¥æ¨¡å‹1: nomic-embed-text
    async def embed_nomic(texts):
        # è°ƒç”¨ Ollama API
        ...
    
    # åµŒå…¥æ¨¡å‹2: bge-m3
    async def embed_bge(texts):
        # è°ƒç”¨ Ollama API
        ...
    
    # è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹
    eval1 = EmbeddingEvaluator(embedding_func=embed_nomic)
    eval2 = EmbeddingEvaluator(embedding_func=embed_bge)
    
    metrics1 = await eval1.evaluate(test_pairs=test_pairs)
    metrics2 = await eval2.evaluate(test_pairs=test_pairs)
    
    print(f"nomic-embed-text: {metrics1._overall_score():.2%}")
    print(f"bge-m3:           {metrics2._overall_score():.2%}")
```

### æ£€ç´¢è¯„ä¼°è¯¦ç»†ç¤ºä¾‹

```python
from component_evaluators import RetrievalEvaluator

async def compare_retrieval_methods():
    # åˆ›å»ºæµ‹è¯•æŸ¥è¯¢
    test_queries = [
        {
            "query": "What is LightRAG?",
            "relevant_docs": ["doc1", "doc2"],
            "relevance_scores": {"doc1": 1.0, "doc2": 0.8}
        },
        # ... æ›´å¤šæŸ¥è¯¢
    ]
    
    # æ£€ç´¢æ–¹æ³•1: çº¯å‘é‡æ£€ç´¢
    async def retrieve_vector(query, top_k):
        # è°ƒç”¨ LightRAG API (mode="naive")
        ...
    
    # æ£€ç´¢æ–¹æ³•2: æ··åˆæ£€ç´¢
    async def retrieve_hybrid(query, top_k):
        # è°ƒç”¨ LightRAG API (mode="hybrid")
        ...
    
    # è¯„ä¼°ä¸¤ç§æ–¹æ³•
    eval1 = RetrievalEvaluator(retrieval_func=retrieve_vector)
    eval2 = RetrievalEvaluator(retrieval_func=retrieve_hybrid)
    
    metrics1 = await eval1.evaluate(test_queries=test_queries)
    metrics2 = await eval2.evaluate(test_queries=test_queries)
    
    print(f"å‘é‡æ£€ç´¢: MRR={metrics1.mrr:.4f}, P@5={metrics1.precision_at_k[5]:.2%}")
    print(f"æ··åˆæ£€ç´¢: MRR={metrics2.mrr:.4f}, P@5={metrics2.precision_at_k[5]:.2%}")
```

---

## ç«¯åˆ°ç«¯è¯„ä¼°

### ä½¿ç”¨ RAGAS è¯„ä¼°

```bash
# ç¡®ä¿ LightRAG æœåŠ¡è¿è¡Œä¸­
# ç¡®ä¿å·²æ’å…¥æµ‹è¯•æ–‡æ¡£

# è¿è¡Œç«¯åˆ°ç«¯è¯„ä¼°
cd /home/ik2200-2025-g2/WorkZone/LightRAG/lightrag/evaluation
python eval_rag_quality.py
```

### è‡ªå®šä¹‰è¯„ä¼°æ•°æ®é›†

```json
// custom_dataset.json
[
  {
    "question": "æ‚¨çš„é—®é¢˜?",
    "ground_truth": "æ ‡å‡†ç­”æ¡ˆ"
  },
  {
    "question": "å¦ä¸€ä¸ªé—®é¢˜?",
    "ground_truth": "å¦ä¸€ä¸ªæ ‡å‡†ç­”æ¡ˆ"
  }
]
```

```bash
# ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†
python eval_rag_quality.py --dataset custom_dataset.json
```

---

## ç³»ç»Ÿå¯¹æ¯”

### å¯¹æ¯”ä¸åŒé…ç½®

```python
from rag_evaluator_system import RAGEvaluationSystem, RAGSystemConfig

async def compare_configs():
    eval_system = RAGEvaluationSystem()
    
    # é…ç½®1: å° chunk
    config1 = RAGSystemConfig(
        name="Small_Chunks",
        chunking_method="fixed_size",
        chunk_size=256,
        chunk_overlap=50,
        embedding_model="nomic-embed-text",
        embedding_dim=768,
        retrieval_method="hybrid",
        top_k=10,
        llm_model="qwen2.5:7b-instruct"
    )
    
    # é…ç½®2: å¤§ chunk
    config2 = RAGSystemConfig(
        name="Large_Chunks",
        chunking_method="fixed_size",
        chunk_size=512,
        chunk_overlap=100,
        # ... å…¶ä»–å‚æ•°ç›¸åŒ
    )
    
    # è¯„ä¼°ä¸¤ä¸ªé…ç½®
    result1 = await eval_system.evaluate_system(config=config1, ...)
    result2 = await eval_system.evaluate_system(config=config2, ...)
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    comparison_df = eval_system.compare_systems()
    
    # ä¿å­˜å¯¹æ¯”ç»“æœ
    comparison_df.to_csv("config_comparison.csv")
```

### å¯¹æ¯”ä¸åŒ RAG ç³»ç»Ÿ

```python
# è¯„ä¼° LightRAG
result_lightrag = await eval_lightrag(...)

# è¯„ä¼°å…¶ä»– RAG ç³»ç»Ÿï¼ˆå¦‚ LlamaIndexï¼‰
result_llamaindex = await eval_llamaindex(...)

# å¯¹æ¯”
eval_system.compare_systems([result_lightrag, result_llamaindex])
```

---

## æœ€ä½³å®è·µ

### 1ï¸âƒ£ é€‰æ‹©åˆé€‚çš„åˆ†å—å¤§å°

**å»ºè®®ï¼š**
- **çŸ­æ–‡æ¡£/é—®ç­”åœºæ™¯**: 256-512 å­—ç¬¦
- **é•¿æ–‡æ¡£/æŠ€æœ¯æ–‡æ¡£**: 512-1024 å­—ç¬¦
- **ä¹¦ç±/æ•™ç§‘ä¹¦**: 1024-2048 å­—ç¬¦

**è¯„ä¼°æ–¹æ³•ï¼š**
```python
# æµ‹è¯•å¤šä¸ªåˆ†å—å¤§å°
chunk_sizes = [256, 512, 768, 1024]
for size in chunk_sizes:
    chunks = create_chunks(doc, size=size, overlap=size//5)
    metrics = await chunking_evaluator.evaluate(doc, chunks)
    print(f"Size {size}: {metrics._overall_score():.2%}")
```

### 2ï¸âƒ£ é€‰æ‹©åˆé€‚çš„åµŒå…¥æ¨¡å‹

**å¸¸è§é€‰æ‹©ï¼š**
| æ¨¡å‹ | ç»´åº¦ | é€Ÿåº¦ | è´¨é‡ | é€‚ç”¨åœºæ™¯ |
|------|------|------|------|----------|
| nomic-embed-text | 768 | å¿« | ä¸­ | é€šç”¨åœºæ™¯ |
| bge-m3 | 1024 | ä¸­ | é«˜ | å¤šè¯­è¨€/é«˜è´¨é‡éœ€æ±‚ |
| OpenAI text-embedding-3-small | 1536 | å¿« | é«˜ | äº‘ç«¯API |

**è¯„ä¼°æ–¹æ³•ï¼š**
ä½¿ç”¨åµŒå…¥è¯„ä¼°å™¨å¯¹æ¯”ä¸åŒæ¨¡å‹çš„æ•ˆæœã€‚

### 3ï¸âƒ£ ä¼˜åŒ–æ£€ç´¢å‚æ•°

**Top-K é€‰æ‹©ï¼š**
- å¤ªå°: å¬å›ç‡ä½
- å¤ªå¤§: ç²¾ç¡®åº¦ä½ï¼Œç”Ÿæˆæ—¶é—´é•¿

**å»ºè®®ï¼š**
- åˆå§‹å°è¯•: `top_k=10`
- é€šè¿‡æ£€ç´¢è¯„ä¼°å™¨æ‰¾åˆ°æœ€ä½³å€¼

```python
for k in [3, 5, 10, 15, 20]:
    # ä¿®æ”¹ LightRAG é…ç½®çš„ top_k
    metrics = await retrieval_evaluator.evaluate(...)
    print(f"K={k}: P@K={metrics.precision_at_k[k]:.2%}, R@K={metrics.recall_at_k[k]:.2%}")
```

### 4ï¸âƒ£ æŒç»­ç›‘æ§å’Œä¼˜åŒ–

**å»ºè®®æµç¨‹ï¼š**
```
1. åŸºçº¿è¯„ä¼° â†’ è®°å½•å½“å‰ç³»ç»Ÿæ€§èƒ½
2. å•ç»„ä»¶ä¼˜åŒ– â†’ é€ä¸ªä¼˜åŒ–åˆ†å—/åµŒå…¥/æ£€ç´¢
3. ç«¯åˆ°ç«¯éªŒè¯ â†’ ç¡®è®¤æ•´ä½“æ€§èƒ½æå‡
4. A/B æµ‹è¯• â†’ å¯¹æ¯”æ–°æ—§é…ç½®
5. å®šæœŸé‡è¯„ â†’ éšç€æ•°æ®å¢é•¿é‡æ–°è¯„ä¼°
```

---

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆåˆ†å—è¯„åˆ†ä½ï¼Ÿ

**å¯èƒ½åŸå› ï¼š**
- åˆ†å—è¾¹ç•Œåˆ‡æ–­äº†å¥å­
- chunk å¤§å°ä¸ä¸€è‡´
- ä¿¡æ¯å¯†åº¦ä½ï¼ˆå¤ªå¤šç©ºç™½æˆ–æ— æ„ä¹‰å†…å®¹ï¼‰

**è§£å†³æ–¹æ¡ˆï¼š**
- ä½¿ç”¨å¥å­è¾¹ç•Œåˆ†å—
- è°ƒæ•´ chunk_size å’Œ overlap
- æ¸…ç†æ–‡æ¡£ä¸­çš„æ— æ„ä¹‰å†…å®¹

### Q2: æ£€ç´¢è¯„åˆ†ä½æ€ä¹ˆåŠï¼Ÿ

**å¯èƒ½åŸå› ï¼š**
- åµŒå…¥æ¨¡å‹ä¸é€‚åˆå½“å‰é¢†åŸŸ
- Top-K å‚æ•°ä¸åˆé€‚
- æ£€ç´¢ç­–ç•¥ï¼ˆçº¯å‘é‡ vs æ··åˆï¼‰é€‰æ‹©ä¸å½“

**è§£å†³æ–¹æ¡ˆï¼š**
- å°è¯•ä¸åŒçš„åµŒå…¥æ¨¡å‹
- è°ƒæ•´ top_k å‚æ•°
- å¯¹æ¯”ä¸åŒæ£€ç´¢ç­–ç•¥ï¼ˆnaive/local/global/hybridï¼‰

### Q3: ç«¯åˆ°ç«¯è¯„åˆ†ä½ä½†ç»„ä»¶è¯„åˆ†é«˜ï¼Ÿ

**å¯èƒ½åŸå› ï¼š**
- ç”Ÿæˆæ¨¡å‹èƒ½åŠ›ä¸è¶³
- Prompt è®¾è®¡ä¸ä½³
- ä¸Šä¸‹æ–‡å¤ªé•¿/å¤ªçŸ­

**è§£å†³æ–¹æ¡ˆï¼š**
- å‡çº§ LLM æ¨¡å‹
- ä¼˜åŒ– Prompt æ¨¡æ¿
- è°ƒæ•´æ£€ç´¢çš„ top_k

---

## è¾“å‡ºæ–‡ä»¶è¯´æ˜

### è¯„ä¼°ç»“æœç›®å½•ç»“æ„

```
lightrag_evaluation_results/
â”œâ”€â”€ LightRAG_Small_Chunks_256_20260121_143022.json  # è¯¦ç»†è¯„ä¼°ç»“æœ
â”œâ”€â”€ LightRAG_Large_Chunks_512_20260121_143155.json
â”œâ”€â”€ comparison_20260121_143200.csv                  # å¯¹æ¯”è¡¨æ ¼
â””â”€â”€ report_20260121_143200.html                     # HTML æŠ¥å‘Š
```

### JSON ç»“æœæ–‡ä»¶æ ¼å¼

```json
{
  "config": {
    "name": "LightRAG_Small_Chunks_256",
    "chunking_method": "fixed_size",
    "chunk_size": 256,
    ...
  },
  "chunking_metrics": {
    "semantic_completeness": 0.85,
    "boundary_quality": 0.92,
    ...
  },
  "embedding_metrics": { ... },
  "retrieval_metrics": { ... },
  "end_to_end_metrics": { ... },
  "timestamp": "2026-01-21T14:30:22.123456"
}
```

---

## æ€»ç»“

âœ… ä½¿ç”¨æœ¬è¯„ä¼°ç³»ç»Ÿï¼Œæ‚¨å¯ä»¥ï¼š
1. **å…¨é¢è¯„ä¼°** RAG ç³»ç»Ÿçš„æ¯ä¸ªç»„ä»¶
2. **å¯¹æ¯”åˆ†æ** ä¸åŒé…ç½®å’Œç³»ç»Ÿ
3. **æ‰¾åˆ°ç“¶é¢ˆ** å®šä½æ€§èƒ½é—®é¢˜
4. **æŒç»­ä¼˜åŒ–** è·Ÿè¸ªæ”¹è¿›æ•ˆæœ

ğŸš€ ç«‹å³å¼€å§‹è¯„ä¼°æ‚¨çš„ RAG ç³»ç»Ÿå§ï¼

```bash
cd /home/ik2200-2025-g2/WorkZone/LightRAG/lightrag/evaluation
python evaluate_lightrag_complete.py
```

---

## ç›¸å…³èµ„æº

- [RAGAS å®˜æ–¹æ–‡æ¡£](https://docs.ragas.io/)
- [LightRAG é¡¹ç›®ä¸»é¡µ](https://github.com/HKUDS/LightRAG)
- [RAG è¯„ä¼°æœ€ä½³å®è·µ](https://www.anthropic.com/research/rag-evaluation)

---

**ä½œè€…**: LightRAG è¯„ä¼°å›¢é˜Ÿ  
**æ›´æ–°æ—¶é—´**: 2026-01-21  
**ç‰ˆæœ¬**: 1.0
