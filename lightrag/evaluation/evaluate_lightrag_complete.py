#!/usr/bin/env python3
"""
LightRAG å®Œæ•´è¯„ä¼°è„šæœ¬

åŠŸèƒ½:
1. è¯„ä¼° LightRAG çš„æ‰€æœ‰ç»„ä»¶ï¼ˆåˆ†å—ã€åµŒå…¥ã€æ£€ç´¢ã€ç”Ÿæˆï¼‰
2. æ”¯æŒå¤šç§é…ç½®å¯¹æ¯”
3. ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
"""

import asyncio
import sys
import logging
from pathlib import Path
from typing import List, Dict, Any
import aiohttp
import numpy as np
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from rag_evaluator_system import (
    RAGEvaluationSystem,
    RAGSystemConfig,
    RAGEvaluationResult
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class LightRAGEvaluator:
    """LightRAG è¯„ä¼°å™¨"""
    
    def __init__(
        self,
        lightrag_api_url: str = "http://localhost:9621",
        ollama_api_url: str = "http://localhost:11434",
        embedding_model: str = "nomic-embed-text",
        llm_model: str = "qwen2.5:7b-instruct"
    ):
        """
        Args:
            lightrag_api_url: LightRAG API åœ°å€
            ollama_api_url: Ollama API åœ°å€
            embedding_model: åµŒå…¥æ¨¡å‹åç§°
            llm_model: LLM æ¨¡å‹åç§°
        """
        self.lightrag_api_url = lightrag_api_url
        self.ollama_api_url = ollama_api_url
        self.embedding_model = embedding_model
        self.llm_model = llm_model
    
    async def get_embedding_func(self):
        """åˆ›å»ºåµŒå…¥å‡½æ•°"""
        async def embed_texts(texts: List[str]) -> np.ndarray:
            """ä½¿ç”¨ Ollama åµŒå…¥æ–‡æœ¬"""
            embeddings = []
            async with aiohttp.ClientSession() as session:
                for text in texts:
                    async with session.post(
                        f"{self.ollama_api_url}/api/embeddings",
                        json={
                            "model": self.embedding_model,
                            "prompt": text
                        }
                    ) as response:
                        if response.status == 200:
                            result = await response.json()
                            embeddings.append(result["embedding"])
                        else:
                            logger.error(f"åµŒå…¥è¯·æ±‚å¤±è´¥: {response.status}")
                            embeddings.append([0.0] * 768)  # å›é€€
            return np.array(embeddings)
        
        return embed_texts
    
    async def get_retrieval_func(self, mode: str = "hybrid"):
        """åˆ›å»ºæ£€ç´¢å‡½æ•°"""
        async def retrieve_docs(query: str, top_k: int) -> List[str]:
            """ä½¿ç”¨ LightRAG æ£€ç´¢æ–‡æ¡£"""
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.lightrag_api_url}/query",
                    json={
                        "query": query,
                        "mode": mode,
                        "only_need_context": True,
                        "top_k": top_k
                    }
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        # æå–æ£€ç´¢åˆ°çš„æ–‡æ¡£ ID
                        # æ³¨æ„ï¼šéœ€è¦æ ¹æ®å®é™… API å“åº”è°ƒæ•´
                        contexts = result.get("contexts", [])
                        return [ctx.get("chunk_id", f"doc_{i}") for i, ctx in enumerate(contexts[:top_k])]
                    else:
                        logger.error(f"æ£€ç´¢è¯·æ±‚å¤±è´¥: {response.status}")
                        return []
        
        return retrieve_docs
    
    async def get_end_to_end_func(self):
        """åˆ›å»ºç«¯åˆ°ç«¯è¯„ä¼°å‡½æ•°ï¼ˆè°ƒç”¨ç°æœ‰çš„ RAGAS è¯„ä¼°ï¼‰"""
        async def evaluate_e2e(test_cases: List) -> Dict[str, Any]:
            """
            è¿è¡Œ RAGAS è¯„ä¼°
            
            æ³¨æ„ï¼šè¿™é‡Œåº”è¯¥è°ƒç”¨ eval_rag_quality.py ä¸­çš„è¯„ä¼°é€»è¾‘
            ä¸ºäº†æ¼”ç¤ºï¼Œè¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
            """
            # TODO: é›†æˆå®é™…çš„ RAGAS è¯„ä¼°
            # å¯ä»¥é€šè¿‡å¯¼å…¥ eval_rag_quality.py ä¸­çš„ RAGQualityEvaluator å®ç°
            
            logger.info("è¿è¡Œç«¯åˆ°ç«¯è¯„ä¼°ï¼ˆRAGASï¼‰...")
            
            # æ¨¡æ‹Ÿè¯„ä¼°ç»“æœ
            return {
                "faithfulness": 0.85,
                "answer_relevancy": 0.78,
                "context_recall": 0.92,
                "context_precision": 0.88,
                "ragas_score": 0.86
            }
        
        return evaluate_e2e
    
    def load_sample_documents(self, docs_dir: Path = None) -> List[str]:
        """åŠ è½½ç¤ºä¾‹æ–‡æ¡£"""
        if docs_dir is None:
            docs_dir = Path(__file__).parent / "sample_documents"
        
        if not docs_dir.exists():
            logger.warning(f"æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {docs_dir}")
            return []
        
        documents = []
        for file_path in docs_dir.glob("*.md"):
            try:
                content = file_path.read_text(encoding='utf-8')
                documents.append(content)
            except Exception as e:
                logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        logger.info(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
        return documents
    
    def create_test_chunks(self, document: str, chunk_size: int, overlap: int) -> List[str]:
        """åˆ›å»ºæµ‹è¯•åˆ†å—"""
        chunks = []
        start = 0
        doc_len = len(document)
        
        while start < doc_len:
            end = min(start + chunk_size, doc_len)
            chunk = document[start:end]
            chunks.append(chunk)
            start += (chunk_size - overlap)
        
        return chunks
    
    def create_embedding_test_data(self) -> tuple:
        """åˆ›å»ºåµŒå…¥è¯„ä¼°æµ‹è¯•æ•°æ®"""
        # è¯­ä¹‰ç›¸ä¼¼åº¦æµ‹è¯•å¯¹
        test_pairs = [
            ("LightRAG is a RAG framework", "LightRAG is a retrieval system", 0.9),
            ("LightRAG is a RAG framework", "Python is a programming language", 0.1),
            ("RAG systems combine retrieval and generation", "Retrieval-augmented generation merges search and LLMs", 0.95),
            ("Knowledge graphs represent structured data", "Unstructured text contains free-form information", 0.2),
            ("Vector databases store embeddings", "Embedding databases use vector search", 0.9),
        ]
        
        # ä¸»é¢˜ç°‡æµ‹è¯•
        test_clusters = [
            # RAG ä¸»é¢˜
            [
                "LightRAG is a retrieval-augmented generation framework",
                "RAG systems combine retrieval and generation",
                "Retrieval-augmented generation improves LLM accuracy"
            ],
            # æ•°æ®åº“ä¸»é¢˜
            [
                "Vector databases store embeddings efficiently",
                "Graph databases represent relationships",
                "NoSQL databases handle unstructured data"
            ],
            # AI æ¨¡å‹ä¸»é¢˜
            [
                "Large language models generate human-like text",
                "Embedding models convert text to vectors",
                "Neural networks learn from data"
            ]
        ]
        
        # æ£€ç´¢æµ‹è¯•
        retrieval_test = [
            (
                "What is LightRAG?",
                [
                    "LightRAG is a RAG framework",
                    "Python is a language",
                    "Databases store data"
                ],
                0
            ),
            (
                "How do vector databases work?",
                [
                    "RAG systems use retrieval",
                    "Vector databases store embeddings",
                    "LLMs generate text"
                ],
                1
            ),
        ]
        
        return test_pairs, test_clusters, retrieval_test
    
    def create_retrieval_test_data(self) -> List[Dict[str, Any]]:
        """åˆ›å»ºæ£€ç´¢è¯„ä¼°æµ‹è¯•æ•°æ®"""
        # æ³¨æ„ï¼šè¿™é‡Œçš„ doc ID éœ€è¦ä¸å®é™…æ’å…¥ LightRAG çš„æ–‡æ¡£å¯¹åº”
        return [
            {
                "query": "What is LightRAG?",
                "relevant_docs": ["doc1", "doc2"],
                "relevance_scores": {"doc1": 1.0, "doc2": 0.8}
            },
            {
                "query": "How does RAG work?",
                "relevant_docs": ["doc1", "doc3"],
                "relevance_scores": {"doc1": 0.9, "doc3": 1.0}
            },
            {
                "query": "What databases does LightRAG support?",
                "relevant_docs": ["doc4"],
                "relevance_scores": {"doc4": 1.0}
            },
        ]


async def main():
    """ä¸»è¯„ä¼°æµç¨‹"""
    
    print(f"\n{'='*80}")
    print("ğŸš€ LightRAG å®Œæ•´è¯„ä¼°ç³»ç»Ÿ")
    print(f"{'='*80}\n")
    
    # åˆå§‹åŒ–è¯„ä¼°ç³»ç»Ÿ
    eval_system = RAGEvaluationSystem(
        output_dir=Path("./lightrag_evaluation_results")
    )
    
    lightrag_eval = LightRAGEvaluator()
    
    # ========================================================================
    # é…ç½®1: å° chunk é…ç½®
    # ========================================================================
    config1 = RAGSystemConfig(
        name="LightRAG_Small_Chunks_256",
        chunking_method="fixed_size",
        chunk_size=256,
        chunk_overlap=50,
        embedding_model="nomic-embed-text",
        embedding_dim=768,
        retrieval_method="hybrid",
        top_k=10,
        llm_model="qwen2.5:7b-instruct"
    )
    
    # ========================================================================
    # é…ç½®2: å¤§ chunk é…ç½®
    # ========================================================================
    config2 = RAGSystemConfig(
        name="LightRAG_Large_Chunks_512",
        chunking_method="fixed_size",
        chunk_size=512,
        chunk_overlap=100,
        embedding_model="nomic-embed-text",
        embedding_dim=768,
        retrieval_method="hybrid",
        top_k=10,
        llm_model="qwen2.5:7b-instruct"
    )
    
    # ========================================================================
    # é…ç½®3: è¯­ä¹‰åˆ†å—ï¼ˆå¦‚æœæ”¯æŒï¼‰
    # ========================================================================
    config3 = RAGSystemConfig(
        name="LightRAG_Semantic_Chunks",
        chunking_method="semantic",
        chunk_size=400,
        chunk_overlap=80,
        embedding_model="nomic-embed-text",
        embedding_dim=768,
        retrieval_method="hybrid",
        top_k=10,
        llm_model="qwen2.5:7b-instruct"
    )
    
    # åŠ è½½æµ‹è¯•æ–‡æ¡£
    docs = lightrag_eval.load_sample_documents()
    if not docs:
        logger.error("âŒ æœªæ‰¾åˆ°æµ‹è¯•æ–‡æ¡£ï¼Œè¯·ç¡®ä¿ sample_documents ç›®å½•å­˜åœ¨")
        return
    
    test_doc = "\n\n".join(docs)  # åˆå¹¶æ‰€æœ‰æ–‡æ¡£
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    embedding_test_pairs, embedding_test_clusters, embedding_retrieval_test = \
        lightrag_eval.create_embedding_test_data()
    
    retrieval_test_queries = lightrag_eval.create_retrieval_test_data()
    
    # è·å–è¯„ä¼°å‡½æ•°
    embedding_func = await lightrag_eval.get_embedding_func()
    retrieval_func = await lightrag_eval.get_retrieval_func(mode="hybrid")
    end_to_end_func = await lightrag_eval.get_end_to_end_func()
    
    # ========================================================================
    # è¯„ä¼°é…ç½®1
    # ========================================================================
    logger.info("\n" + "="*80)
    logger.info("è¯„ä¼°é…ç½® 1: å° Chunk (256)")
    logger.info("="*80)
    
    chunks1 = lightrag_eval.create_test_chunks(test_doc, 256, 50)
    
    result1 = await eval_system.evaluate_system(
        config=config1,
        test_document=test_doc,
        chunks=chunks1,
        embedding_func=embedding_func,
        embedding_test_pairs=embedding_test_pairs,
        embedding_test_clusters=embedding_test_clusters,
        retrieval_func=retrieval_func,
        retrieval_test_queries=retrieval_test_queries,
        end_to_end_func=end_to_end_func,
        end_to_end_test_cases=[],  # ä¼ å…¥å®é™…çš„æµ‹è¯•ç”¨ä¾‹
        evaluate_chunking=True,
        evaluate_embedding=True,
        evaluate_retrieval=True,
        evaluate_end_to_end=False  # å¯é€‰ï¼šå¯ç”¨ç«¯åˆ°ç«¯è¯„ä¼°
    )
    
    # ========================================================================
    # è¯„ä¼°é…ç½®2
    # ========================================================================
    logger.info("\n" + "="*80)
    logger.info("è¯„ä¼°é…ç½® 2: å¤§ Chunk (512)")
    logger.info("="*80)
    
    chunks2 = lightrag_eval.create_test_chunks(test_doc, 512, 100)
    
    result2 = await eval_system.evaluate_system(
        config=config2,
        test_document=test_doc,
        chunks=chunks2,
        embedding_func=embedding_func,
        embedding_test_pairs=embedding_test_pairs,
        embedding_test_clusters=embedding_test_clusters,
        retrieval_func=retrieval_func,
        retrieval_test_queries=retrieval_test_queries,
        end_to_end_func=end_to_end_func,
        end_to_end_test_cases=[],
        evaluate_chunking=True,
        evaluate_embedding=True,
        evaluate_retrieval=True,
        evaluate_end_to_end=False
    )
    
    # ========================================================================
    # å¯¹æ¯”åˆ†æ
    # ========================================================================
    logger.info("\n" + "="*80)
    logger.info("ç”Ÿæˆå¯¹æ¯”åˆ†æ")
    logger.info("="*80)
    
    comparison_df = eval_system.compare_systems()
    
    # ========================================================================
    # ç”ŸæˆæŠ¥å‘Š
    # ========================================================================
    logger.info("\n" + "="*80)
    logger.info("ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
    logger.info("="*80)
    
    eval_system.generate_report()
    
    print(f"\n{'='*80}")
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print(f"{'='*80}\n")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {eval_system.output_dir}")


if __name__ == "__main__":
    asyncio.run(main())
