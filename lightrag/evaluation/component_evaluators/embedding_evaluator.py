#!/usr/bin/env python3
"""
åµŒå…¥è´¨é‡è¯„ä¼°å™¨ (Embedding Evaluator)

è¯„ä¼°æŒ‡æ ‡:
1. è¯­ä¹‰ç›¸ä¼¼åº¦ä¿æŒ (Semantic Similarity Preservation)
2. ä¸»é¢˜åŒºåˆ†åº¦ (Topic Separation)
3. æ£€ç´¢å‡†ç¡®ç‡ (Retrieval Accuracy)
4. é™ç»´è´¨é‡ (Dimensionality Quality)
"""

import asyncio
import numpy as np
from typing import List, Dict, Any, Tuple, Callable
from dataclasses import dataclass
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from pathlib import Path


@dataclass
class EmbeddingMetrics:
    """åµŒå…¥è¯„ä¼°æŒ‡æ ‡"""
    semantic_similarity_preservation: float  # è¯­ä¹‰ç›¸ä¼¼åº¦ä¿æŒ (0-1)
    topic_separation: float                  # ä¸»é¢˜åŒºåˆ†åº¦ (0-1)
    retrieval_accuracy: float                # æ£€ç´¢å‡†ç¡®ç‡ (0-1)
    intra_cluster_similarity: float          # ç°‡å†…ç›¸ä¼¼åº¦ (0-1)
    inter_cluster_distance: float            # ç°‡é—´è·ç¦» (0-1)
    dimension: int                           # åµŒå…¥ç»´åº¦
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "semantic_similarity_preservation": round(self.semantic_similarity_preservation, 4),
            "topic_separation": round(self.topic_separation, 4),
            "retrieval_accuracy": round(self.retrieval_accuracy, 4),
            "intra_cluster_similarity": round(self.intra_cluster_similarity, 4),
            "inter_cluster_distance": round(self.inter_cluster_distance, 4),
            "dimension": self.dimension,
            "overall_score": round(self._overall_score(), 4)
        }
    
    def _overall_score(self) -> float:
        """è®¡ç®—æ€»ä½“åˆ†æ•°"""
        return (
            self.semantic_similarity_preservation * 0.3 +
            self.topic_separation * 0.25 +
            self.retrieval_accuracy * 0.35 +
            self.intra_cluster_similarity * 0.05 +
            self.inter_cluster_distance * 0.05
        )


class EmbeddingEvaluator:
    """åµŒå…¥è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, embedding_func: Callable):
        """
        Args:
            embedding_func: åµŒå…¥å‡½æ•°ï¼Œè¾“å…¥æ–‡æœ¬åˆ—è¡¨ï¼Œè¿”å›åµŒå…¥å‘é‡æ•°ç»„
                           ç¤ºä¾‹: async def embed(texts: List[str]) -> np.ndarray
        """
        self.embedding_func = embedding_func
    
    async def evaluate(
        self,
        test_pairs: List[Tuple[str, str, float]],  # (text1, text2, äººå·¥ç›¸ä¼¼åº¦åˆ†æ•°)
        test_clusters: List[List[str]] = None,     # ä¸åŒä¸»é¢˜çš„æ–‡æœ¬ç°‡
        retrieval_test: List[Tuple[str, List[str], int]] = None  # (query, candidates, æ­£ç¡®ç­”æ¡ˆç´¢å¼•)
    ) -> EmbeddingMetrics:
        """
        è¯„ä¼°åµŒå…¥è´¨é‡
        
        Args:
            test_pairs: æ–‡æœ¬å¯¹åŠå…¶äººå·¥æ ‡æ³¨çš„ç›¸ä¼¼åº¦ (0-1)
            test_clusters: ä¸åŒä¸»é¢˜çš„æ–‡æœ¬ç°‡ï¼Œç”¨äºè¯„ä¼°ä¸»é¢˜åŒºåˆ†
            retrieval_test: æ£€ç´¢æµ‹è¯•ç”¨ä¾‹ (query, å€™é€‰æ–‡æœ¬åˆ—è¡¨, æ­£ç¡®ç­”æ¡ˆç´¢å¼•)
        
        Returns:
            EmbeddingMetrics: è¯„ä¼°æŒ‡æ ‡
        """
        print(f"\n{'='*70}")
        print("ğŸ§¬ åµŒå…¥è´¨é‡è¯„ä¼°")
        print(f"{'='*70}")
        
        # 1. è¯­ä¹‰ç›¸ä¼¼åº¦ä¿æŒ
        similarity_preservation = await self._evaluate_similarity_preservation(test_pairs)
        
        # 2. ä¸»é¢˜åŒºåˆ†åº¦
        if test_clusters:
            topic_separation, intra_sim, inter_dist = await self._evaluate_topic_separation(test_clusters)
        else:
            topic_separation, intra_sim, inter_dist = 0.0, 0.0, 0.0
        
        # 3. æ£€ç´¢å‡†ç¡®ç‡
        if retrieval_test:
            retrieval_accuracy = await self._evaluate_retrieval_accuracy(retrieval_test)
        else:
            retrieval_accuracy = 0.0
        
        # è·å–åµŒå…¥ç»´åº¦
        sample_embedding = await self.embedding_func([test_pairs[0][0]])
        dimension = sample_embedding.shape[1] if len(sample_embedding.shape) > 1 else len(sample_embedding)
        
        metrics = EmbeddingMetrics(
            semantic_similarity_preservation=similarity_preservation,
            topic_separation=topic_separation,
            retrieval_accuracy=retrieval_accuracy,
            intra_cluster_similarity=intra_sim,
            inter_cluster_distance=inter_dist,
            dimension=dimension
        )
        
        self._display_results(metrics)
        return metrics
    
    async def _evaluate_similarity_preservation(
        self,
        test_pairs: List[Tuple[str, str, float]]
    ) -> float:
        """
        è¯„ä¼°è¯­ä¹‰ç›¸ä¼¼åº¦ä¿æŒ
        
        æ–¹æ³•ï¼šè®¡ç®—åµŒå…¥ç›¸ä¼¼åº¦ä¸äººå·¥æ ‡æ³¨ç›¸ä¼¼åº¦çš„ç›¸å…³æ€§ (Pearson/Spearman)
        """
        if not test_pairs:
            return 0.0
        
        texts1, texts2, human_scores = [], [], []
        for text1, text2, score in test_pairs:
            texts1.append(text1)
            texts2.append(text2)
            human_scores.append(score)
        
        # è·å–åµŒå…¥
        embeddings1 = await self.embedding_func(texts1)
        embeddings2 = await self.embedding_func(texts2)
        
        # è®¡ç®—åµŒå…¥ç›¸ä¼¼åº¦
        embedding_scores = []
        for emb1, emb2 in zip(embeddings1, embeddings2):
            similarity = cosine_similarity([emb1], [emb2])[0][0]
            embedding_scores.append(similarity)
        
        # è®¡ç®—ç›¸å…³æ€§ (Pearson)
        correlation = np.corrcoef(human_scores, embedding_scores)[0, 1]
        
        # è½¬æ¢ä¸º 0-1 åˆ†æ•°ï¼ˆç›¸å…³æ€§ -1 åˆ° 1ï¼Œè½¬æ¢ä¸º 0 åˆ° 1ï¼‰
        score = (correlation + 1) / 2
        
        return score
    
    async def _evaluate_topic_separation(
        self,
        test_clusters: List[List[str]]
    ) -> Tuple[float, float, float]:
        """
        è¯„ä¼°ä¸»é¢˜åŒºåˆ†åº¦
        
        æ–¹æ³•ï¼š
        - ç°‡å†…ç›¸ä¼¼åº¦åº”è¯¥é«˜
        - ç°‡é—´è·ç¦»åº”è¯¥å¤§
        
        Returns:
            (ä¸»é¢˜åŒºåˆ†åº¦, ç°‡å†…ç›¸ä¼¼åº¦, ç°‡é—´è·ç¦»)
        """
        if len(test_clusters) < 2:
            return 0.0, 0.0, 0.0
        
        # è·å–æ‰€æœ‰æ–‡æœ¬çš„åµŒå…¥
        all_texts = []
        cluster_labels = []
        for cluster_id, cluster in enumerate(test_clusters):
            all_texts.extend(cluster)
            cluster_labels.extend([cluster_id] * len(cluster))
        
        embeddings = await self.embedding_func(all_texts)
        
        # è®¡ç®—ç°‡å†…ç›¸ä¼¼åº¦ (Intra-cluster similarity)
        intra_similarities = []
        for cluster_id, cluster in enumerate(test_clusters):
            cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_id]
            cluster_embeddings = embeddings[cluster_indices]
            
            if len(cluster_embeddings) > 1:
                # è®¡ç®—ç°‡å†…æ‰€æœ‰å¯¹çš„å¹³å‡ç›¸ä¼¼åº¦
                sim_matrix = cosine_similarity(cluster_embeddings)
                # å»é™¤å¯¹è§’çº¿ï¼ˆè‡ªå·±ä¸è‡ªå·±çš„ç›¸ä¼¼åº¦ï¼‰
                mask = ~np.eye(len(cluster_embeddings), dtype=bool)
                intra_sim = sim_matrix[mask].mean()
                intra_similarities.append(intra_sim)
        
        avg_intra_similarity = np.mean(intra_similarities) if intra_similarities else 0.0
        
        # è®¡ç®—ç°‡é—´è·ç¦» (Inter-cluster distance)
        cluster_centroids = []
        for cluster_id in range(len(test_clusters)):
            cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_id]
            cluster_embeddings = embeddings[cluster_indices]
            centroid = cluster_embeddings.mean(axis=0)
            cluster_centroids.append(centroid)
        
        # è®¡ç®—æ‰€æœ‰ç°‡ä¸­å¿ƒå¯¹çš„è·ç¦»
        inter_distances = []
        for i in range(len(cluster_centroids)):
            for j in range(i + 1, len(cluster_centroids)):
                distance = 1 - cosine_similarity([cluster_centroids[i]], [cluster_centroids[j]])[0][0]
                inter_distances.append(distance)
        
        avg_inter_distance = np.mean(inter_distances) if inter_distances else 0.0
        
        # ä¸»é¢˜åŒºåˆ†åº¦ = ç°‡é—´è·ç¦» - (1 - ç°‡å†…ç›¸ä¼¼åº¦)
        # ç°‡é—´è·ç¦»å¤§ä¸”ç°‡å†…ç›¸ä¼¼åº¦é«˜ = åŒºåˆ†åº¦å¥½
        topic_separation = (avg_inter_distance + avg_intra_similarity) / 2
        
        return topic_separation, avg_intra_similarity, avg_inter_distance
    
    async def _evaluate_retrieval_accuracy(
        self,
        retrieval_test: List[Tuple[str, List[str], int]]
    ) -> float:
        """
        è¯„ä¼°æ£€ç´¢å‡†ç¡®ç‡
        
        æ–¹æ³•ï¼š
        - å¯¹äºæ¯ä¸ªæŸ¥è¯¢ï¼Œæ£€ç´¢æœ€ç›¸ä¼¼çš„å€™é€‰æ–‡æœ¬
        - è®¡ç®— Top-1 å‡†ç¡®ç‡
        """
        if not retrieval_test:
            return 0.0
        
        correct_count = 0
        
        for query, candidates, correct_idx in retrieval_test:
            # è·å–æŸ¥è¯¢å’Œå€™é€‰æ–‡æœ¬çš„åµŒå…¥
            query_embedding = await self.embedding_func([query])
            candidate_embeddings = await self.embedding_func(candidates)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = cosine_similarity(query_embedding, candidate_embeddings)[0]
            
            # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„å€™é€‰
            predicted_idx = np.argmax(similarities)
            
            if predicted_idx == correct_idx:
                correct_count += 1
        
        accuracy = correct_count / len(retrieval_test)
        return accuracy
    
    def _display_results(self, metrics: EmbeddingMetrics):
        """æ˜¾ç¤ºè¯„ä¼°ç»“æœ"""
        print(f"\nğŸ“ˆ åµŒå…¥è´¨é‡æŒ‡æ ‡:")
        print(f"  â€¢ è¯­ä¹‰ç›¸ä¼¼åº¦ä¿æŒ: {metrics.semantic_similarity_preservation:.2%}")
        print(f"  â€¢ ä¸»é¢˜åŒºåˆ†åº¦:     {metrics.topic_separation:.2%}")
        print(f"  â€¢ æ£€ç´¢å‡†ç¡®ç‡:     {metrics.retrieval_accuracy:.2%}")
        print(f"  â€¢ ç°‡å†…ç›¸ä¼¼åº¦:     {metrics.intra_cluster_similarity:.2%}")
        print(f"  â€¢ ç°‡é—´è·ç¦»:       {metrics.inter_cluster_distance:.2%}")
        print(f"\nğŸ“Š åµŒå…¥ä¿¡æ¯:")
        print(f"  â€¢ åµŒå…¥ç»´åº¦: {metrics.dimension}")
        print(f"\nğŸ¯ æ€»ä½“è¯„åˆ†: {metrics._overall_score():.2%}")
        print(f"{'='*70}\n")


# ============================================================================
# ç¤ºä¾‹ä½¿ç”¨
# ============================================================================

async def mock_embedding_func(texts: List[str]) -> np.ndarray:
    """æ¨¡æ‹ŸåµŒå…¥å‡½æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    # ç®€å•çš„è¯è¢‹æ¨¡å‹åµŒå…¥
    embeddings = []
    for text in texts:
        # ç®€åŒ–ï¼šä½¿ç”¨æ–‡æœ¬é•¿åº¦å’Œå•è¯æ•°ä½œä¸ºç‰¹å¾
        words = text.lower().split()
        embedding = np.random.randn(128)  # 128ç»´éšæœºå‘é‡
        # æ·»åŠ ä¸€äº›åŸºäºæ–‡æœ¬å†…å®¹çš„ç‰¹å¾
        embedding[0] = len(text) / 100
        embedding[1] = len(words) / 10
        embeddings.append(embedding)
    return np.array(embeddings)


async def test_embedding_evaluator():
    """æµ‹è¯•åµŒå…¥è¯„ä¼°å™¨"""
    
    evaluator = EmbeddingEvaluator(embedding_func=mock_embedding_func)
    
    # 1. è¯­ä¹‰ç›¸ä¼¼åº¦æµ‹è¯•å¯¹
    test_pairs = [
        ("LightRAG is a RAG framework", "LightRAG is a retrieval system", 0.9),
        ("LightRAG is a RAG framework", "Python is a programming language", 0.1),
        ("The cat sat on the mat", "A cat was sitting on a mat", 0.95),
        ("The cat sat on the mat", "Dogs are loyal animals", 0.2),
    ]
    
    # 2. ä¸»é¢˜ç°‡æµ‹è¯•
    test_clusters = [
        # RAG ä¸»é¢˜
        [
            "LightRAG is a retrieval-augmented generation framework",
            "RAG systems combine retrieval and generation",
            "Retrieval-augmented generation improves LLM accuracy"
        ],
        # Python ä¸»é¢˜
        [
            "Python is a high-level programming language",
            "Python supports object-oriented programming",
            "Python has a rich ecosystem of libraries"
        ],
        # æ•°æ®åº“ä¸»é¢˜
        [
            "MySQL is a relational database",
            "PostgreSQL supports advanced SQL features",
            "Databases store and manage structured data"
        ]
    ]
    
    # 3. æ£€ç´¢æµ‹è¯•
    retrieval_test = [
        (
            "What is LightRAG?",
            [
                "LightRAG is a RAG framework for AI applications",
                "Python is a programming language",
                "Databases store data efficiently"
            ],
            0  # æ­£ç¡®ç­”æ¡ˆç´¢å¼•
        ),
        (
            "Tell me about Python",
            [
                "LightRAG is a RAG framework",
                "Python is a versatile programming language",
                "SQL is used for database queries"
            ],
            1  # æ­£ç¡®ç­”æ¡ˆç´¢å¼•
        ),
    ]
    
    # è¿è¡Œè¯„ä¼°
    metrics = await evaluator.evaluate(
        test_pairs=test_pairs,
        test_clusters=test_clusters,
        retrieval_test=retrieval_test
    )
    
    print(f"è¯„ä¼°ç»“æœ: {metrics.to_dict()}")


if __name__ == "__main__":
    asyncio.run(test_embedding_evaluator())
