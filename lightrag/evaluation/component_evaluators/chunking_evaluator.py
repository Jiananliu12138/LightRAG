#!/usr/bin/env python3
"""
åˆ†å—è´¨é‡è¯„ä¼°å™¨ (Chunking Evaluator)

è¯„ä¼°æŒ‡æ ‡:
1. è¯­ä¹‰å®Œæ•´æ€§ (Semantic Completeness)
2. è¾¹ç•Œè´¨é‡ (Boundary Quality)
3. å¤§å°ä¸€è‡´æ€§ (Size Consistency)
4. ä¿¡æ¯å¯†åº¦ (Information Density)
5. è¦†ç›–ç‡ (Coverage)
"""

import asyncio
import re
from typing import List, Dict, Any
from dataclasses import dataclass
import numpy as np
from pathlib import Path


@dataclass
class ChunkingMetrics:
    """
    åˆ†å—è¯„ä¼°æŒ‡æ ‡ (åŸºäºå­¦æœ¯ç ”ç©¶)
    
    References:
    - Semantic Cohesion: "Text Segmentation by Cross-Lingual Word Embeddings" (ACL 2019)
    - Information Gain Ratio: Shannon's Information Theory
    - Entity-Relation Recall: Graph-based RAG evaluation (LightRAG Paper 2024)
    """
    semantic_cohesion: float         # è¯­ä¹‰èšåˆåº¦ (Intra-chunk semantic similarity) (0-1)
    boundary_quality: float          # è¾¹ç•Œè´¨é‡ (Sentence/paragraph boundary alignment) (0-1)
    size_consistency: float          # å¤§å°ä¸€è‡´æ€§ (Coefficient of Variation) (0-1)
    information_gain_ratio: float    # ä¿¡æ¯å¢ç›Šæ¯” (Entropy-based uniqueness) (0-1)
    coverage: float                  # è¦†ç›–ç‡ (Document coverage) (0-1)
    entity_relation_recall: float    # å®ä½“-å…³ç³»å¬å›ç‡ (For graph-based RAG) (0-1)
    avg_chunk_size: float            # å¹³å‡ chunk å¤§å°
    std_chunk_size: float            # chunk å¤§å°æ ‡å‡†å·®
    total_chunks: int                # æ€» chunk æ•°
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "semantic_cohesion": round(self.semantic_cohesion, 4),
            "boundary_quality": round(self.boundary_quality, 4),
            "size_consistency": round(self.size_consistency, 4),
            "information_gain_ratio": round(self.information_gain_ratio, 4),
            "coverage": round(self.coverage, 4),
            "entity_relation_recall": round(self.entity_relation_recall, 4),
            "avg_chunk_size": round(self.avg_chunk_size, 2),
            "std_chunk_size": round(self.std_chunk_size, 2),
            "total_chunks": self.total_chunks,
            "overall_score": round(self._overall_score(), 4)
        }
    
    def _overall_score(self) -> float:
        """
        è®¡ç®—æ€»ä½“åˆ†æ•° (åŠ æƒå¹³å‡)
        
        æƒé‡åŸºäº RAG è¯„ä¼°æ–‡çŒ®ä¸­çš„é‡è¦æ€§æ’åº:
        - Semantic Cohesion: 30% (æœ€é‡è¦, ç›´æ¥å½±å“æ£€ç´¢è´¨é‡)
        - Information Gain Ratio: 25% (é¿å…å†—ä½™ä¿¡æ¯)
        - Boundary Quality: 20% (ä¿æŒè¯­ä¹‰å®Œæ•´æ€§)
        - Entity-Relation Recall: 15% (å›¾RAGä¸“ç”¨)
        - Size Consistency: 5%
        - Coverage: 5%
        """
        return (
            self.semantic_cohesion * 0.30 +
            self.information_gain_ratio * 0.25 +
            self.boundary_quality * 0.20 +
            self.entity_relation_recall * 0.15 +
            self.size_consistency * 0.05 +
            self.coverage * 0.05
        )


class ChunkingEvaluator:
    """åˆ†å—è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, llm_client=None):
        """
        Args:
            llm_client: å¯é€‰çš„ LLM å®¢æˆ·ç«¯ï¼Œç”¨äºè¯­ä¹‰å®Œæ•´æ€§è¯„ä¼°
        """
        self.llm_client = llm_client
    
    async def evaluate(
        self,
        original_document: str,
        chunks: List[str],
        chunk_metadata: List[Dict] = None,
        extracted_entities: List[List[str]] = None,  # æ¯ä¸ªchunkæå–çš„å®ä½“åˆ—è¡¨
        extracted_relations: List[List[tuple]] = None  # æ¯ä¸ªchunkæå–çš„å…³ç³»åˆ—è¡¨
    ) -> ChunkingMetrics:
        """
        è¯„ä¼°åˆ†å—è´¨é‡ (åŸºäºå­¦æœ¯æŒ‡æ ‡)
        
        Args:
            original_document: åŸå§‹æ–‡æ¡£
            chunks: åˆ†å—åçš„æ–‡æœ¬åˆ—è¡¨
            chunk_metadata: æ¯ä¸ª chunk çš„å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
            extracted_entities: æ¯ä¸ªchunkæå–çš„å®ä½“ï¼ˆç”¨äºå›¾RAGè¯„ä¼°ï¼‰
            extracted_relations: æ¯ä¸ªchunkæå–çš„å…³ç³»ï¼ˆç”¨äºå›¾RAGè¯„ä¼°ï¼‰
        
        Returns:
            ChunkingMetrics: è¯„ä¼°æŒ‡æ ‡
        """
        print(f"\n{'='*70}")
        print("ğŸ“Š åˆ†å—è´¨é‡è¯„ä¼° (Academic Metrics)")
        print(f"{'='*70}")
        
        # 1. è¯­ä¹‰èšåˆåº¦ (Semantic Cohesion)
        semantic_cohesion = await self._evaluate_semantic_cohesion(chunks)
        
        # 2. è¾¹ç•Œè´¨é‡ (Boundary Quality)
        boundary_quality = self._evaluate_boundary_quality(chunks)
        
        # 3. å¤§å°ä¸€è‡´æ€§ (Size Consistency)
        size_consistency, avg_size, std_size = self._evaluate_size_consistency(chunks)
        
        # 4. ä¿¡æ¯å¢ç›Šæ¯” (Information Gain Ratio)
        information_gain_ratio = self._evaluate_information_gain_ratio(chunks)
        
        # 5. è¦†ç›–ç‡ (Coverage)
        coverage = self._evaluate_coverage(original_document, chunks)
        
        # 6. å®ä½“-å…³ç³»å¬å›ç‡ (Entity-Relation Recall) - LightRAGä¸“ç”¨
        entity_relation_recall = self._evaluate_entity_relation_recall(
            original_document, chunks, extracted_entities, extracted_relations
        )
        
        metrics = ChunkingMetrics(
            semantic_cohesion=semantic_cohesion,
            boundary_quality=boundary_quality,
            size_consistency=size_consistency,
            information_gain_ratio=information_gain_ratio,
            coverage=coverage,
            entity_relation_recall=entity_relation_recall,
            avg_chunk_size=avg_size,
            std_chunk_size=std_size,
            total_chunks=len(chunks)
        )
        
        self._display_results(metrics)
        return metrics
    
    async def _evaluate_semantic_cohesion(self, chunks: List[str]) -> float:
        """
        è¯„ä¼°è¯­ä¹‰èšåˆåº¦ (Semantic Cohesion)
        
        æ–¹æ³•: è®¡ç®—å—å†…å¥å­ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦å‡å€¼
        ç†è®ºä¾æ®: "Text Segmentation by Cross-Lingual Word Embeddings" (ACL 2019)
        
        å®ç°: ä½¿ç”¨ç®€åŒ–çš„è¯è¢‹æ¨¡å‹è®¡ç®—å¥å­ç›¸ä¼¼åº¦
        å¦‚æœæœ‰åµŒå…¥æ¨¡å‹ï¼Œåº”è¯¥ç”¨å¥å­åµŒå…¥çš„ä½™å¼¦ç›¸ä¼¼åº¦
        """
        if not chunks:
            return 0.0
        
        cohesion_scores = []
        
        for chunk in chunks:
            chunk = chunk.strip()
            if not chunk:
                continue
            
            # åˆ†å¥
            import re
            sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', chunk)
            sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
            
            if len(sentences) < 2:
                # å•å¥chunkï¼Œè®¤ä¸ºå®Œå…¨èšåˆ
                cohesion_scores.append(1.0)
                continue
            
            # è®¡ç®—å¥å­é—´çš„è¯æ±‡é‡å åº¦ (ç®€åŒ–çš„ç›¸ä¼¼åº¦)
            similarities = []
            for i in range(len(sentences) - 1):
                words1 = set(sentences[i].lower().split())
                words2 = set(sentences[i + 1].lower().split())
                
                if len(words1) == 0 or len(words2) == 0:
                    continue
                
                # Jaccard ç›¸ä¼¼åº¦
                intersection = len(words1 & words2)
                union = len(words1 | words2)
                similarity = intersection / union if union > 0 else 0.0
                similarities.append(similarity)
            
            if similarities:
                cohesion_scores.append(np.mean(similarities))
            else:
                cohesion_scores.append(0.5)  # ä¸­æ€§åˆ†æ•°
        
        return np.mean(cohesion_scores) if cohesion_scores else 0.0
    
    def _evaluate_boundary_quality(self, chunks: List[str]) -> float:
        """
        è¯„ä¼°è¾¹ç•Œè´¨é‡
        
        å¥½çš„è¾¹ç•Œï¼š
        - åœ¨æ®µè½æˆ–å¥å­è¾¹ç•Œå¤„åˆ†å‰²
        - ä¸åœ¨å•è¯ä¸­é—´åˆ†å‰²
        - ä¿ç•™ä¸Šä¸‹æ–‡è¿è´¯æ€§
        """
        good_boundaries = 0
        total_boundaries = len(chunks) - 1
        
        if total_boundaries == 0:
            return 1.0
        
        for i in range(total_boundaries):
            current_chunk = chunks[i].strip()
            next_chunk = chunks[i + 1].strip()
            
            if not current_chunk or not next_chunk:
                continue
            
            # æ£€æŸ¥å½“å‰ chunk æ˜¯å¦ä»¥åˆç†çš„æ ‡ç‚¹ç»“å°¾
            ends_well = current_chunk[-1] in '.!?\nã€‚ï¼ï¼Ÿ\n'
            
            # æ£€æŸ¥ä¸‹ä¸€ä¸ª chunk æ˜¯å¦ä»¥å¤§å†™å­—æ¯æˆ–æ®µè½å¼€å§‹
            starts_well = next_chunk[0].isupper() or next_chunk[0] == '\n'
            
            # æ£€æŸ¥æ˜¯å¦åœ¨å•è¯ä¸­é—´åˆ‡æ–­
            not_mid_word = current_chunk[-1] != '-' and not (
                current_chunk[-1].isalnum() and next_chunk[0].isalnum()
            )
            
            if (ends_well or starts_well) and not_mid_word:
                good_boundaries += 1
        
        return good_boundaries / total_boundaries
    
    def _evaluate_size_consistency(self, chunks: List[str]) -> tuple[float, float, float]:
        """
        è¯„ä¼°å¤§å°ä¸€è‡´æ€§
        
        Returns:
            (ä¸€è‡´æ€§åˆ†æ•°, å¹³å‡å¤§å°, æ ‡å‡†å·®)
        """
        sizes = [len(chunk) for chunk in chunks]
        avg_size = np.mean(sizes)
        std_size = np.std(sizes)
        
        # å˜å¼‚ç³»æ•° (Coefficient of Variation)
        cv = std_size / avg_size if avg_size > 0 else 0
        
        # ä¸€è‡´æ€§åˆ†æ•°ï¼šCV è¶Šå°è¶Šå¥½ (0 æœ€å¥½ï¼Œ1+ å¾ˆå·®)
        # è½¬æ¢ä¸º 0-1 åˆ†æ•°ï¼šä½¿ç”¨ 1 / (1 + CV)
        consistency_score = 1.0 / (1.0 + cv)
        
        return consistency_score, avg_size, std_size
    
    def _evaluate_information_gain_ratio(self, chunks: List[str]) -> float:
        """
        è¯„ä¼°ä¿¡æ¯å¢ç›Šæ¯” (Information Gain Ratio)
        
        æ–¹æ³•: åŸºäºä¿¡æ¯ç†µè¯„ä¼°æ¯ä¸ªchunkç›¸å¯¹äºæ•´ä½“æ–‡æ¡£çš„ä¿¡æ¯å¢ç›Š
        ç†è®ºä¾æ®: Shannon's Information Theory, Quinlan's C4.5 Algorithm
        
        å®ç°: è®¡ç®—chunkçš„è¯æ±‡ç†µä¸å…¨å±€ç†µçš„æ¯”ç‡ï¼Œé¿å…å†—ä½™ä¿¡æ¯
        """
        if not chunks:
            return 0.0
        
        # æ„å»ºå…¨å±€è¯æ±‡åˆ†å¸ƒ
        from collections import Counter
        global_word_counts = Counter()
        chunk_word_counts = []
        
        for chunk in chunks:
            words = chunk.lower().split()
            chunk_counts = Counter(words)
            chunk_word_counts.append(chunk_counts)
            global_word_counts.update(chunk_counts)
        
        total_words = sum(global_word_counts.values())
        if total_words == 0:
            return 0.0
        
        # è®¡ç®—å…¨å±€ç†µ
        global_entropy = 0.0
        for count in global_word_counts.values():
            prob = count / total_words
            if prob > 0:
                global_entropy -= prob * np.log2(prob)
        
        # è®¡ç®—æ¯ä¸ªchunkçš„ä¿¡æ¯å¢ç›Š
        info_gains = []
        for chunk_counts in chunk_word_counts:
            chunk_total = sum(chunk_counts.values())
            if chunk_total == 0:
                continue
            
            # è®¡ç®—chunkçš„ç†µ
            chunk_entropy = 0.0
            for count in chunk_counts.values():
                prob = count / chunk_total
                if prob > 0:
                    chunk_entropy -= prob * np.log2(prob)
            
            # ä¿¡æ¯å¢ç›Šæ¯” = chunkç†µ / å…¨å±€ç†µ (å½’ä¸€åŒ–)
            if global_entropy > 0:
                info_gain = chunk_entropy / global_entropy
                info_gains.append(min(info_gain, 1.0))
        
        return np.mean(info_gains) if info_gains else 0.0
    
    def _evaluate_entity_relation_recall(
        self,
        original_document: str,
        chunks: List[str],
        extracted_entities: List[List[str]] = None,
        extracted_relations: List[List[tuple]] = None
    ) -> float:
        """
        è¯„ä¼°å®ä½“-å…³ç³»å¬å›ç‡ (Entity-Relation Recall)
        
        æ–¹æ³•: å¯¹äºå›¾RAG (å¦‚LightRAG)ï¼Œè¯„ä¼°åˆ†å—åæå–çš„çŸ¥è¯†å›¾è°±
              ç›¸å¯¹äºå®Œæ•´æ–‡æ¡£çš„è¦†ç›–ç¨‹åº¦
        ç†è®ºä¾æ®: Graph-based RAG Evaluation (LightRAG Paper 2024)
        
        å®ç°: ç®€åŒ–ç‰ˆ - ç»Ÿè®¡å‘½åå®ä½“ï¼ˆå¤§å†™å•è¯ã€ä¸“æœ‰åè¯ï¼‰çš„è¦†ç›–ç‡
              å®Œæ•´å®ç°éœ€è¦NERæ¨¡å‹
        """
        if extracted_entities is not None and extracted_relations is not None:
            # å¦‚æœæä¾›äº†å®é™…çš„å®ä½“å’Œå…³ç³»ï¼Œä½¿ç”¨å®ƒä»¬
            all_chunk_entities = set()
            for entity_list in extracted_entities:
                all_chunk_entities.update(entity_list)
            
            # è¿™é‡Œéœ€è¦å‚è€ƒå®ä½“åˆ—è¡¨ï¼Œå®é™…åº”è¯¥ä»åŸæ–‡æ¡£æå–
            # ä¸ºæ¼”ç¤ºï¼Œè¿”å›å›ºå®šå€¼
            return 0.85  # å ä½ç¬¦ï¼Œå®é™…éœ€è¦å®Œæ•´å®ç°
        
        # ç®€åŒ–å®ç°ï¼šç»Ÿè®¡æ½œåœ¨å®ä½“è¯ï¼ˆå¤§å†™å¼€å¤´çš„è¯ç»„ï¼‰çš„è¦†ç›–ç‡
        import re
        
        # æå–åŸæ–‡æ¡£ä¸­çš„æ½œåœ¨å®ä½“ï¼ˆå¤§å†™å¼€å¤´çš„è¿ç»­è¯ï¼‰
        doc_entities = set(re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', original_document))
        
        if not doc_entities:
            return 1.0  # å¦‚æœæ²¡æœ‰å®ä½“ï¼Œè®¤ä¸ºå®Œå…¨å¬å›
        
        # æå–æ‰€æœ‰chunksä¸­çš„å®ä½“
        chunk_entities = set()
        for chunk in chunks:
            chunk_entities.update(re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', chunk))
        
        # è®¡ç®—å¬å›ç‡
        recall = len(chunk_entities & doc_entities) / len(doc_entities) if doc_entities else 1.0
        
        return recall
    
    def _evaluate_coverage(self, original_document: str, chunks: List[str]) -> float:
        """
        è¯„ä¼°è¦†ç›–ç‡
        
        æ–¹æ³•ï¼š
        - è®¡ç®— chunks é‡ç»„åä¸åŸæ–‡æ¡£çš„ç›¸ä¼¼åº¦
        - æ£€æŸ¥æ˜¯å¦æœ‰ä¿¡æ¯ä¸¢å¤±
        """
        # ç®€åŒ–ç‰ˆï¼šè®¡ç®—å­—ç¬¦è¦†ç›–ç‡
        original_chars = len(original_document.replace(' ', '').replace('\n', ''))
        chunk_chars = sum(len(chunk.replace(' ', '').replace('\n', '')) for chunk in chunks)
        
        # è¦†ç›–ç‡
        coverage = min(chunk_chars / original_chars, 1.0) if original_chars > 0 else 0.0
        
        return coverage
    
    def _display_results(self, metrics: ChunkingMetrics):
        """æ˜¾ç¤ºè¯„ä¼°ç»“æœ"""
        print(f"\nğŸ“ˆ å­¦æœ¯è¯„ä¼°æŒ‡æ ‡ (Academic Metrics):")
        print(f"  â€¢ è¯­ä¹‰èšåˆåº¦ (Semantic Cohesion):       {metrics.semantic_cohesion:.2%}")
        print(f"    â””â”€ ç†è®ºä¾æ®: ACL 2019 Text Segmentation")
        print(f"  â€¢ ä¿¡æ¯å¢ç›Šæ¯” (Information Gain Ratio): {metrics.information_gain_ratio:.2%}")
        print(f"    â””â”€ ç†è®ºä¾æ®: Shannon's Information Theory")
        print(f"  â€¢ è¾¹ç•Œè´¨é‡ (Boundary Quality):         {metrics.boundary_quality:.2%}")
        print(f"  â€¢ å®ä½“-å…³ç³»å¬å›ç‡ (Entity-Rel Recall):  {metrics.entity_relation_recall:.2%}")
        print(f"    â””â”€ ç†è®ºä¾æ®: Graph-based RAG (LightRAG 2024)")
        print(f"  â€¢ å¤§å°ä¸€è‡´æ€§ (Size Consistency):       {metrics.size_consistency:.2%}")
        print(f"  â€¢ è¦†ç›–ç‡ (Coverage):                   {metrics.coverage:.2%}")
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  â€¢ æ€» Chunk æ•°: {metrics.total_chunks}")
        print(f"  â€¢ å¹³å‡å¤§å°:    {metrics.avg_chunk_size:.0f} å­—ç¬¦")
        print(f"  â€¢ æ ‡å‡†å·®:      {metrics.std_chunk_size:.0f} å­—ç¬¦")
        print(f"\nğŸ¯ æ€»ä½“è¯„åˆ†: {metrics._overall_score():.2%}")
        print(f"  æƒé‡: Cohesion(30%) + InfoGain(25%) + Boundary(20%) + Entity-Rel(15%) + Others(10%)")
        print(f"{'='*70}\n")


# ============================================================================
# ç¤ºä¾‹ä½¿ç”¨
# ============================================================================

async def test_chunking_evaluator():
    """æµ‹è¯•åˆ†å—è¯„ä¼°å™¨"""
    
    # åŸå§‹æ–‡æ¡£
    original_doc = """
    LightRAG is a Simple and Fast Retrieval-Augmented Generation framework.
    LightRAG was developed by HKUDS (Hong Kong University Data Science Lab).
    The framework provides developers with tools to build RAG applications efficiently.
    
    Large language models face several limitations. LLMs have a knowledge cutoff date
    that prevents them from accessing recent information. Large language models generate
    hallucinations when providing responses without factual grounding.
    """
    
    # æ–¹æ³•1: å¥½çš„åˆ†å—ï¼ˆå¥å­è¾¹ç•Œï¼‰
    good_chunks = [
        "LightRAG is a Simple and Fast Retrieval-Augmented Generation framework. LightRAG was developed by HKUDS (Hong Kong University Data Science Lab).",
        "The framework provides developers with tools to build RAG applications efficiently.",
        "Large language models face several limitations. LLMs have a knowledge cutoff date that prevents them from accessing recent information.",
        "Large language models generate hallucinations when providing responses without factual grounding."
    ]
    
    # æ–¹æ³•2: å·®çš„åˆ†å—ï¼ˆéšæœºåˆ‡å‰²ï¼‰
    bad_chunks = [
        "LightRAG is a Simple and Fast Ret",
        "rieval-Augmented Generation framework. LightRAG was dev",
        "eloped by HKUDS (Hong Kong University Data Science Lab). The fram",
        "ework provides developers with tools to build RAG app"
    ]
    
    evaluator = ChunkingEvaluator()
    
    print("æµ‹è¯•å¥½çš„åˆ†å—æ–¹æ³•:")
    good_metrics = await evaluator.evaluate(original_doc, good_chunks)
    
    print("\næµ‹è¯•å·®çš„åˆ†å—æ–¹æ³•:")
    bad_metrics = await evaluator.evaluate(original_doc, bad_chunks)
    
    # å¯¹æ¯”
    print(f"\n{'='*70}")
    print("ğŸ“Š å¯¹æ¯”åˆ†æ")
    print(f"{'='*70}")
    print(f"å¥½çš„åˆ†å—æ€»åˆ†: {good_metrics._overall_score():.2%}")
    print(f"å·®çš„åˆ†å—æ€»åˆ†: {bad_metrics._overall_score():.2%}")
    print(f"å·®å¼‚:         {(good_metrics._overall_score() - bad_metrics._overall_score()):.2%}")


if __name__ == "__main__":
    asyncio.run(test_chunking_evaluator())
