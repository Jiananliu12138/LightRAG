#!/usr/bin/env python3
"""
é‡æ’è´¨é‡è¯„ä¼°å™¨ (Reranking Evaluator)

åŸºäºå­¦æœ¯ç ”ç©¶çš„é‡æ’è¯„ä¼°æŒ‡æ ‡:
1. Precision Gain - é‡æ’åç²¾ç¡®åº¦æå‡
2. nDCG Improvement - NDCGæŒ‡æ ‡æ”¹è¿›
3. MRR Improvement - MRRæŒ‡æ ‡æ”¹è¿›
4. Latency Cost - å»¶è¿Ÿæˆæœ¬
5. Signal-to-Noise Ratio - ä¿¡å™ªæ¯”æ”¹è¿›

References:
- "Learning to Rank for Information Retrieval" (Liu 2009)
- "Pretrained Transformers for Text Ranking" (Nogueira & Cho 2020)
- "RankGPT: LLMs as Re-Ranking Agent" (Sun et al. 2023)
"""

import asyncio
import time
import numpy as np
from typing import List, Dict, Any, Tuple, Callable
from dataclasses import dataclass
from collections import defaultdict


@dataclass
class RerankingMetrics:
    """
    é‡æ’è¯„ä¼°æŒ‡æ ‡
    
    References:
    - Precision Gain: "Learning to Rank" (Liu 2009)
    - nDCG: "Cumulative Gain-based Evaluation" (JÃ¤rvelin & KekÃ¤lÃ¤inen 2002)
    - MRR: Classical IR metric
    """
    precision_gain_at_k: Dict[int, float]      # ç²¾ç¡®åº¦æå‡ @K
    ndcg_improvement_at_k: Dict[int, float]    # NDCGæ”¹è¿› @K
    mrr_improvement: float                      # MRRæ”¹è¿›
    signal_to_noise_improvement: float          # ä¿¡å™ªæ¯”æ”¹è¿›
    avg_latency_ms: float                       # å¹³å‡å»¶è¿Ÿ(æ¯«ç§’)
    latency_quality_ratio: float                # å»¶è¿Ÿ-è´¨é‡æ¯”
    total_queries: int                          # æ€»æŸ¥è¯¢æ•°
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "precision_gain@1": round(self.precision_gain_at_k.get(1, 0.0), 4),
            "precision_gain@3": round(self.precision_gain_at_k.get(3, 0.0), 4),
            "precision_gain@5": round(self.precision_gain_at_k.get(5, 0.0), 4),
            "ndcg_improvement@1": round(self.ndcg_improvement_at_k.get(1, 0.0), 4),
            "ndcg_improvement@3": round(self.ndcg_improvement_at_k.get(3, 0.0), 4),
            "ndcg_improvement@5": round(self.ndcg_improvement_at_k.get(5, 0.0), 4),
            "mrr_improvement": round(self.mrr_improvement, 4),
            "signal_to_noise_improvement": round(self.signal_to_noise_improvement, 4),
            "avg_latency_ms": round(self.avg_latency_ms, 2),
            "latency_quality_ratio": round(self.latency_quality_ratio, 4),
            "total_queries": self.total_queries,
            "overall_score": round(self._overall_score(), 4)
        }
    
    def _overall_score(self) -> float:
        """
        è®¡ç®—æ€»ä½“è¯„åˆ†
        
        æƒé‡åˆ†é… (åŸºäºå®é™…åº”ç”¨é‡è¦æ€§):
        - NDCG Improvement @3: 40%
        - Precision Gain @3: 30%
        - MRR Improvement: 20%
        - Signal-to-Noise: 10%
        """
        return (
            self.ndcg_improvement_at_k.get(3, 0.0) * 0.40 +
            self.precision_gain_at_k.get(3, 0.0) * 0.30 +
            max(self.mrr_improvement, 0.0) * 0.20 +  # MRRå¯èƒ½ä¸ºè´Ÿ
            max(self.signal_to_noise_improvement, 0.0) * 0.10
        )


class RerankingEvaluator:
    """
    é‡æ’è´¨é‡è¯„ä¼°å™¨
    
    è¯„ä¼°é‡æ’æ¨¡å‹å¯¹åˆå§‹æ£€ç´¢ç»“æœçš„æ”¹è¿›ç¨‹åº¦
    """
    
    def __init__(
        self,
        initial_retrieval_func: Callable,  # åˆå§‹æ£€ç´¢å‡½æ•°
        reranking_func: Callable            # é‡æ’å‡½æ•°
    ):
        """
        Args:
            initial_retrieval_func: async def(query: str, top_k: int) -> List[str]
            reranking_func: async def(query: str, doc_ids: List[str]) -> List[str]
        """
        self.initial_retrieval_func = initial_retrieval_func
        self.reranking_func = reranking_func
    
    async def evaluate(
        self,
        test_queries: List[Dict[str, Any]],  # æµ‹è¯•æŸ¥è¯¢
        k_values: List[int] = [1, 3, 5, 10]
    ) -> RerankingMetrics:
        """
        è¯„ä¼°é‡æ’è´¨é‡
        
        Args:
            test_queries: æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨ï¼Œæ ¼å¼:
                {
                    "query": "æŸ¥è¯¢æ–‡æœ¬",
                    "relevant_docs": ["doc1", "doc2", ...],
                    "relevance_scores": {"doc1": 1.0, "doc2": 0.5, ...}
                }
            k_values: è¦è¯„ä¼°çš„Kå€¼åˆ—è¡¨
        
        Returns:
            RerankingMetrics: è¯„ä¼°æŒ‡æ ‡
        """
        print(f"\n{'='*70}")
        print("ğŸ”„ é‡æ’è´¨é‡è¯„ä¼° (Reranking Evaluation)")
        print(f"{'='*70}")
        
        max_k = max(k_values)
        
        # å­˜å‚¨å„æŒ‡æ ‡
        precision_before = defaultdict(list)
        precision_after = defaultdict(list)
        ndcg_before = defaultdict(list)
        ndcg_after = defaultdict(list)
        mrr_before_list = []
        mrr_after_list = []
        snr_before_list = []
        snr_after_list = []
        latencies = []
        
        for idx, test_query in enumerate(test_queries, 1):
            query = test_query["query"]
            relevant_docs = set(test_query["relevant_docs"])
            relevance_scores = test_query.get("relevance_scores", {})
            
            print(f"\næŸ¥è¯¢ {idx}/{len(test_queries)}: {query[:50]}...")
            
            # 1. åˆå§‹æ£€ç´¢
            initial_results = await self.initial_retrieval_func(query, max_k)
            
            # 2. é‡æ’ (è®¡æ—¶)
            start_time = time.time()
            reranked_results = await self.reranking_func(query, initial_results)
            latency = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            latencies.append(latency)
            
            print(f"  åˆå§‹æ£€ç´¢: {len(initial_results)} ä¸ªæ–‡æ¡£")
            print(f"  é‡æ’å»¶è¿Ÿ: {latency:.2f}ms")
            
            # 3. è®¡ç®—å„Kå€¼çš„æŒ‡æ ‡
            for k in k_values:
                # Precision
                p_before = self._calculate_precision(initial_results[:k], relevant_docs)
                p_after = self._calculate_precision(reranked_results[:k], relevant_docs)
                precision_before[k].append(p_before)
                precision_after[k].append(p_after)
                
                # NDCG
                ndcg_before_k = self._calculate_ndcg(initial_results[:k], relevance_scores, k)
                ndcg_after_k = self._calculate_ndcg(reranked_results[:k], relevance_scores, k)
                ndcg_before[k].append(ndcg_before_k)
                ndcg_after[k].append(ndcg_after_k)
            
            # MRR
            mrr_before = self._calculate_mrr(initial_results, relevant_docs)
            mrr_after = self._calculate_mrr(reranked_results, relevant_docs)
            mrr_before_list.append(mrr_before)
            mrr_after_list.append(mrr_after)
            
            # Signal-to-Noise Ratio
            snr_before = self._calculate_snr(initial_results[:10], relevant_docs)
            snr_after = self._calculate_snr(reranked_results[:10], relevant_docs)
            snr_before_list.append(snr_before)
            snr_after_list.append(snr_after)
        
        # è®¡ç®—æ”¹è¿›åº¦
        precision_gain_at_k = {}
        ndcg_improvement_at_k = {}
        
        for k in k_values:
            p_gain = np.mean(precision_after[k]) - np.mean(precision_before[k])
            precision_gain_at_k[k] = p_gain
            
            ndcg_imp = np.mean(ndcg_after[k]) - np.mean(ndcg_before[k])
            ndcg_improvement_at_k[k] = ndcg_imp
        
        mrr_improvement = np.mean(mrr_after_list) - np.mean(mrr_before_list)
        snr_improvement = np.mean(snr_after_list) - np.mean(snr_before_list)
        avg_latency = np.mean(latencies)
        
        # å»¶è¿Ÿ-è´¨é‡æ¯” (è´¨é‡æå‡ / å»¶è¿Ÿæˆæœ¬)
        # è´¨é‡æå‡ç”¨ NDCG@3 improvement è¡¨ç¤º
        quality_gain = ndcg_improvement_at_k.get(3, 0.0)
        latency_cost = avg_latency / 1000  # è½¬æ¢ä¸ºç§’
        latency_quality_ratio = quality_gain / latency_cost if latency_cost > 0 else 0.0
        
        metrics = RerankingMetrics(
            precision_gain_at_k=precision_gain_at_k,
            ndcg_improvement_at_k=ndcg_improvement_at_k,
            mrr_improvement=mrr_improvement,
            signal_to_noise_improvement=snr_improvement,
            avg_latency_ms=avg_latency,
            latency_quality_ratio=latency_quality_ratio,
            total_queries=len(test_queries)
        )
        
        self._display_results(metrics)
        return metrics
    
    def _calculate_precision(self, retrieved: List[str], relevant: set) -> float:
        """è®¡ç®— Precision"""
        if not retrieved:
            return 0.0
        relevant_count = sum(1 for doc in retrieved if doc in relevant)
        return relevant_count / len(retrieved)
    
    def _calculate_ndcg(self, retrieved: List[str], relevance_scores: Dict[str, float], k: int) -> float:
        """
        è®¡ç®— NDCG@K
        
        Reference: JÃ¤rvelin & KekÃ¤lÃ¤inen (2002)
        "Cumulative gain-based evaluation of IR techniques"
        """
        if not retrieved:
            return 0.0
        
        # DCG
        dcg = 0.0
        for idx, doc in enumerate(retrieved[:k], 1):
            relevance = relevance_scores.get(doc, 0.0)
            dcg += relevance / np.log2(idx + 1)
        
        # IDCG
        ideal_relevances = sorted(relevance_scores.values(), reverse=True)[:k]
        idcg = sum(rel / np.log2(idx + 2) for idx, rel in enumerate(ideal_relevances))
        
        return dcg / idcg if idcg > 0 else 0.0
    
    def _calculate_mrr(self, retrieved: List[str], relevant: set) -> float:
        """
        è®¡ç®— MRR (Mean Reciprocal Rank)
        
        Reference: Classical IR metric
        """
        for idx, doc in enumerate(retrieved, 1):
            if doc in relevant:
                return 1.0 / idx
        return 0.0
    
    def _calculate_snr(self, retrieved: List[str], relevant: set) -> float:
        """
        è®¡ç®—ä¿¡å™ªæ¯” (Signal-to-Noise Ratio)
        
        Signal: ç›¸å…³æ–‡æ¡£æ•°
        Noise: ä¸ç›¸å…³æ–‡æ¡£æ•°
        SNR = Signal / (Signal + Noise + epsilon)
        """
        if not retrieved:
            return 0.0
        
        signal = sum(1 for doc in retrieved if doc in relevant)
        noise = len(retrieved) - signal
        epsilon = 1e-6  # é¿å…é™¤é›¶
        
        return signal / (signal + noise + epsilon)
    
    def _display_results(self, metrics: RerankingMetrics):
        """æ˜¾ç¤ºè¯„ä¼°ç»“æœ"""
        print(f"\n{'='*70}")
        print("ğŸ“ˆ é‡æ’è´¨é‡æŒ‡æ ‡ (Academic Metrics)")
        print(f"{'='*70}")
        
        print(f"\nğŸ“Š Precision Gain @K:")
        print(f"  ç†è®ºä¾æ®: Learning to Rank (Liu 2009)")
        for k in sorted(metrics.precision_gain_at_k.keys()):
            gain = metrics.precision_gain_at_k[k]
            sign = "+" if gain >= 0 else ""
            print(f"  â€¢ P@{k:2d}: {sign}{gain:.2%}")
        
        print(f"\nğŸ“Š NDCG Improvement @K:")
        print(f"  ç†è®ºä¾æ®: JÃ¤rvelin & KekÃ¤lÃ¤inen (2002)")
        for k in sorted(metrics.ndcg_improvement_at_k.keys()):
            imp = metrics.ndcg_improvement_at_k[k]
            sign = "+" if imp >= 0 else ""
            print(f"  â€¢ NDCG@{k:2d}: {sign}{imp:.2%}")
        
        print(f"\nğŸ“Š å…¶ä»–æŒ‡æ ‡:")
        mrr_sign = "+" if metrics.mrr_improvement >= 0 else ""
        snr_sign = "+" if metrics.signal_to_noise_improvement >= 0 else ""
        print(f"  â€¢ MRR Improvement:    {mrr_sign}{metrics.mrr_improvement:.4f}")
        print(f"  â€¢ SNR Improvement:    {snr_sign}{metrics.signal_to_noise_improvement:.4f}")
        print(f"  â€¢ Avg Latency:        {metrics.avg_latency_ms:.2f}ms")
        print(f"  â€¢ Quality/Latency:    {metrics.latency_quality_ratio:.4f}")
        
        print(f"\nğŸ¯ æ€»ä½“è¯„åˆ†: {metrics._overall_score():.2%}")
        print(f"  æƒé‡: NDCG_Imp(40%) + Precision_Gain(30%) + MRR_Imp(20%) + SNR_Imp(10%)")
        print(f"{'='*70}\n")


# ============================================================================
# ç¤ºä¾‹ä½¿ç”¨
# ============================================================================

# æ¨¡æ‹Ÿæ–‡æ¡£åº“
MOCK_DOCS = {
    "doc1": "LightRAG is a Simple and Fast Retrieval-Augmented Generation framework",
    "doc2": "LightRAG was developed by HKUDS",
    "doc3": "Python is a high-level programming language",
    "doc4": "RAG systems combine retrieval and generation",
    "doc5": "Vector databases store embeddings efficiently",
    "doc6": "Knowledge graphs represent structured information",
    "doc7": "RAGAS is a framework for evaluating RAG systems",
}


async def mock_initial_retrieval(query: str, top_k: int) -> List[str]:
    """æ¨¡æ‹Ÿåˆå§‹æ£€ç´¢ (ç®€å•çš„å…³é”®è¯åŒ¹é…)"""
    query_words = set(query.lower().split())
    scores = {}
    for doc_id, doc_text in MOCK_DOCS.items():
        doc_words = set(doc_text.lower().split())
        overlap = len(query_words & doc_words)
        scores[doc_id] = overlap
    
    sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    return [doc_id for doc_id, score in sorted_docs[:top_k]]


async def mock_reranking(query: str, doc_ids: List[str]) -> List[str]:
    """æ¨¡æ‹Ÿé‡æ’ (åŸºäºæ›´å¤æ‚çš„ç›¸ä¼¼åº¦è®¡ç®—)"""
    # ç®€åŒ–ï¼šæ·»åŠ ä¸€äº›å»¶è¿Ÿï¼Œæ¨¡æ‹Ÿé‡æ’è®¡ç®—
    await asyncio.sleep(0.01)  # 10ms å»¶è¿Ÿ
    
    # é‡æ’ï¼šæ ¹æ®æ–‡æ¡£ä¸æŸ¥è¯¢çš„æ›´ç²¾ç»†åŒ¹é…
    query_words = query.lower().split()
    scores = {}
    for doc_id in doc_ids:
        doc_text = MOCK_DOCS[doc_id].lower()
        # æ›´å¤æ‚çš„è¯„åˆ†ï¼šè€ƒè™‘è¯åºã€ä½ç½®ç­‰
        score = sum(
            doc_text.index(word) if word in doc_text else 0
            for word in query_words
        )
        scores[doc_id] = score
    
    # é™åºæ’åˆ— (åˆ†æ•°ä½çš„åœ¨å‰ï¼Œå› ä¸ºæ˜¯ç´¢å¼•)
    sorted_docs = sorted(scores.items(), key=lambda x: x[1])
    return [doc_id for doc_id, score in sorted_docs]


async def test_reranking_evaluator():
    """æµ‹è¯•é‡æ’è¯„ä¼°å™¨"""
    
    evaluator = RerankingEvaluator(
        initial_retrieval_func=mock_initial_retrieval,
        reranking_func=mock_reranking
    )
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        {
            "query": "What is LightRAG?",
            "relevant_docs": ["doc1", "doc2"],
            "relevance_scores": {"doc1": 1.0, "doc2": 0.8}
        },
        {
            "query": "How do RAG systems work?",
            "relevant_docs": ["doc1", "doc4", "doc7"],
            "relevance_scores": {"doc1": 0.6, "doc4": 1.0, "doc7": 0.9}
        },
        {
            "query": "Tell me about vector databases",
            "relevant_docs": ["doc5"],
            "relevance_scores": {"doc5": 1.0}
        },
    ]
    
    # è¿è¡Œè¯„ä¼°
    metrics = await evaluator.evaluate(
        test_queries=test_queries,
        k_values=[1, 3, 5]
    )
    
    print(f"\nå®Œæ•´è¯„ä¼°ç»“æœ:\n{metrics.to_dict()}")


if __name__ == "__main__":
    asyncio.run(test_reranking_evaluator())
