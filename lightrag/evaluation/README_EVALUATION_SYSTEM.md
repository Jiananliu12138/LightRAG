# ğŸ¯ RAG ç»„ä»¶çº§è¯„ä¼°ç³»ç»Ÿ

## æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ª**å®Œæ•´çš„ RAG è¯„ä¼°ç³»ç»Ÿ**,ä¸ä»…èƒ½è¯„ä¼°ç«¯åˆ°ç«¯æ€§èƒ½,è¿˜èƒ½æ·±å…¥åˆ†ææ¯ä¸ªç»„ä»¶(åˆ†å—ã€åµŒå…¥ã€æ£€ç´¢ã€ç”Ÿæˆ)çš„æ€§èƒ½ã€‚

### âœ¨ æ ¸å¿ƒç‰¹æ€§

- âœ… **ç»„ä»¶çº§è¯„ä¼°**: ç‹¬ç«‹è¯„ä¼°åˆ†å—ã€åµŒå…¥ã€æ£€ç´¢ã€ç”Ÿæˆè´¨é‡
- âœ… **ç«¯åˆ°ç«¯è¯„ä¼°**: ä½¿ç”¨ RAGAS æ¡†æ¶è¯„ä¼°æ•´ä½“æ€§èƒ½
- âœ… **çµæ´»å¯¹æ¯”**: æ”¯æŒä¸åŒé…ç½®ã€ä¸åŒç³»ç»Ÿçš„æ€§èƒ½å¯¹æ¯”
- âœ… **å¯è§†åŒ–æŠ¥å‘Š**: ç”Ÿæˆè¯¦ç»†çš„ CSV å’Œ HTML æŠ¥å‘Š
- âœ… **æ˜“äºæ‰©å±•**: æ¨¡å—åŒ–è®¾è®¡,è½»æ¾æ·»åŠ æ–°çš„è¯„ä¼°æŒ‡æ ‡

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
RAG è¯„ä¼°ç³»ç»Ÿ
â”œâ”€â”€ component_evaluators/           # ç»„ä»¶è¯„ä¼°å™¨
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chunking_evaluator.py      # åˆ†å—è´¨é‡è¯„ä¼°
â”‚   â”œâ”€â”€ embedding_evaluator.py     # åµŒå…¥è´¨é‡è¯„ä¼°
â”‚   â””â”€â”€ retrieval_evaluator.py     # æ£€ç´¢æ€§èƒ½è¯„ä¼°
â”‚
â”œâ”€â”€ rag_evaluator_system.py        # å®Œæ•´è¯„ä¼°ç³»ç»Ÿ
â”œâ”€â”€ evaluate_lightrag_complete.py  # LightRAG é›†æˆè„šæœ¬
â”œâ”€â”€ quick_start_example.py         # å¿«é€Ÿå…¥é—¨ç¤ºä¾‹
â”œâ”€â”€ eval_rag_quality.py            # ç«¯åˆ°ç«¯è¯„ä¼° (RAGAS)
â”‚
â””â”€â”€ æ–‡æ¡£
    â”œâ”€â”€ README_EVALUATION_SYSTEM.md   # æœ¬æ–‡ä»¶
    â””â”€â”€ RAG_EVALUATION_GUIDE.md       # è¯¦ç»†ä½¿ç”¨æŒ‡å—
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1ï¸âƒ£ å®‰è£…ä¾èµ–

```bash
pip install numpy pandas scikit-learn matplotlib ragas langchain
```

### 2ï¸âƒ£ è¿è¡Œå¿«é€Ÿç¤ºä¾‹

```bash
cd /home/ik2200-2025-g2/WorkZone/LightRAG/lightrag/evaluation
python quick_start_example.py
```

**è¾“å‡ºç¤ºä¾‹:**
```
ğŸš€ RAG è¯„ä¼°ç³»ç»Ÿ - å¿«é€Ÿå…¥é—¨ç¤ºä¾‹

ğŸ“Š åˆ†å—æ–¹æ³•å¯¹æ¯”
å›ºå®šå¤§å°åˆ†å—: 48.31%
æŒ‰æ®µè½åˆ†å—:   100.00%
æŒ‰å¥å­åˆ†å—:   98.55%

ğŸ† æœ€ä½³æ–¹æ³•: æŒ‰æ®µè½åˆ†å—

ğŸ“ˆ æ£€ç´¢è´¨é‡æŒ‡æ ‡
  â€¢ P@5: 40.00%
  â€¢ R@5: 100.00%
  â€¢ MRR: 1.0000

ğŸ¯ æ€»ä½“è¯„åˆ†: 84.11%
```

### 3ï¸âƒ£ è¯„ä¼° LightRAG ç³»ç»Ÿ

```bash
# ç¡®ä¿ LightRAG æœåŠ¡è¿è¡Œåœ¨ http://localhost:9621
# ç¡®ä¿ Ollama æœåŠ¡è¿è¡Œåœ¨ http://localhost:11434

python evaluate_lightrag_complete.py
```

---

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡è¯¦è§£

### åˆ†å—è¯„ä¼° (Chunking)

| æŒ‡æ ‡ | è¯´æ˜ | æœ€ä½³å€¼ |
|------|------|--------|
| è¯­ä¹‰å®Œæ•´æ€§ | chunk æ˜¯å¦ä¿æŒè¯­ä¹‰å®Œæ•´ | è¶Šé«˜è¶Šå¥½ |
| è¾¹ç•Œè´¨é‡ | åˆ†å—è¾¹ç•Œæ˜¯å¦åˆç† | è¶Šé«˜è¶Šå¥½ |
| å¤§å°ä¸€è‡´æ€§ | chunk å¤§å°æ˜¯å¦å‡åŒ€ | è¶Šé«˜è¶Šå¥½ |
| ä¿¡æ¯å¯†åº¦ | chunk ä¿¡æ¯å«é‡ | è¶Šé«˜è¶Šå¥½ |
| è¦†ç›–ç‡ | æ˜¯å¦è¦†ç›–åŸæ–‡æ¡£ | è¶Šé«˜è¶Šå¥½ |

### åµŒå…¥è¯„ä¼° (Embedding)

| æŒ‡æ ‡ | è¯´æ˜ | æœ€ä½³å€¼ |
|------|------|--------|
| è¯­ä¹‰ç›¸ä¼¼åº¦ä¿æŒ | åµŒå…¥æ˜¯å¦ä¿æŒæ–‡æœ¬ç›¸ä¼¼åº¦ | è¶Šé«˜è¶Šå¥½ |
| ä¸»é¢˜åŒºåˆ†åº¦ | ä¸åŒä¸»é¢˜æ˜¯å¦èƒ½åŒºåˆ† | è¶Šé«˜è¶Šå¥½ |
| æ£€ç´¢å‡†ç¡®ç‡ | åŸºäºåµŒå…¥çš„æ£€ç´¢å‡†ç¡®æ€§ | è¶Šé«˜è¶Šå¥½ |

### æ£€ç´¢è¯„ä¼° (Retrieval)

| æŒ‡æ ‡ | è¯´æ˜ | å…¬å¼ |
|------|------|------|
| Precision@K | å‰Kä¸ªç»“æœçš„å‡†ç¡®ç‡ | TP/(TP+FP) |
| Recall@K | å‰Kä¸ªç»“æœçš„å¬å›ç‡ | TP/(TP+FN) |
| MRR | å¹³å‡å€’æ•°æ’å | 1/rank |
| NDCG@K | å½’ä¸€åŒ–æŠ˜æŸç´¯ç§¯å¢ç›Š | DCG/IDCG |

### ç«¯åˆ°ç«¯è¯„ä¼° (RAGAS)

| æŒ‡æ ‡ | è¯´æ˜ |
|------|------|
| Faithfulness | ç­”æ¡ˆå¿ å®åº¦ |
| Answer Relevancy | ç­”æ¡ˆç›¸å…³æ€§ |
| Context Recall | ä¸Šä¸‹æ–‡å¬å›ç‡ |
| Context Precision | ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ |

---

## ğŸ“– ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: ä¼˜åŒ–åˆ†å—ç­–ç•¥

```python
from component_evaluators import ChunkingEvaluator

# æµ‹è¯•ä¸åŒçš„åˆ†å—å¤§å°
chunk_sizes = [128, 256, 512, 1024]
best_score = 0
best_size = 0

for size in chunk_sizes:
    chunks = create_chunks(document, size=size, overlap=size//5)
    metrics = await evaluator.evaluate(document, chunks)
    
    if metrics._overall_score() > best_score:
        best_score = metrics._overall_score()
        best_size = size

print(f"æœ€ä½³åˆ†å—å¤§å°: {best_size} (å¾—åˆ†: {best_score:.2%})")
```

### åœºæ™¯ 2: å¯¹æ¯”ä¸åŒåµŒå…¥æ¨¡å‹

```python
from component_evaluators import EmbeddingEvaluator

models = ["nomic-embed-text", "bge-m3", "text-embedding-3-small"]
results = {}

for model in models:
    embedding_func = create_embedding_func(model)
    evaluator = EmbeddingEvaluator(embedding_func)
    metrics = await evaluator.evaluate(test_pairs=test_data)
    results[model] = metrics._overall_score()

best_model = max(results, key=results.get)
print(f"æœ€ä½³åµŒå…¥æ¨¡å‹: {best_model} ({results[best_model]:.2%})")
```

### åœºæ™¯ 3: ä¼˜åŒ–æ£€ç´¢å‚æ•°

```python
from component_evaluators import RetrievalEvaluator

# æµ‹è¯•ä¸åŒçš„ top_k
for k in [3, 5, 10, 15, 20]:
    retrieval_func = create_retrieval_func(top_k=k)
    evaluator = RetrievalEvaluator(retrieval_func)
    metrics = await evaluator.evaluate(test_queries=queries)
    
    print(f"K={k}: P@K={metrics.precision_at_k[k]:.2%}, " 
          f"R@K={metrics.recall_at_k[k]:.2%}, "
          f"MRR={metrics.mrr:.4f}")
```

### åœºæ™¯ 4: å¯¹æ¯”ä¸åŒ RAG ç³»ç»Ÿ

```python
from rag_evaluator_system import RAGEvaluationSystem, RAGSystemConfig

eval_system = RAGEvaluationSystem()

# è¯„ä¼° LightRAG
config_lightrag = RAGSystemConfig(
    name="LightRAG",
    chunking_method="fixed_size",
    chunk_size=512,
    embedding_model="nomic-embed-text",
    # ... å…¶ä»–é…ç½®
)
result_lightrag = await eval_system.evaluate_system(config_lightrag, ...)

# è¯„ä¼° LlamaIndex
config_llamaindex = RAGSystemConfig(
    name="LlamaIndex",
    # ... é…ç½®
)
result_llamaindex = await eval_system.evaluate_system(config_llamaindex, ...)

# ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
eval_system.compare_systems([result_lightrag, result_llamaindex])
```

---

## ğŸ“ è¾“å‡ºæ–‡ä»¶

è¯„ä¼°å®Œæˆå,ä¼šåœ¨è¾“å‡ºç›®å½•ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶:

```
lightrag_evaluation_results/
â”œâ”€â”€ LightRAG_Small_Chunks_20260121_143022.json  # è¯¦ç»†è¯„ä¼°ç»“æœ
â”œâ”€â”€ comparison_20260121_143200.csv              # å¯¹æ¯”è¡¨æ ¼
â””â”€â”€ report_20260121_143200.html                 # HTML æŠ¥å‘Š
```

### JSON ç»“æœç¤ºä¾‹

```json
{
  "config": {
    "name": "LightRAG_Small_Chunks_256",
    "chunking_method": "fixed_size",
    "chunk_size": 256,
    "embedding_model": "nomic-embed-text",
    "embedding_dim": 768,
    "retrieval_method": "hybrid",
    "top_k": 10,
    "llm_model": "qwen2.5:7b-instruct"
  },
  "chunking_metrics": {
    "semantic_completeness": 0.85,
    "boundary_quality": 0.92,
    "size_consistency": 0.78,
    "information_density": 0.88,
    "coverage": 0.95,
    "overall_score": 0.8760
  },
  "embedding_metrics": {
    "semantic_similarity_preservation": 0.82,
    "topic_separation": 0.78,
    "retrieval_accuracy": 0.90,
    "overall_score": 0.8350
  },
  "retrieval_metrics": {
    "precision@5": 0.70,
    "recall@5": 0.75,
    "mrr": 0.85,
    "ndcg@5": 0.72,
    "overall_score": 0.7375
  },
  "end_to_end_metrics": {
    "faithfulness": 0.85,
    "answer_relevancy": 0.78,
    "context_recall": 0.92,
    "context_precision": 0.88,
    "ragas_score": 0.8675
  },
  "timestamp": "2026-01-21T14:30:22.123456"
}
```

---

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡

æ‚¨å¯ä»¥æ‰©å±•ç°æœ‰çš„è¯„ä¼°å™¨:

```python
from component_evaluators import ChunkingEvaluator

class MyCustomChunkingEvaluator(ChunkingEvaluator):
    def _evaluate_custom_metric(self, chunks):
        """æ‚¨çš„è‡ªå®šä¹‰è¯„ä¼°é€»è¾‘"""
        score = 0.0
        # ... è®¡ç®—é€»è¾‘
        return score
    
    async def evaluate(self, original_document, chunks):
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        metrics = await super().evaluate(original_document, chunks)
        
        # æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡
        custom_score = self._evaluate_custom_metric(chunks)
        
        return metrics
```

### é›†æˆåˆ° CI/CD æµç¨‹

```bash
#!/bin/bash
# ci_evaluate.sh

# è¿è¡Œè¯„ä¼°
python evaluate_lightrag_complete.py

# æ£€æŸ¥è¯„ä¼°ç»“æœ
SCORE=$(jq '.end_to_end_metrics.ragas_score' results/latest.json)

if (( $(echo "$SCORE < 0.8" | bc -l) )); then
    echo "âŒ RAGAS åˆ†æ•°è¿‡ä½: $SCORE < 0.8"
    exit 1
fi

echo "âœ… è¯„ä¼°é€šè¿‡: RAGAS åˆ†æ•° = $SCORE"
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. å»ºç«‹åŸºçº¿

```python
# ç¬¬ä¸€æ¬¡è¯„ä¼°æ—¶å»ºç«‹åŸºçº¿
baseline_result = await eval_system.evaluate_system(current_config, ...)

# ä¿å­˜åŸºçº¿
with open("baseline.json", "w") as f:
    json.dump(baseline_result.to_dict(), f)

# åç»­è¯„ä¼°æ—¶å¯¹æ¯”åŸºçº¿
current_result = await eval_system.evaluate_system(new_config, ...)
compare_with_baseline(baseline_result, current_result)
```

### 2. é€ç»„ä»¶ä¼˜åŒ–

```
ä¼˜åŒ–æµç¨‹:
1. è¿è¡Œå®Œæ•´è¯„ä¼°,æ‰¾å‡ºç“¶é¢ˆ
2. ä¼˜åŒ–å•ä¸ªç»„ä»¶(å¦‚åˆ†å—)
3. é‡æ–°è¯„ä¼°è¯¥ç»„ä»¶
4. è¯„ä¼°å¯¹ç«¯åˆ°ç«¯æ€§èƒ½çš„å½±å“
5. é‡å¤æ­¥éª¤ 2-4 ç›´åˆ°æ»¡æ„
```

### 3. A/B æµ‹è¯•

```python
# é…ç½® A
config_a = RAGSystemConfig(name="Config_A", chunk_size=256, ...)

# é…ç½® B
config_b = RAGSystemConfig(name="Config_B", chunk_size=512, ...)

# è¯„ä¼°ä¸¤ä¸ªé…ç½®
result_a = await eval_system.evaluate_system(config_a, ...)
result_b = await eval_system.evaluate_system(config_b, ...)

# å¯¹æ¯”
eval_system.compare_systems([result_a, result_b])
```

### 4. å®šæœŸç›‘æ§

```python
# è®¾ç½®å®šæœŸè¯„ä¼°ä»»åŠ¡ (å¦‚æ¯å‘¨)
import schedule

def weekly_evaluation():
    result = await eval_system.evaluate_system(...)
    save_to_monitoring_db(result)
    check_performance_degradation(result)

schedule.every().monday.at("02:00").do(weekly_evaluation)
```

---

## ğŸ” æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: åˆ†å—è¯„åˆ†ä½

**å¯èƒ½åŸå› :**
- è¾¹ç•Œåˆ‡æ–­å¥å­
- Chunk å¤§å°ä¸ä¸€è‡´

**è§£å†³æ–¹æ¡ˆ:**
```python
# ä½¿ç”¨å¥å­è¾¹ç•Œåˆ†å—
chunks = chunk_by_sentence(doc, max_sentences=3)

# æˆ–è°ƒæ•´å‚æ•°
chunks = chunk_fixed_size(doc, size=512, overlap=100)
```

### é—®é¢˜ 2: æ£€ç´¢è¯„åˆ†ä½

**å¯èƒ½åŸå› :**
- åµŒå…¥æ¨¡å‹ä¸é€‚åˆé¢†åŸŸ
- Top-K ä¸åˆé€‚

**è§£å†³æ–¹æ¡ˆ:**
```python
# å°è¯•ä¸åŒåµŒå…¥æ¨¡å‹
for model in ["nomic-embed-text", "bge-m3"]:
    test_embedding_model(model)

# è°ƒæ•´ top_k
for k in [3, 5, 10, 15]:
    test_retrieval_with_k(k)
```

### é—®é¢˜ 3: ç«¯åˆ°ç«¯è¯„åˆ†ä½ä½†ç»„ä»¶è¯„åˆ†é«˜

**å¯èƒ½åŸå› :**
- LLM ç”Ÿæˆèƒ½åŠ›ä¸è¶³
- Prompt è®¾è®¡ä¸ä½³

**è§£å†³æ–¹æ¡ˆ:**
```python
# å‡çº§ LLM æ¨¡å‹
config.llm_model = "qwen2.5:14b-instruct"  # æ›´å¤§çš„æ¨¡å‹

# ä¼˜åŒ– Prompt
# ä¿®æ”¹ LightRAG çš„ prompt æ¨¡æ¿
```

---

## ğŸ“š ç›¸å…³èµ„æº

- [è¯¦ç»†ä½¿ç”¨æŒ‡å—](RAG_EVALUATION_GUIDE.md)
- [RAGAS å®˜æ–¹æ–‡æ¡£](https://docs.ragas.io/)
- [LightRAG é¡¹ç›®](https://github.com/HKUDS/LightRAG)

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®æ–°çš„è¯„ä¼°æŒ‡æ ‡å’ŒåŠŸèƒ½ï¼

### æ·»åŠ æ–°çš„è¯„ä¼°æŒ‡æ ‡

1. åœ¨ç›¸åº”çš„ evaluator ä¸­æ·»åŠ æ–¹æ³•
2. æ›´æ–° Metrics æ•°æ®ç±»
3. æ·»åŠ æµ‹è¯•ç”¨ä¾‹
4. æ›´æ–°æ–‡æ¡£

---

## ğŸ“„ è®¸å¯

MIT License

---

## âœ¨ æ€»ç»“

è¿™ä¸ªè¯„ä¼°ç³»ç»Ÿä¸ºæ‚¨æä¾›äº†:

âœ… **å…¨é¢çš„æ€§èƒ½æ´å¯Ÿ**: ä»ç»„ä»¶åˆ°ç³»ç»Ÿçš„å®Œæ•´è¯„ä¼°  
âœ… **æ•°æ®é©±åŠ¨çš„ä¼˜åŒ–**: åŸºäºå®¢è§‚æŒ‡æ ‡åšå†³ç­–  
âœ… **çµæ´»çš„å¯¹æ¯”åˆ†æ**: è½»æ¾å¯¹æ¯”ä¸åŒé…ç½®å’Œç³»ç»Ÿ  
âœ… **æŒç»­ç›‘æ§èƒ½åŠ›**: è¿½è¸ªç³»ç»Ÿæ€§èƒ½å˜åŒ–

ç«‹å³å¼€å§‹è¯„ä¼°æ‚¨çš„ RAG ç³»ç»Ÿ:

```bash
python quick_start_example.py
```

---

**ä½œè€…**: LightRAG è¯„ä¼°å›¢é˜Ÿ  
**ç‰ˆæœ¬**: 1.0.0  
**æ›´æ–°æ—¶é—´**: 2026-01-21
