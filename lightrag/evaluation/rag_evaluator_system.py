#!/usr/bin/env python3
"""
å®Œæ•´çš„ RAG è¯„ä¼°ç³»ç»Ÿ (RAG Evaluation System)

åŠŸèƒ½:
1. ç»„ä»¶çº§è¯„ä¼°ï¼šåˆ†å—ã€åµŒå…¥ã€æ£€ç´¢ã€ç”Ÿæˆ
2. ç«¯åˆ°ç«¯è¯„ä¼°ï¼šä½¿ç”¨ RAGAS
3. ç³»ç»Ÿå¯¹æ¯”ï¼šä¸åŒé…ç½®/ç³»ç»Ÿçš„æ€§èƒ½å¯¹æ¯”
4. æŠ¥å‘Šç”Ÿæˆï¼šç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
"""

import asyncio
import json
import logging
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass, asdict
from pathlib import Path
from datetime import datetime
import pandas as pd

from component_evaluators import (
    ChunkingEvaluator, ChunkingMetrics,
    EmbeddingEvaluator, EmbeddingMetrics,
    RetrievalEvaluator, RetrievalMetrics
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class RAGSystemConfig:
    """RAG ç³»ç»Ÿé…ç½®"""
    name: str                          # ç³»ç»Ÿåç§°
    chunking_method: str               # åˆ†å—æ–¹æ³• (e.g., "fixed_size", "sentence", "semantic")
    chunk_size: int                    # Chunk å¤§å°
    chunk_overlap: int                 # Chunk é‡å 
    embedding_model: str               # åµŒå…¥æ¨¡å‹
    embedding_dim: int                 # åµŒå…¥ç»´åº¦
    retrieval_method: str              # æ£€ç´¢æ–¹æ³• (e.g., "vector", "graph", "hybrid")
    top_k: int                         # æ£€ç´¢ Top-K
    llm_model: str                     # ç”Ÿæˆæ¨¡å‹
    rerank_model: Optional[str] = None # é‡æ’æ¨¡å‹
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class RAGEvaluationResult:
    """RAG è¯„ä¼°ç»“æœ"""
    config: RAGSystemConfig
    chunking_metrics: Optional[ChunkingMetrics]
    embedding_metrics: Optional[EmbeddingMetrics]
    retrieval_metrics: Optional[RetrievalMetrics]
    end_to_end_metrics: Optional[Dict[str, Any]]  # RAGAS æŒ‡æ ‡
    timestamp: str
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "config": self.config.to_dict(),
            "chunking_metrics": self.chunking_metrics.to_dict() if self.chunking_metrics else None,
            "embedding_metrics": self.embedding_metrics.to_dict() if self.embedding_metrics else None,
            "retrieval_metrics": self.retrieval_metrics.to_dict() if self.retrieval_metrics else None,
            "end_to_end_metrics": self.end_to_end_metrics,
            "timestamp": self.timestamp
        }


class RAGEvaluationSystem:
    """å®Œæ•´çš„ RAG è¯„ä¼°ç³»ç»Ÿ"""
    
    def __init__(
        self,
        output_dir: Path = Path("./evaluation_results")
    ):
        """
        Args:
            output_dir: è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.results: List[RAGEvaluationResult] = []
    
    async def evaluate_system(
        self,
        config: RAGSystemConfig,
        # åˆ†å—è¯„ä¼°å‚æ•°
        test_document: Optional[str] = None,
        chunks: Optional[List[str]] = None,
        # åµŒå…¥è¯„ä¼°å‚æ•°
        embedding_func: Optional[Callable] = None,
        embedding_test_pairs: Optional[List] = None,
        embedding_test_clusters: Optional[List] = None,
        # æ£€ç´¢è¯„ä¼°å‚æ•°
        retrieval_func: Optional[Callable] = None,
        retrieval_test_queries: Optional[List] = None,
        # ç«¯åˆ°ç«¯è¯„ä¼°å‚æ•°
        end_to_end_func: Optional[Callable] = None,
        end_to_end_test_cases: Optional[List] = None,
        # è¯„ä¼°é€‰é¡¹
        evaluate_chunking: bool = True,
        evaluate_embedding: bool = True,
        evaluate_retrieval: bool = True,
        evaluate_end_to_end: bool = True
    ) -> RAGEvaluationResult:
        """
        è¯„ä¼°å®Œæ•´çš„ RAG ç³»ç»Ÿ
        
        Args:
            config: RAG ç³»ç»Ÿé…ç½®
            test_document: ç”¨äºåˆ†å—è¯„ä¼°çš„æµ‹è¯•æ–‡æ¡£
            chunks: åˆ†å—åçš„ç»“æœ
            embedding_func: åµŒå…¥å‡½æ•°
            embedding_test_pairs: åµŒå…¥è¯„ä¼°æµ‹è¯•å¯¹
            embedding_test_clusters: åµŒå…¥è¯„ä¼°æµ‹è¯•ç°‡
            retrieval_func: æ£€ç´¢å‡½æ•°
            retrieval_test_queries: æ£€ç´¢è¯„ä¼°æµ‹è¯•æŸ¥è¯¢
            end_to_end_func: ç«¯åˆ°ç«¯è¯„ä¼°å‡½æ•°ï¼ˆRAGASï¼‰
            end_to_end_test_cases: ç«¯åˆ°ç«¯è¯„ä¼°æµ‹è¯•ç”¨ä¾‹
            evaluate_chunking: æ˜¯å¦è¯„ä¼°åˆ†å—
            evaluate_embedding: æ˜¯å¦è¯„ä¼°åµŒå…¥
            evaluate_retrieval: æ˜¯å¦è¯„ä¼°æ£€ç´¢
            evaluate_end_to_end: æ˜¯å¦è¯„ä¼°ç«¯åˆ°ç«¯
        
        Returns:
            RAGEvaluationResult: è¯„ä¼°ç»“æœ
        """
        print(f"\n{'='*80}")
        print(f"ğŸš€ å¼€å§‹è¯„ä¼° RAG ç³»ç»Ÿ: {config.name}")
        print(f"{'='*80}\n")
        
        chunking_metrics = None
        embedding_metrics = None
        retrieval_metrics = None
        end_to_end_metrics = None
        
        # 1. åˆ†å—è¯„ä¼°
        if evaluate_chunking and test_document and chunks:
            logger.info("ğŸ“Š æ­¥éª¤ 1/4: åˆ†å—è¯„ä¼°")
            chunking_evaluator = ChunkingEvaluator()
            chunking_metrics = await chunking_evaluator.evaluate(
                original_document=test_document,
                chunks=chunks
            )
        
        # 2. åµŒå…¥è¯„ä¼°
        if evaluate_embedding and embedding_func and embedding_test_pairs:
            logger.info("ğŸ“Š æ­¥éª¤ 2/4: åµŒå…¥è¯„ä¼°")
            embedding_evaluator = EmbeddingEvaluator(embedding_func=embedding_func)
            embedding_metrics = await embedding_evaluator.evaluate(
                test_pairs=embedding_test_pairs,
                test_clusters=embedding_test_clusters
            )
        
        # 3. æ£€ç´¢è¯„ä¼°
        if evaluate_retrieval and retrieval_func and retrieval_test_queries:
            logger.info("ğŸ“Š æ­¥éª¤ 3/4: æ£€ç´¢è¯„ä¼°")
            retrieval_evaluator = RetrievalEvaluator(retrieval_func=retrieval_func)
            retrieval_metrics = await retrieval_evaluator.evaluate(
                test_queries=retrieval_test_queries
            )
        
        # 4. ç«¯åˆ°ç«¯è¯„ä¼° (RAGAS)
        if evaluate_end_to_end and end_to_end_func and end_to_end_test_cases:
            logger.info("ğŸ“Š æ­¥éª¤ 4/4: ç«¯åˆ°ç«¯è¯„ä¼° (RAGAS)")
            end_to_end_metrics = await end_to_end_func(end_to_end_test_cases)
        
        # åˆ›å»ºè¯„ä¼°ç»“æœ
        result = RAGEvaluationResult(
            config=config,
            chunking_metrics=chunking_metrics,
            embedding_metrics=embedding_metrics,
            retrieval_metrics=retrieval_metrics,
            end_to_end_metrics=end_to_end_metrics,
            timestamp=datetime.now().isoformat()
        )
        
        # ä¿å­˜ç»“æœ
        self.results.append(result)
        self._save_result(result)
        
        print(f"\n{'='*80}")
        print(f"âœ… è¯„ä¼°å®Œæˆ: {config.name}")
        print(f"{'='*80}\n")
        
        return result
    
    def compare_systems(
        self,
        results: Optional[List[RAGEvaluationResult]] = None
    ) -> pd.DataFrame:
        """
        å¯¹æ¯”å¤šä¸ª RAG ç³»ç»Ÿ
        
        Args:
            results: è¦å¯¹æ¯”çš„è¯„ä¼°ç»“æœåˆ—è¡¨ï¼ˆé»˜è®¤ä½¿ç”¨æ‰€æœ‰å·²è¯„ä¼°çš„ç³»ç»Ÿï¼‰
        
        Returns:
            pd.DataFrame: å¯¹æ¯”è¡¨æ ¼
        """
        if results is None:
            results = self.results
        
        if not results:
            logger.warning("æ²¡æœ‰å¯å¯¹æ¯”çš„ç»“æœ")
            return pd.DataFrame()
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š ç³»ç»Ÿå¯¹æ¯”åˆ†æ ({len(results)} ä¸ªç³»ç»Ÿ)")
        print(f"{'='*80}\n")
        
        # æ„å»ºå¯¹æ¯”è¡¨æ ¼
        comparison_data = []
        
        for result in results:
            row = {
                "ç³»ç»Ÿåç§°": result.config.name,
                "åˆ†å—æ–¹æ³•": result.config.chunking_method,
                "Chunkå¤§å°": result.config.chunk_size,
                "åµŒå…¥æ¨¡å‹": result.config.embedding_model,
                "åµŒå…¥ç»´åº¦": result.config.embedding_dim,
                "æ£€ç´¢æ–¹æ³•": result.config.retrieval_method,
                "Top-K": result.config.top_k,
                "LLMæ¨¡å‹": result.config.llm_model,
            }
            
            # æ·»åŠ åˆ†å—æŒ‡æ ‡
            if result.chunking_metrics:
                row["åˆ†å—-æ€»åˆ†"] = result.chunking_metrics._overall_score()
                row["åˆ†å—-è¯­ä¹‰å®Œæ•´æ€§"] = result.chunking_metrics.semantic_completeness
                row["åˆ†å—-è¾¹ç•Œè´¨é‡"] = result.chunking_metrics.boundary_quality
            
            # æ·»åŠ åµŒå…¥æŒ‡æ ‡
            if result.embedding_metrics:
                row["åµŒå…¥-æ€»åˆ†"] = result.embedding_metrics._overall_score()
                row["åµŒå…¥-æ£€ç´¢å‡†ç¡®ç‡"] = result.embedding_metrics.retrieval_accuracy
            
            # æ·»åŠ æ£€ç´¢æŒ‡æ ‡
            if result.retrieval_metrics:
                row["æ£€ç´¢-æ€»åˆ†"] = result.retrieval_metrics._overall_score()
                row["æ£€ç´¢-P@5"] = result.retrieval_metrics.precision_at_k.get(5, 0.0)
                row["æ£€ç´¢-R@5"] = result.retrieval_metrics.recall_at_k.get(5, 0.0)
                row["æ£€ç´¢-MRR"] = result.retrieval_metrics.mrr
            
            # æ·»åŠ ç«¯åˆ°ç«¯æŒ‡æ ‡ (RAGAS)
            if result.end_to_end_metrics:
                row["ç«¯åˆ°ç«¯-Faithfulness"] = result.end_to_end_metrics.get("faithfulness", 0.0)
                row["ç«¯åˆ°ç«¯-AnswerRelevancy"] = result.end_to_end_metrics.get("answer_relevancy", 0.0)
                row["ç«¯åˆ°ç«¯-ContextRecall"] = result.end_to_end_metrics.get("context_recall", 0.0)
                row["ç«¯åˆ°ç«¯-ContextPrecision"] = result.end_to_end_metrics.get("context_precision", 0.0)
            
            comparison_data.append(row)
        
        df = pd.DataFrame(comparison_data)
        
        # æ˜¾ç¤ºå¯¹æ¯”è¡¨æ ¼
        print(df.to_string(index=False))
        
        # ä¿å­˜å¯¹æ¯”è¡¨æ ¼
        comparison_file = self.output_dir / f"comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(comparison_file, index=False, encoding='utf-8-sig')
        logger.info(f"ğŸ’¾ å¯¹æ¯”è¡¨æ ¼å·²ä¿å­˜: {comparison_file}")
        
        # æ‰¾å‡ºæœ€ä½³ç³»ç»Ÿ
        self._highlight_best_systems(df)
        
        return df
    
    def _highlight_best_systems(self, df: pd.DataFrame):
        """é«˜äº®æ˜¾ç¤ºæœ€ä½³ç³»ç»Ÿ"""
        print(f"\n{'='*80}")
        print("ğŸ† æœ€ä½³ç³»ç»Ÿ")
        print(f"{'='*80}")
        
        metrics = [
            ("åˆ†å—-æ€»åˆ†", "åˆ†å—è´¨é‡æœ€ä½³"),
            ("åµŒå…¥-æ€»åˆ†", "åµŒå…¥è´¨é‡æœ€ä½³"),
            ("æ£€ç´¢-æ€»åˆ†", "æ£€ç´¢è´¨é‡æœ€ä½³"),
            ("ç«¯åˆ°ç«¯-Faithfulness", "ç«¯åˆ°ç«¯-å¿ å®åº¦æœ€ä½³"),
        ]
        
        for metric, description in metrics:
            if metric in df.columns:
                best_idx = df[metric].idxmax()
                best_system = df.loc[best_idx, "ç³»ç»Ÿåç§°"]
                best_score = df.loc[best_idx, metric]
                print(f"  â€¢ {description}: {best_system} ({best_score:.2%})")
    
    def _save_result(self, result: RAGEvaluationResult):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        # ä¿å­˜ä¸º JSON
        result_file = self.output_dir / f"{result.config.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result.to_dict(), f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜: {result_file}")
    
    def generate_report(self, output_file: Optional[Path] = None):
        """
        ç”Ÿæˆ HTML è¯„ä¼°æŠ¥å‘Š
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if output_file is None:
            output_file = self.output_dir / f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        
        # TODO: ç”Ÿæˆè¯¦ç»†çš„ HTML æŠ¥å‘Š
        logger.info(f"ğŸ“„ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š: {output_file}")
        
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>RAG ç³»ç»Ÿè¯„ä¼°æŠ¥å‘Š</title>
            <meta charset="utf-8">
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                h1 {{ color: #333; }}
                table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
                th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
                th {{ background-color: #4CAF50; color: white; }}
                tr:nth-child(even) {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <h1>ğŸ¯ RAG ç³»ç»Ÿè¯„ä¼°æŠ¥å‘Š</h1>
            <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            <p>è¯„ä¼°ç³»ç»Ÿæ•°: {len(self.results)}</p>
            
            <h2>è¯„ä¼°ç»“æœæ‘˜è¦</h2>
            {self._generate_summary_html()}
            
            <h2>è¯¦ç»†è¯„ä¼°ç»“æœ</h2>
            {self._generate_detailed_html()}
        </body>
        </html>
        """
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
    
    def _generate_summary_html(self) -> str:
        """ç”Ÿæˆæ‘˜è¦ HTML"""
        # ç®€åŒ–ç‰ˆæ‘˜è¦
        return "<p>è¯„ä¼°æ‘˜è¦ï¼ˆå¾…å®ç°ï¼‰</p>"
    
    def _generate_detailed_html(self) -> str:
        """ç”Ÿæˆè¯¦ç»†ç»“æœ HTML"""
        # ç®€åŒ–ç‰ˆè¯¦ç»†ç»“æœ
        return "<p>è¯¦ç»†ç»“æœï¼ˆå¾…å®ç°ï¼‰</p>"


# ============================================================================
# ç¤ºä¾‹ä½¿ç”¨
# ============================================================================

async def mock_end_to_end_evaluation(test_cases: List) -> Dict[str, Any]:
    """æ¨¡æ‹Ÿç«¯åˆ°ç«¯è¯„ä¼°ï¼ˆå®é™…åº”è¯¥è°ƒç”¨ RAGASï¼‰"""
    return {
        "faithfulness": 0.85,
        "answer_relevancy": 0.78,
        "context_recall": 0.92,
        "context_precision": 0.88
    }


async def test_rag_evaluation_system():
    """æµ‹è¯• RAG è¯„ä¼°ç³»ç»Ÿ"""
    
    eval_system = RAGEvaluationSystem(output_dir=Path("./test_evaluation_results"))
    
    # é…ç½®1: LightRAG with small chunks
    config1 = RAGSystemConfig(
        name="LightRAG_Small_Chunks",
        chunking_method="fixed_size",
        chunk_size=256,
        chunk_overlap=50,
        embedding_model="nomic-embed-text",
        embedding_dim=768,
        retrieval_method="hybrid",
        top_k=10,
        llm_model="qwen2.5:7b-instruct"
    )
    
    # é…ç½®2: LightRAG with large chunks
    config2 = RAGSystemConfig(
        name="LightRAG_Large_Chunks",
        chunking_method="fixed_size",
        chunk_size=512,
        chunk_overlap=100,
        embedding_model="nomic-embed-text",
        embedding_dim=768,
        retrieval_method="hybrid",
        top_k=10,
        llm_model="qwen2.5:7b-instruct"
    )
    
    # æµ‹è¯•æ–‡æ¡£
    test_doc = "LightRAG is a Simple and Fast Retrieval-Augmented Generation framework. " * 10
    chunks1 = [test_doc[i:i+256] for i in range(0, len(test_doc), 256-50)]
    chunks2 = [test_doc[i:i+512] for i in range(0, len(test_doc), 512-100)]
    
    # è¯„ä¼°ç³»ç»Ÿ1
    result1 = await eval_system.evaluate_system(
        config=config1,
        test_document=test_doc,
        chunks=chunks1,
        evaluate_chunking=True,
        evaluate_embedding=False,  # è·³è¿‡åµŒå…¥è¯„ä¼°ï¼ˆæ¼”ç¤ºï¼‰
        evaluate_retrieval=False,  # è·³è¿‡æ£€ç´¢è¯„ä¼°ï¼ˆæ¼”ç¤ºï¼‰
        evaluate_end_to_end=False  # è·³è¿‡ç«¯åˆ°ç«¯è¯„ä¼°ï¼ˆæ¼”ç¤ºï¼‰
    )
    
    # è¯„ä¼°ç³»ç»Ÿ2
    result2 = await eval_system.evaluate_system(
        config=config2,
        test_document=test_doc,
        chunks=chunks2,
        evaluate_chunking=True,
        evaluate_embedding=False,
        evaluate_retrieval=False,
        evaluate_end_to_end=False
    )
    
    # å¯¹æ¯”ç³»ç»Ÿ
    comparison_df = eval_system.compare_systems()
    
    # ç”ŸæˆæŠ¥å‘Š
    eval_system.generate_report()


if __name__ == "__main__":
    asyncio.run(test_rag_evaluation_system())
